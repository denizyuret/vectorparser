2015-01-16    <dyuret@ku.edu.tr>

	* vectorparser_rnet.m: debugging vectorparser with rnet:

	1. dump-speed: 29 sent/sec. result is correct.

	2. d1=double2single(d0): convert all data to single.
	dump-speed: 29 sent/sec. result is correct.

	3. evalnet speed: 2.7 secs for 76834 dev instances.
	630 sent/sec.
	static accuracy: 0.9724. (1-0.0276)

	4. test speed: 9.7 sent/sec.
	sent_pct: 0.5888
	head_pct: 0.0894
	word_pct: 0.0785
	move_pct: 0.0340

	5. eliminate softmax from soft.fforw.
	test speed: 10.5 sent/sec.

	6. use bsxfun: tried, slower.

	7. try profiling: nothing useful.

	8. no dumping:
	test speed: 11.64 sent/sec.

	9. train speed: minibatch=100, no save, test, stats
	dev train speed: 88.40 sent/sec.

	10. vectorparser train speed: 1.6 sent/sec.
	need minibatching.

	11. Alkan's idea of sentence parallelism: place the initial states
	of 100 sentences in one matrix and parse in parallel.

2015-01-14    <dyuret@ku.edu.tr>

	* TODO:
	- parse!
	- try maxout
	- try gaussian dropout
	- try hinge loss
	- debug metatrain using caffe.
	- finetuning: delete dw2 at start after convergence and try lr.
	- finetuning: customize dropout to drop whole features (word embeddings)
	- feature select with neural nets (start with 8 anchor words)

	+ try adaptive sgd with lr and momentum: metatrain=sgd, metatrain2=nesterov
	+ check training loss/acc, test loss: rerun trndev_01_2_7
	+ try smaller/larger lr for adagrad?
	+ output w1 w2 stats
	+ check derivatives for finetuning
	+ check does the same if lr=0: NO, DEBUG! fixed.
	+ try 2x4096 layers
	x try smaller maxnorm?  but maxnorm made it worse! bug?

	the art of generalization:
	- dropout
	- hidden units
	- hidden layers
	- l1,l2,maxnorm
	- gaussian dropout
	- input noise
	- bagging
	- bayesian averaging
	- relation between mcmc and optimizers
	- models: bayes vs slt vs regret vs bandit?

	
	*** METATRAIN EXPERIMENTS:

	Find out if SGD or Nesterov is competitive with Adagrad if the
	best lr and momentum is picked every epoch.  acc is the trn
	accuracy, how come they never get to .99?  (TODO: investigate with
	caffe)

	metatrain_04_2_5_9_.out	 epoch=200 lr=0.0003125 mom=0.75 acc=0.976162 loss=0.0665793
	metatrain2_04_2_5_9_.out epoch=200 lr=0.00015625 mom=0.9375 acc=0.976947 loss=0.0645551
	
	
2015-01-10    <dyuret@ku.edu.tr>

	*** FINETUNING EXPERIMENTS:

	Same settings as Dropout experiments except the word embeddings
	are allowed to change.  Convergence is faster but lower?
	
	Script: finetune.m 
	Parameters: finetune_lr_d1_d2_m1_m2: 0.lr is the learning rate,
	  0.d1 is dropout for 1326, 0.d2 is dropout for 20000, m1 is maxnorm
	  for 1326, m2 is maxnorm for 20000 if specified.

	finetune_01_2_5_.out  49400084	 0.01158    0.99680    0.09453	  0.97208    1452.3	34014.6
	finetune_01_2_5_2_4_  33000056	 0.0252694  0.99224    0.0827823  0.970924   1377.55	23955.5	   
	finetune_02_2_7_.out  355200040	 0.00135715 0.99948    0.174747	  0.970339   1538.35	230897	   
	finetune_02_2_9_.out  57800052	 0.0243736  0.99188    0.106695	  0.968451   1539.81	37537.1	   
	
	- start finetuning after lr=0 reaches optimum: crashed:	biyofiz-4-2-crash.txt
	- observed <.1% improvement before crash.
	- TODO: rerun.
	133100016    0.00297701   0.99923      0.102768     0.973384     1805.91      73702.4
	- TODO: more efficient finetune?
	S = sparse(i,j,s,m,n,nzmax) uses vectors i, j, and s to generate an m-by-n sparse matrix such that S(i(k),j(k)) = s(k), with space allocated for nzmax nonzeros. Vectors i, j, and s are all the same length. Any elements of s that are zero are ignored, along with the corresponding values of i and j. Any elements of s that have duplicate values of i and j are added together.
	
	* FINETUNE DEBUG:
	
	This should give the same result as trndev (lr=0 for finetune
	layer) but it doesn't?
	
	finetune2_01_2_7_inf_inf_.out	341100004    0.00183221   0.99938      0.248045     0.964989     1356.27      251498       

	Due to small bug (lr=0 was skipped, in effect using lr=1 for first
	layer).  Fixed it.  However, this does not explain why other
	finetune experiments did not perform better.  Try finetuning after
	regular training.  Finetuning starting with random weights may be
	difficult to regularize.

	
	*** DROPOUT EXPERIMENTS:

	Data: archybrid_conllWSJToken_wikipedia2MUNK-100_fv021a_xy.mat 
	Train: x_trn, y_trn (1820392)
	Test: x_dev, y_dev (76834)
	Epochs: 200 (last output at 364000008 instances)
	Architecture: 1326 x 20000 x 3
	  except for trndev4096 which has 1326 x 4096 x 4096 x 3
	Solver: Adagrad
	Parameters: trndev_lr_d1_d2 where 0.lr is the learning rate, 0.d1 is
	  dropout rate for 1326, 0.d2 is dropout rate for 20000.
	Columns: experiment, instances trained at best point, trnloss
	  (for 100K sample), trnacc, devloss, devacc, speed, time.
	Script: trndev.m
	Cmd: for i in trndev*.out; do echo -n "$i	"; grep '^[0-9]' $i | sort -gr -k5 | head -1; done
	
	trndev4096_02_2_7_7 273500000 0.0387397 0.98938   0.074399  0.973046  2616.6    104525       
	trndev_02_2_7_.out  349900064 0.0138664 0.99678	  0.0851207 0.972994  2709.98	129115
	trndev_02_2_8_.out  357200032 0.0239076 0.99315   0.0793867 0.972994  2516.52   141942       
	trndev_03_2_7_.out  195100044 0.0190186 0.99487   0.0837947 0.972916  2525.96   77238.1      
	trndev_01_2_6_.out  324400076 0.0158897 0.99623	  0.0832696 0.972772  2715.88	119446	  
	trndev_01_1_7_.out  252300096 0.0193457 0.99496	  0.0808878 0.972733  2717.08	92856.9	  
	trndev_01_2_7_.out  323100084 0.02304	0.99345	  0.07941   0.97262   2468.9	130868.4
	trndev_02_3_7_.out  344800088 0.0221946 0.99403   0.0804343 0.972577  2534.09   136065       
	trndev_02_1_7_.out  249800004 0.0113104 0.99745   0.0898223 0.972577  2533.56   98596.4      
	trndev_01_2_5_.out  197800036 0.01672	0.99576	  0.08264   0.97253   2578.7	76706.3
	trndev_01_2_8_.out  336200028 0.0335366 0.98931	  0.0776822 0.972512  2714.95	123833	  
	trndev_01_3_7_.out  308100048 0.0326249 0.98985	  0.0778532 0.972499  2721.05	113229	  
	trndev_01_2_7_.out  350000064 0.0219642 0.99403	  0.0794849 0.97246   2717.94	128774	  
	trndev_02_2_6_.out  206700096 0.0134424 0.99676   0.086945  0.972291  2510.21   82343.7      	
	trndev_01_0_5_.out  91300000  0.01546	0.99646	  0.08890   0.97211   2485.9	36726.8
	trndev_005_2_7_.out 332600044 0.0377884 0.9878	  0.0778049 0.972096  2716.29	122446	  
	trndev_01_2_9_.out  360900016 0.05180	0.98197	  0.07899   0.97158   2607.2	138424.8
	trndev_005_0_0_.out 179800016 0.00809	0.99840	  0.10002   0.97112   2602.1	69099.1
	trndev_01_0_0_.out  48400092  0.01139	0.99720	  0.09796   0.97108   2597.6	18632.7
	
	*** SMALL DROPOUT EXPERIMENTS:

	The settings are similar to dropout experiments except they use
	the smaller x_tst (108536) for training to get quick results.  The
	training is for 200 epochs, 21700064 instances.

	tstdev_0_01_0_1_0_9_.out	20600004	0.00961	0.99890	0.16212	0.95108	2573.8	8003.9
	tstdev_0_01_0_2_0_85_.out	20600004	0.00650	0.99943	0.16662	0.95096	2572.3	8008.3
	tstdev_0_02_0_2_0_9_.out	16700008	0.01333	0.99805	0.16215	0.95084	2579.1	6475.0
	tstdev_0_01_0_2_0_9_.out	15400076	0.02035	0.99659	0.14859	0.95074	2582.1	5964.1
	tstdev_0_02_0_2_0_7_.out	19600080	0.00069	0.99990	0.22781	0.95066	2587.7	7574.3
	tstdev_0_01_0_2_0_8_.out	16800044	0.00466	0.99960	0.17460	0.95066	2571.4	6533.4
	tstdev_0_005_0_2_0_7_.out	20500068	0.00497	0.99968	0.16888	0.95057	2573.4	7966.3
	tstdev_0_005_0_2_0_5_.out	18500020	0.00212	0.99990	0.18473	0.95057	2581.8	7165.6
	tstdev_0_01_0_3_0_9_.out	21700064	0.02035	0.99667	0.14735	0.95056	2567.2	8452.9
	tstdev_0_01_0_2_0_7_.out	16700008	0.00189	0.99987	0.19339	0.95045	2569.4	6499.6
	tstdev_0_01_0_3_0_7_.out	14200080	0.00412	0.99971	0.17445	0.95039	2581.1	5501.5
	tstdev_0_01_0_1_0_7_.out	15600048	0.00137	0.99989	0.20579	0.95033	2581.1	6043.9
	tstdev_0_005_0_2_0_9_.out	20600004	0.02983	0.99378	0.14162	0.95032	2583.4	7973.9
	tstdev_0_005_0_3_0_5_.out	20500068	0.00294	0.99985	0.17499	0.95027	2571.3	7972.5
	tstdev_0_03_0_2_0_7_.out	21700064	0.00054	0.99987	0.25134	0.95015	2473.0	8774.9
	tstdev_0_01_0_2_0_6_.out	11500080	0.00177	0.99982	0.19593	0.95015	2562.1	4488.6
	tstdev_0_01_0_2_0_5_.out	11900024	0.00105	0.99992	0.20476	0.95010	2564.1	4641.1
	tstdev_0_005_0_2_0_3_.out	 8100064	0.00399	0.99971	0.17634	0.95009	2567.3	3155.1
	tstdev_0_005_0_1_0_5_.out	18100076	0.00155	0.99990	0.19938	0.95005	2571.2	7039.5
	tstdev_0_0025_0_2_0_5_.out	16700008	0.01289	0.99887	0.15255	0.94998	2569.4	6499.6
	tstdev_0_01_0_2_0_95_.out	20900012	0.03979	0.98924	0.14130	0.94989	2584.0	8088.1
	tstdev_0_01_0_1_0_5_.out	20200096	0.00044	0.99993	0.23925	0.94989	2583.5	7818.8
	tstdev_0_01_0_3_0_5_.out	18000040	0.00098	0.99989	0.20646	0.94979	2572.5	6997.2
	tstdev_0_01_0_2_0_3_.out	 9100088	0.00092	0.99990	0.21214	0.94979	2583.4	3522.5
	tstdev_0_02_0_2_0_5_.out	19700016	0.00040	0.99995	0.25615	0.94971	2580.1	7635.4
	
	
2014-12-26    <dyuret@ku.edu.tr>

	* adagrad_of3/oftest3: Trying L1 regularization.

	Summary of results: 
	
	lambda	devloss			devacc
	1e-5	0.145027 @ 9.5e6	0.944972 @ 9.0e6
	1e-6	0.136917 @ 3.7e6	0.949619 @ 6.3e6
	
	Notes: L1 with 1e-6 gives almost identical results with L2 with
	1e-4.  L1 with 1e-5 starts around the same with L2@1e-3 but later
	gains both in trn and dev.  Fairly certain there is a hard limit
	at 0.95 devacc using L1/L2.

	These experiments give nearly identical final devacc:
	L2@1e-5, L2@1e-4, L2@0, L1@1e-6
	These are followed by (in order):
	L2@3e-4, L1@1e-5, L2@1e-3

	Haven't tried: 
	- separate w1 w2 regularization constants.
	- higher learning rate
	- not applying regularization to bias terms.
	- maxnorm regularization.
	- dropout
	
	* adagrad_of2/oftest2: Trying L2 regularization: 
	
	Summary of results:
	
	lambda	devloss			devacc
	1e-2	0.340102 @ 9.5e6	0.882383 @ 6.8e6
	1e-3	0.169720 @ 9.5e6	0.938582 @ 9.0e6
	3e-4	0.141040 @ 4.7e6	0.946899 @ 5.2e6
	1e-4	0.137502 @ 3.7e6	0.949606 @ 9.0e6
	1e-5	0.143399 @ 6e5		0.948877 @ 3.0e6
	0	0.144738 @ 6e5		0.949189 @ 9.2e6
	
	Notes: 1e-5 is very close to no regularization in trnloss, but the
	upturn in devloss is less pronounced.  1e-4 and larger lambda have
	no upturn in devloss at all.
	
	Implementation details:
	ufldl gives the total cost as 
	
	J = 1/M sum_m instance_cost_m + lambda/2 |W|^2

	The gradient is

	dJ/dw_ij = 1/M sum_m dJ_m/dw_ij + lambda * W

	The update rule is

	w{l} = w{l} - alpha * [ (1/M) sum_m dJ_m/dw{l} + lambda * w{l} ]
	
	- How do we apply this to minibatch?  Can we have lambda be
	independent of minibatch size?  -- I think it already is, lambda
	is the tradeoff between |W|^2 and per-instance cost.  Thus it is
	invariant to batch size in the above expression.
	
	- lambda is the precision of individual values in W.  We start
	from std=1e-2, var=1e-4, prec=1e4.  At the end, with w101 (our
	model with best dev accuracy): w{1} std=2.8e-2, var=7.9e-4,
	prec=1.26e3; w{2} std=1.3e-1, var=1.7e-2, prec=5.7e1.  Should we
	use separate regularization coefficients for w{1} and w{2}?  --
	next version.

	- Does the scaling in w{2} matter? yes it does.  if all answers
	are correct w{2}->inf keeps increasing likelihood.

	
	* adagrad_of1/oftest1: original adagrad for reference, using tst
	for training and evaluating on dev.  complete evaluation every
	100K instances.

	trnloss: 1.5e-3 @ 1e7	trnacc: .9999 @ 1e7
	devloss: 1.4e-1 @ 6e5	devacc: .9492 @ 9e6

	iter	devacc
	4e5	.943892
	5e5	.944985
	6e5	.946469
	1.2e6	.947810
	2.0e6	.948096
	6.2e6	.949098

	* Try:
	- dropout: seems best chance?
	- L2 reg
	- L1 reg
	- sparsity reg
	- change obj function (but that is not where the problem is)
	- start with random w0
	- reduce hidden layer?

	* starter.mat: To test overfitting remedies quickly, use the 108K
	tst for training and 76K dev for testing from
	archybrid_conllWSJToken_wikipedia2MUNK-100_fv021a_xy.mat.  w000 is
	a random network, w101 is the optimized network from startup1.m.
	
	  Name           Size                    Bytes  Class
	  w000           1x2                       440  cell                
	  w101           1x2                       440  cell                
	  x1_dev      1327x76834             407834872  single              
	  x1_tst      1327x108536            576109088  single              
	  y_dev          1x76834                307336  single              
	  y_tst          1x108536               434144  single              

2014-12-25    <dyuret@ku.edu.tr>

	* weight-decay: Here are the average weight magnitudes:
	mean(abs(w(:))); and dev/trn loss/acc for the final network.
	
		1,M	N	O	0
	w0	0.0801	0.0823	0.0818	-
	w{1}	0.0221	0.0200	0.0256	0.0831
	w{2}	0.1114	0.0912	0.1379	0.1297
	devloss	0.0949	0.1742	0.1399	0.1627
	trnloss	0.0123	0.0033	0.0018	0.0032
	devacc	0.9707	0.9655	0.9697	0.9692
	trnacc	0.9975	0.9992	0.9994	0.9992

	columns:
	1,M: experiments 1 and M use adagrad lr=0.004 with fixed embeddings w0.
	N: variable embeddings w0, random initial weights w{1,2}.
	O: variable embeddings w0, using M's final weights to initialize w{1,2}.
	0: experiment 0 lr=0.04 with fixed embeddings w0.

	There is definite overfitting signs in loss/acc for N/O but the
	weight magnitudes are not very different from M.  Unlike
	experiment 0 (lr=0.04) where you can see a definite increase in
	weight magnitudes.  So regularization may have improved experiment
	0 but is not likely to help the others.  Here are the
	min/mean(abs)/max numbers:

		w0			w{1}			w{2}
	1,M	[-0.4448:0.0801:0.5000]	[-0.2838:0.0221:0.2211]	[-0.5210:0.1114:0.4351]
	N	[-0.5575:0.0823:0.6051]	[-0.2218:0.0200:0.1827]	[-0.4414:0.0912:0.3210]
	O	[-0.4759:0.0818:0.5686]	[-0.2794:0.0256:0.2510]	[-0.5975:0.1379:0.4962]
	0	--			[-1.3639:0.0831:1.4626]	[-1.0373:0.1297:0.9182]
	
	Try:
	- dropout: seems best chance?
	- L2 reg
	- L1 reg
	- sparsity reg
	- change obj function (but that is not where the problem is)
	- start with random w0
	- reduce hidden layer?
	
2014-12-23    <dyuret@ku.edu.tr>

	* summary: definite overfitting:

		startup1	startupN	startupO
	trnloss	0.00928@1.8e8	0.00475@7.7e7	0.00111@8.0e7
	devloss	0.080819@4.4e7	0.0839447@5.7e6	0.0940749@3.3e6
	devacc	0.971614@1.7e8	0.969662@1.1e7	0.97138@2.3e7

	Wish I recorded the average size of weights, gradients, steps,
	etc: for w101 (final weights of startup1):
	>> mean(abs(w101{1}(:))) % 0.0221
	>> mean(abs(w101{2}(:))) % 0.1119

	For the word embeddings:
	>> mean(abs(w0(:))) % 0.0801

	Overfitting remedies: early-stop, weight-decay, dropout.
	:(early-stop would not help us).

	Better search remedies: momentum, nesterov, lecunn.  (however we
	are doing fine with trnloss, so search is not the problem).

	Better objective function: devacc does not always follow devloss.
	:(could also check trnacc vs trnloss).  At the end of training in
	startup1, trnacc=0.9975 and trnloss=0.0122.  Note that this is the
	complete trnloss as opposed to the numbers in the table above
	which just give a moving average.

	* startupO.out: this one is updating w0, starting from the already
	converged w.  Stops impoving after 1e7.

	* startupN.out: this one is updating w0 (still starting from
	random w).  Starts ahead of startup1, but devacc stops improving
	and goes down after 1e7.  trnloss is reduced much faster to a
	lower minimum compared to startup1, but devloss hits an earlier
	:(7e6) and shallower minimum:

	* startupM.out: replicating startup1.

	* prepdata.m: get it all ready for finetuning. data saved in
	prepdata.mat:

	Name          Size                      Bytes  Class     Attributes
	w             1x2                   106405544  cell                
	w0          100x37641                15056400  single              
	xf_dev      526x76834               161658736  single              
	xf_trn      526x1820392            3830104768  single              
	xf_tst      526x108536              228359744  single              
	xs_dev        8x76834                 2458688  single              
	xs_trn        8x1820392              58252544  single              
	xs_tst        8x108536                3473152  single              
	y_dev         1x76834                  307336  single              
	y_trn         1x1820392               7281568  single              
	y_tst         1x108536                 434144  single              
	

	* test-plan:
	
	1. compare forward: To compare forward and ftforward, we need to
	rearrange x or rearrange w1 or both.  rearrangewx does both.  I
	saved w101, x1_dev, y_dev, w0 and w0hash for testing under
	forwtest.mat.  forwtest.m ensures the forward and ftforward work
	the same way.
	
	2. compare backward: backtest works using ftbackward and
	backsparse but nothing to compare it to.
	
	3. test new gradient: gradtest works perfectly.
	
	4. finetune: need to transform trn and dev sets, adapt adagrad (is
	the learning rate going to be ok without the history?), train
	network.  But first, we may want to replicate w101 using the new
	architecture without touching the embeddings.

	* function: z = forwsparse(w, x)
	% x(n,m) is the data matrix where each column is an instance
	% containing the indices of n words; w(d,v) is the embedding
	% matrix, column i contains the embedding for word i.  The
	% resulting z(d*n,m) will contain the embeddings instead of the
	% indices for the words.

	* function: [xsparse, xfull] = xsplit(x, wcache)
	% x is the original feature vector with word embeddings mixed with
	% other features.  wcache is a hash table that returns the word index
	% given its embedding.  In the output xsparse will contain the indices
	% of words whose embeddings are in x and xfull will contain all the
	% other features.
	

2014-12-20    <dyuret@ku.edu.tr>

	* xformx(x, wcache): transform feature vectors replacing each word
	embedding with the embedding index.  xformy(y,uniqw) reverses the
	transformation for debugging.

	* vtable.m (find): fixes the problem with kernelcache, and now
	wcache = vtable(uniqw, 1:size(uniqw,2)) works.

	* wcache: kernelcache(uniqw, 1:size(uniqw,2)) does not work
	because kernelcache may not keep all keys.  I need to add
	collision resolution.

	* uniqw.mat: a dictionary of all unique word embeddings, size:
	100, 37641.

	* mergew.m (mergew): takes an x matrix, splits into feature
	matrices, identifies the features that are word embeddings (the
	ones ending with 4 in fv021a), merges and uniques them.

	* splitx.m (splitx): takes an x matrix and splits it into a cell
	array of individual feature matrices.

	* fine-tuning:
	1. figure out which parts of the 1326 dims correspond to word embeddings.
	2. convert all of x into sparse format:
	2a. for embeddings use one-hot
	2b. for the rest keep the original values
	3. initialize the first layer of weights
	3a. for embeddings use embeddings as weight
	3b. for the rest use 1 as weight
	4. how do we do weight sharing?

	fv021a:
		dims
		s2w	100w
		s2r1l=	4
		s1w	100w
		s1r<	4
		s1r1l=	4
		s0l1c	100
		s0l1r=	4
		s0l1w	100w
		s0c	100
		s0-	1
		s0r=	4
		s0w	100w
		s0r1r=	4
		s0r1w	100w
		n0l1c	100
		n0l1-	1
		n0l1w	100w
		n0c	100
		n0w	100w
		n1c	100
		n1w	100w

		total	1326
	

2014-12-07    <dyuret@ku.edu.tr>

	* net_forward.py: THE PYCAFFE INTERFACE

	

2014-12-04    <dyuret@ku.edu.tr>

	* a: NO OBVIOUS ADVANTAGE OF USING MULTILAYER...

	
	* compare: to single layer results: 40K@14221=.9633;
	10K@20000=.9631; 20K@20000=.9637; 50K@20000=.9640;
	100K@20000=.9646.  2-layer result does as well as 50K with a
	quarter of the weights.  3-layer result is as big as the 100K
	1-layer net and performs about the same.  Not sure if the training
	and initialization settings are optimal for deeper nets.
	
	
	
	* caffe/try2layer.solver (weight_decay): try best two layer
	multi-epoch.  Not sure about base_lr, gamma and power.  Climbs to
	0.969481 in 29/100 epochs.  Not better than the 10K 1-layer
	result.
		
	
	* optimize01multi3s.py: search for better solver parameters:
	hidden_type, solver_type, base_lr, momentum, filler.  Fixing
	layers to best found: [41744.0, 2139.0, 1278.0].  Crashed after 42
	epochs.  Could not beat original multi3 result.

	best: {'status': 'ok', 'accuracy': 0.96450599999999997, 'batch_size': 128, 'base_lr': 0.0065169111999158246, 'num_layers': 3, 'num_weights': 147353031.0, 'loss': 0.094876199999999994, 'hidden_size': [1326, 41744.0, 2139.0, 1278.0], 'filler': 'gaussian', 'iter': 14221.0, 'hidden_type': 'RELU', 'sparse': 15, 'time': 2506.8834750652313, 'solver_type': 'ADAGRAD', 'momentum': 0, 'layer_size': (0.40213720841133166, 0.23012665018520029, 0.95601491622076562)}
	

	* optimize01multi2s.py: fixes the layers and plays with solver
	parameters: default initialization and base_lr should change:

	gaussian with sparse:15 initialization works better than xavier.
	RELU and ADAGRAD still best.
	optimize01multi2.py base_lr was too high:
	base_lr   GOOD: 0.0170648 */ 1.80459   BAD: 0.0258022 */ 15.6296   ALL: 0.0239517 */ 12.269
	best: {'status': 'ok', 'accuracy': 0.96415300000000004, 'batch_size': 128, 'base_lr': 0.019044659399853212, 'num_layers': 2, 'num_weights': 19413489, 'loss': 0.095748799999999995, 'hidden_size': [1326, 12684.0, 205.0], 'filler': 'gaussian', 'iter': 14221.0, 'hidden_type': 'RELU', 'sparse': 15, 'time': 388.48415303230286, 'solver_type': 'ADAGRAD', 'momentum': 0, 'layer_size': (0.17713910718922066, 0.76564147675251237)}
	

	
	* optimize01multi2: (-t 0.0975, good:26, bad:151, filler:xavier,
	batch_size: 128, iter:14221, hidden_type:RELU, base_lr: 0.03,
	solver_type: ADAGRAD, num_layers: 2)
	
	num_weights   GOOD: 1.5909e+07 */ 2.04684   BAD: 1.57541e+07 */ 4.80142   ALL: 1.57767e+07 */ 4.37054
	hidden_size1   GOOD: 10420.6 */ 2.07898   BAD: 6850.87 */ 6.43285   ALL: 7286.21 */ 5.74497
	hidden_size2   GOOD: 182.231 */ 1.63645   BAD: 414.664 */ 6.82097   ALL: 367.489 */ 6.09214
	
	best: {'status': 'ok', 'accuracy': 0.96398300000000003, 'batch_size': 128, 'base_lr': 0.029999999999999999, 'num_layers': 2, 'num_weights': 19413489.0, 'loss': 0.096830600000000003, 'hidden_size': [1326, 12684.0, 205.0], 'filler': 'xavier', 'iter': 14221.0, 'hidden_type': 'RELU', 'sparse': -1, 'time': 387.58310914039612, 'solver_type': 'ADAGRAD', 'momentum': 0, 'layer_size': (0.17713910718922066, 0.76564147675251237)}
	
	
	* optimize01multi3: (-t 0.0962, good:10, bad:86, filler:xavier,
	batch_size: 128, iter: 14221, hidden_type: RELU, base_lr: 0.015,
	solver_type: ADAGRAD, num_layers: 3)

	num_weights   GOOD: 1.10835e+08 */ 1.49896   BAD: 2.3453e+07 */ 6.12883   ALL: 2.75712e+07 */ 5.96042
	hidden_size1   GOOD: 31209.1 */ 1.47272   BAD: 4963.13 */ 7.06238   ALL: 6010.85 */ 6.94193
	hidden_size2   GOOD: 2038.14 */ 1.53243   BAD: 1648.99 */ 4.05912   ALL: 1685.79 */ 3.79886
	hidden_size3   GOOD: 1448.22 */ 1.63807   BAD: 1414.01 */ 17.2858   ALL: 1417.54 */ 14.9108
	
	best: {'status': 'ok', 'accuracy': 0.96455900000000006, 'batch_size': 128, 'base_lr': 0.014999999999999999, 'num_layers': 3, 'num_weights': 147353031.0, 'loss': 0.094832200000000005, 'hidden_size': [1326, 41744.0, 2139.0, 1278.0], 'filler': 'xavier', 'iter': 14221.0, 'hidden_type': 'RELU', 'sparse': -1, 'time': 2457.8661398887634, 'solver_type': 'ADAGRAD', 'momentum': 0, 'layer_size': (0.40213720841133166, 0.23012665018520029, 0.95601491622076562)}
	

	* optimize01multi: Crashed after 214 epochs. best:0.964179.

	best: {'status': 'ok', 'accuracy': 0.96417900000000001,
	'batch_size': 128, 'base_lr': 0.0097047551399354146, 'num_layers':
	2, 'num_weights': 127870788.0, 'loss': 0.095408099999999996,
	'hidden_size': [1326, 45907.0, 1460.0], 'filler': 'xavier',
	'iter': 14221.0, 'hidden_type': 'RELU', 'sparse': -1, 'time':
	2219.0223879814148, 'solver_type': 'ADAGRAD', 'momentum': 0,
	'layer_size': (0.39584370595375412, 0.36059017496041618)}


	* optimize01multi4: Crashed after 106 epochs.  best:0.962856.

	best: {'status': 'ok', 'accuracy': 0.96285600000000005, 'batch_size': 128, 'base_lr': 0.01, 'num_layers': 4, 'num_weights': 76562671.0, 'loss': 0.099107100000000004, 'hidden_size': [1326, 14702.0, 1870.0, 1671.0, 15833.0], 'filler': 'xavier', 'iter': 14221.0, 'hidden_type': 'RELU', 'sparse': -1, 'time': 1358.0881118774414, 'solver_type': 'ADAGRAD', 'momentum': 0, 'layer_size': (0.59006931288208042, 0.47527763928829037, 0.91893784535046497, 0.48874269490839806)}
	

	
2014-12-02    <dyuret@ku.edu.tr>

	* q: start trying multi-layer nets. $$ilac-3-0
	BUG: we have loss: nan in a number of cases.
	The culprit seems to be POWER and BNLL units.	


2014-11-30    <dyuret@ku.edu.tr>

	* TODO: get ready for fine-tuning.

	* q: try 5k and 20k 50k 100k hidden long training.  100 epochs,
	1.8e8 instances total.  File:
	caffe_bin_train_solver_try*_solver.dat,out
	;(x1000 hidden, best-inst, best-err, last-inst, last-err):

	5	1.5232e+08	0.968657	1.8176e+08	0.968539
	10	1.2992e+08	0.969549	1.82016e+08	0.969182
	20	1.408e+08	0.970071	1.8176e+08	0.969873
	50	1.4208e+08	0.970241	1.8176e+08	0.969952
	100	1.6896e+08	0.970358	1.8176e+08	0.970189
	
	* optimize01big2: Unfortunately nets larger than ~12000 fail!
	Possibly because I don't batch test set.  Partial results suggest
	still preferring big networks.  After fixing, still prefers big
	nets:

	$ analyze.pl -t 0.1 optimize01big2.out
	iter   GOOD: 14221 */ 1   BAD: 14221 */ 1   ALL: 14221 */ 1
	hidden_type   GOOD: RELU:13   BAD: RELU:87   ALL: RELU:100
	status   GOOD: ok:13   BAD: ok:87   ALL: ok:100
	time   GOOD: 839.432 */ 1.32284   BAD: 277.928 */ 2.47621   ALL: 320.877 */ 2.5328
	base_lr   GOOD: 0.0212872 */ 1.36468   BAD: 0.018504 */ 9.33697   ALL: 0.0188441 */ 8.06283
	solver_type   GOOD: ADAGRAD:13   BAD: ADAGRAD:87   ALL: ADAGRAD:100
	loss   GOOD: 0.0995812 */ 1.00284   BAD: 0.12355 */ 1.31899   ALL: 0.120134 */ 1.30765
	filler   GOOD: xavier:13   BAD: xavier:87   ALL: xavier:100
	hidden_size   GOOD: 36747.9 */ 1.34561   BAD: 9342.57 */ 3.49491   ALL: 11163.1 */ 3.5229
	batch_size   GOOD: 128 */ 1   BAD: 128 */ 1   ALL: 128 */ 1
	accuracy   GOOD: 0.962935 */ 1.00029   BAD: 0.953317 */ 1.01653   ALL: 0.954562 */ 1.01578

	* try.net: trying best guess network on multi-epoch.  Reached .031
	error with 10K hidden layer.
	Error curve is a good fit to: f(x)=a*(x**b)+c with
	a               = 444.994          +/- 48.55        (10.91%)
	b               = -0.760042        +/- 0.007633     (1.004%)
	c               = 0.0303667        +/- 1.943e-05    (0.06398%)
	x: number of instances processed
	which obviously asymptotes at 0.0303667 (1-0.969633).


2014-11-29    <dyuret@ku.edu.tr>

	* julia-vs-matlab: Even though julia seems to multiply large
	square matrices faster, matlab beats it on nnet code:
	sparseAutoencoderCost x 100 1.56 seconds vs 5.05 seconds.  Top
	shows julia utilizing 16 threads, matlab 9 threads.

	Splitting the julia script for timing:
	total: 50.5 ms
	reshape: 0.0217 ms
	+cost: 29.32 ms

	Here is the core forward calculation:

	julia:
	w=rand(25,64);x=rand(64,10000);
	tic();for i=1:1000; c=w*x; end; toc()
	elapsed time: 2.811285643 seconds

	matlab:
	w=rand(25,64);x=rand(64,10000);
	tic(); for i=1:1000; c=w*x; end; toc()
	Elapsed time is 0.789418 seconds.

	Note that for 1000x1000 square matrices julia wins:
	11.89ms vs 13.81ms.

	But when the dims are nnet like: 100x100x10000:
	julia:5.86ms matlab:2.80ms

	So it is the blas difference.  Now comes a bigger one:

	julia:
	size(c) => (100,10000)
	tic(); for i=1:1000; d=1./(1+exp(-c)); end; toc()
	46.63ms

	matlab:
	tic(); for i=1:1000; d=1./(1+exp(-c)); end; toc()
	2.066ms

	julia:
	tic(); for i=1:1000; sigmoid!(c); end; toc()
	15.73ms

	In both experiments Julia uses single core.  Matlab uses all
	available (19) cores.  Let's try julia with multicore.

	First of all, the blas calculation slows down:
	tic();for i=1:1000; c=w*x; end; toc();
	11.5ms instead of 5.86ms

	Elementwise calculation uses single core and does not improve.
	tic(); for i=1:1000; sigmoid!(c);end; toc()
	elapsed time: 15.731746731 seconds

	Seems we need to tell Julia explicitly to use a distributed array.
	DArray or SharedArray: In a DArray, each process has local access
	to just a chunk of the data, and no two processes share the same
	chunk; in contrast, in a SharedArray each “participating” process
	has access to the entire array. A SharedArray is a good choice
	when you want to have a large amount of data jointly accessible to
	two or more processes on the same machine.

	function shsigm!(x)
	for i in localindexes(x)
        x[i] = 1.0 / (1.0 + exp(-x[i]))
	end
	end

	tic();  for i=1:1000; @sync for p in procs(cs); @async remotecall_wait(p, shsigm!, cs); end; end; toc()
	elapsed time: 4.623549889 seconds

	tic(); @sync for i=1:1000;  for p in procs(cs);  remotecall(p, shsigm!, cs); end; end; toc()
	elapsed time: 4.306722203 seconds

	So we need to use shared arrays for elementwise non-blas
	operations.  Or just use cuda.

	* julia: what do we really need:
	1. we can look at all the array operators in julia.
	2. we can look at the ufldl code.

	Here is some julia code in ufldl: ~/ufldl/sparseae_exercise/julia

	2.1 matrix mult (gemm)
	2.2 broadcast? broadcast(+, W1 * data, b1))
	2.3 elementwise sigmoid, log
	2.4 normsq (blas?)
	2.5 mean, sum, std (row and column-wise?)
	2.6 array-array ops
	2.7 scalar-array ops
	2.8 reshape, concat
	2.9 inplace versions

	Is this really worth it?  Why not just use python where people
	already have done the work?

	* optimize01: 	restarting optimize01 with just adagrad and batchsize=128.

	epoch01a: (t=0.09, 13/161)
	base_lr: 0.0405939188295189 */ 1.40090316764676
	hidden_size: 7682.650574705 */ 1.23317528466568
	hidden_type: RELU:13
	filler: xavier:12 gaussian:1

	Best fit power law with epoch01a and epoch5:
	inv: rate = base_lr * pow(1 + gamma * iter, -power)
	base_lr = 0.0406889
	gamma = 2.53082
	power = 0.81739

	It chooses huge nets that probably overfit.

	{'status': 'ok', 'loss': 0.0881099, 'accuracy': 0.967216, 'filler': 'xavier', 'batch_size': 128, 'hidden_type': 'RELU', 'base_lr': 0.03625285061190084, 'iter': 14221.0, 'sparse': -1, 'time': 331.99004197120667, 'hidden_size': 9871.0, 'momentum': 0, 'solver_type': 'ADAGRAD'}

	caffe.bin test -model optimize01t.net -weights data/optimize01_iter_14221.caffemodel -gpu 0 -iterations 1
	accuracy = 0.961471, Loss: 0.103898

	I better pick the net size and type based on dev set performance:

	optimize01dev (-t0.103, 10/100):
	base_lr: 0.0256353 */ 1.48226
	hidden_type: RELU
	filler: xavier (9/10)
	hidden_size: 6175.21 */ 1.40084

	The top entry: {'status': 'ok', 'loss': 0.10075, 'accuracy': 0.962636, 'filler': 'xavier', 'batch_size': 128, 'hidden_type': 'RELU', 'base_lr': 0.02818327987838749, 'iter': 14221.0, 'sparse': -1, 'time': 225.74818801879883, 'hidden_size': 8526.0, 'momentum': 0, 'solver_type': 'ADAGRAD'}

	The minimum hidden_size in the top 10 is 3611.  So it really
	prefers big nets.  I should change the range and re-experiment.
	We can fix ADAGRAD, xavier and RELU, leaving only lr and
	hidden_size to optimize.

2014-11-28    <yuret@biyofiz-4-2>

	* julia: speedtest julia.  does it matter which
	atlas/blas/lapack/cublas/mkl we compile it with?

	Figure out how to add new types.

	Figure out how to call C code.

	Standard Julia release-3.0 links to its own openblas, utilizes
	multicore, beats MATLAB in multiplying square matrices (6 seconds
	vs 10)!  How come I got beat in the ufldl code?  Was it the
	optimization algorithm?

	Both cublas and julia use column major arrays (meaning columns are
	contiguous, also matlab).  C and python uses row major.

2014-11-28    <dyuret@ku.edu.tr>

	* caffe-sparse-bug: blobs are defined n,k,h,w where w changes
	fastest.  k-dimensional vector inputs are represented n,k,1,1.  An
	fc layer the dimensions are 1,1,o,i.

	* optimize:
	epoch1:
	{'status': 'ok', 'loss': 0.0931981, 'weight_filler_type': 'gaussian', 'accuracy': 0.965215, 'hidden_type': 'RELU', 'weight_filler_sparse': 159.0, 'batch_size': 139.0, 'base_lr': 0.027778874681287355, 'iter': 13096.0, 'time': 158.6161048412323, 'hidden_size': 2291.0, 'momentum': 0, 'solver_type': 'ADAGRAD'}

	epoch2: base_lr (t=0.08): 0.0148726826498462 */ 1.4162664820824
	{'status': 'ok', 'loss': 0.0788819, 'iter': 37150.0, 'base_lr': 0.015652133746125764, 'batch_size': 49.0, 'time': 224.79885601997375, 'accuracy': 0.970867, 'momentum': 0, 'solver_type': 'ADAGRAD'}

	epoch3: base_lr (t=0.071): 0.00909809585906311 */ 1.13924993300822
	{'status': 'ok', 'loss': 0.0708306, 'iter': 7166.0, 'base_lr': 0.0081855879990424, 'batch_size': 254.0, 'time': 122.43483400344849, 'accuracy': 0.97385, 'momentum': 0, 'solver_type': 'ADAGRAD'}

	epoch4: base_lr (t=0.0665): 0.00681034131604815 */ 1.43373450020877
	{'status': 'ok', 'loss': 0.0658699, 'iter': 4815.0, 'base_lr': 0.006898358995819238, 'batch_size': 378.0, 'time': 95.30875086784363, 'accuracy': 0.975837, 'momentum': 0, 'solver_type': 'ADAGRAD'}

	With a batchsize of 128 we have 14222 iterations per epoch.
	;(1820392 total).

	lr_policy types:
	fixed: rate = base_lr
	step: rate = base_lr * pow(gamma, floor(iter/stepsize))
	exp: rate = base_lr * pow(gamma, iter)
	inv: rate = base_lr * pow(1 + gamma * iter, -power)

	The decay slows down so it is not step or exp.
	Best fit inv parameters (assuming iter=epoch):
	base_lr: 0.221148
	gamma: 41.1619
	power: 0.721446

	This predicts the next epoch lr=0.00554191.  Let's try.

	epoch5:	base_lr (t=0.062): 0.00586389209910822 */ 1.28203764210223
	{'status': 'ok', 'loss': 0.0616636, 'iter': 2011.0, 'base_lr': 0.005861326575159059, 'batch_size': 905.0, 'time': 91.94862985610962, 'accuracy': 0.977491, 'momentum': 0, 'solver_type': 'ADAGRAD'}

2014-11-27    <dyuret@ku.edu.tr>

	* summary: Use ADAGRAD.  Fix batch_size at something like 128/256,
	result not very sensitive.  But batchsize effects the exact point
	of lr so we should fix it.  Then optimize lr schedule and network
	size.  Check if the two are correlated looking at optimize0?.out.
	Next try two hidden layers.  Next try finetuning word embeddings.

	ADAGRAD is best and gets rid of momentum.  Batchsize is
	unimportant: the top-10 in epoch 1 is in [35,281].  The filler
	does not have that much affect in one-layer.  That leaves base_lr
	and hidden_size: top-10 in range [342:9457].  Since we are looking
	at training error, it might favor large hidden layers that may not
	generalize well.  If there is interaction between learning rate
	and hidden size that's also bad.

	* optimize02.*: Optimizing the second epoch.  This was the optimal settings for epoch-1:

	{'status': 'ok', 'loss': 0.0931981, 'weight_filler_type': 'gaussian', 'accuracy': 0.965215, 'hidden_type': 'RELU', 'weight_filler_sparse': 159.0, 'batch_size': 139.0, 'base_lr': 0.027778874681287355, 'iter': 13096.0, 'time': 158.6161048412323, 'hidden_size': 2291.0, 'momentum': 0, 'solver_type': 'ADAGRAD'}

	Here is the optimum for epoch-2.  ADAGRAD wins again (the top 10):

	{'status': 'ok', 'loss': 0.0788819, 'iter': 37150.0, 'base_lr': 0.015652133746125764, 'batch_size': 49.0, 'time': 224.79885601997375, 'accuracy': 0.970867, 'momentum': 0, 'solver_type': 'ADAGRAD'}

	A very close second and third with larger batch-size and faster training:

	{'status': 'ok', 'loss': 0.0788897, 'iter': 6817.0, 'base_lr': 0.019324857282157396, 'batch_size': 267.0, 'time': 116.07905602455139, 'accuracy': 0.970713, 'momentum': 0, 'solver_type': 'ADAGRAD'}
	{'status': 'ok', 'loss': 0.0788908, 'iter': 17013.0, 'base_lr': 0.013628816570904334, 'batch_size': 107.0, 'time': 162.01521611213684, 'accuracy': 0.970711, 'momentum': 0, 'solver_type': 'ADAGRAD'}

	Ideal base_lr is affected by batchsize.  But the absolute
	batchsize does not seem to affect performance much.  We should
	just freeze at 128, and focus on optimizing learning rate and
	hidden unit count.  One more epoch to get the lr curve:

	* optimize01.*: Train the first epoch using the optimal parameters
	mentioned below.  Confirmed train accuracy: 0.9652 loss: 0.0932.
	Model saved in data/optimize01_iter_13096.caffemodel.  Testing with dev:

	$ caffe.bin test -model optimize01b.net -weights data/optimize01_iter_13096.caffemodel -gpu 0 -iterations 1

	On the dev set accuracy: 0.9607 loss: 0.1045.

	Now we need to optimize the second epoch to see if the ideal
	settings changed.  (Certainly the learning rate, for example).
	However we cannot change certain things, so we modify optimize.py
	-> optimize02.py.  There is no filler.  Hidden size and type is
	fixed.

	* q: http://caffe.berkeleyvision.org/gathered/examples/finetune_flickr_style.html
	If we give provide the weights argument to the caffe train
	command, the pretrained weights will be loaded into our model,
	matching layers by name.

	Because we are predicting 20 classes instead of a 1,000, we do
	need to change the last layer in the model. Therefore, we change
	the name of the last layer from fc8 to fc8_flickr in our
	prototxt. Since there is no layer named that in the
	bvlc_reference_caffenet, that layer will begin training with
	random weights.

	We will also decrease the overall learning rate base_lr in the
	solver prototxt, but boost the blobs_lr on the newly introduced
	layer. The idea is to have the rest of the model change very
	slowly with new data, but let the new layer learn
	fast. Additionally, we set stepsize in the solver to a lower value
	than if we were training from scratch, since we’re virtually far
	along in training and therefore want the learning rate to go down
	faster. Note that we could also entirely prevent fine-tuning of
	all layers other than fc8_flickr by setting their blobs_lr to 0.

	We also need weight sharing:
	https://github.com/BVLC/caffe/pull/500.
	There are several word vectors in the feature vector and each is
	connected to the one-hot layer with the same weights.  Check out
	the Siamese example.

	We need to have two input layers go into the hidden layer: one for
	words, one for other features.  Currently the loss layer takes
	pred and label for example.  But does the inner-product-layer
	support multiple bottoms: No but we can use the concat layer to
	concatenate multiple blobs.

	Finally we need to initialize the one-hot weights from constant
	scode encodings.  We need to construct and save a network in a way
	caffe will understand.  May need to use the python interface.
	https://github.com/BVLC/caffe/issues/526
	https://github.com/BVLC/caffe/pull/455
	http://nbviewer.ipython.org/github/BVLC/caffe/blob/master/examples/net_surgery.ipynb

	* optimize.py: The best settings for 1-epoch:
	{'status': 'ok', 'loss': 0.0931981, 'weight_filler_type': 'gaussian', 'accuracy': 0.965215, 'hidden_type': 'RELU', 'weight_filler_sparse': 159.0, 'batch_size': 139.0, 'base_lr': 0.027778874681287355, 'iter': 13096.0, 'time': 158.6161048412323, 'hidden_size': 2291.0, 'momentum': 0, 'solver_type': 'ADAGRAD'}

	* analyze.pl: In general the distr of top 27 settings (best in parens):
	solver_type: ADAGRAD:27
	hidden_size(2291): 758.924785608789 */ 2.87852513396102
	hidden_type(RELU): SIGMOID:21 RELU:6
	base_lr(0.028): 0.194088790747838 */ 2.26771784380315
	momentum: 0 (all use ADAGRAD)
	filler(gauss): xavier:10 gaussian:17
	sparse(159): 120.969931661535 */ 4.51797291563553 (only used with half of gaussians)
	batch_size(139): 201.265111755204 */ 1.81666591696717


2014-11-26    <dyuret@ku.edu.tr>

	* q: What should we pursue:
	- fine-tuning
	- increasing data by replacing words with probable substitutes
	; (could test using perceptron)
	- optimizing batch training as below
	; (write our own or use hyperopt)
	- looking into optimizing deep architectures with gp, dhc, bigbatch.
	- fix beam-search parser and go with perceptron training.
	- try speeding up dynamic oracle using batch updates.
	- study the numerical optimization book.

	* 1-epoch: Measure training set loss at the end of one pass over
	the training set.  train=test=1820392

	SOLVER:
	solver_type: SGD, NESTEROV, ADAGRAD
	test_iter: 10 (test_batch_size: 182040)
	test_interval: N (at the end)
	base_lr: 1 0.5 0.2 0.1 0.05 0.02 0.01 0.005 0.002 0.001 0.0005 0.0002 0.0001
	momentum: 0 0.9 0.95 0.99 0.995 0.999 0.9995 0.9999
	weight_decay: 0
	lr_policy: "fixed" (test others later)
	display: N (at the end)
	max_iter: N
	snapshot: N
	snapshot_prefix: "/home/yuret/caffe/examples/vectorparser/data/train2"
	solver_mode: GPU

	NET:
	train.batchsize: 16, 32, 64, 128, 256, 1024 (N = ceil(182040/train.batchsize)).
	test.batchsize: 182040 (or same as train data)
	fc1.num_output: 128, .., 256, .., 512, .., 1024, .., 2048 (where .. is sqrt2 x prev)
	fc1.weight_filler: gaussian, xavier
	fc1.weight_filler.sparsity: 8, 16, 32, 64, 128, 256, 512
	fc1.blobs_lr[]: first one weight, second one bias, 1:2 gives good results.
	fc1.weight_decay[]: first one weight, second bias, 1:0 is the standard.
	relu1.type: RELU, SIGMOID, ...
	fc2.weight_filler.type: gaussian, xavier
	fc2.weight_filler.sparsity: mean number of non-zero inputs: 8, 16, 32, 64, 128, 256, 512 only supported by gaussian
	fc2.blobs_lr[]: 1:2
	fc2.weight_decay[]: 1:0

2014-11-25    <dyuret@ku.edu.tr>

	* stuck: at 0.967 with 700 RELU units (20141125.log).  More units
	help (1000 is better, got to .9677 in 20141126.log).  Compare this
	to .970 with perceptron and .968 with matlab nnet.

	We can play with:
	1. initialization: sparsity,
	2. increase units,
	3. add layers,
	4. unit type: sigmoid,
	5. learning rate
	6. momentum.
	7. batch size

	The learning rate is not slowing down enough?  More momentum may
	help?  We need to monitor training set loss.  Define it as another
	test set?  We can take snapshots at various points and find the
	ideal momentum/lr combination to discover the right schedule.  Can
	we implement momentum schedule in caffe?  Or continue from a given
	snapshot?  How can we do finetuning later?  We need to be able to
	freeze / unfreeze weights.

	* caffe-h5-conversion:
	h5create('dev.h5', '/data', size(x_dev), 'Datatype', 'single');
	h5create('dev.h5', '/label', size(y_dev), 'Datatype', 'single');
	h5write('dev.h5', '/data', single(x_dev));
	h5write('dev.h5', '/label', single(y_dev-1));

	* cpan-non-root: perl -MCPAN -Mlocal::lib -e 'CPAN::install(LMDB_File)'

	* iphython-to-python: ipython nbconvert --to=python hdf5_classification.ipynb

	* pip-non-root: pip install --user --install-option="--prefix=" -U scikit-learn

2014-11-25    <yuret@biyofiz-4-2>

	* extra-packages: yum -y install epel-release

2014-11-25    <dyuret@ku.edu.tr>

	* nnet15_test.m: Result of averaging 100 nets.  Each trained on a
	100K random batch for 180 epochs using 500 hidden units.  Skipped
	nets with loss > 0.04.  Averaged the output of the rest (88 of
	them).  The stats on the 88 nets:

	loss: n=88 m=0.032301 s=0.00184269 q=[0.0295103 0.030836 0.0321156 0.0335805 0.0391357]
	err : n=88 m=0.0596701 s=0.00229243 q=[0.0557826 0.0580602 0.0591405 0.0610797 0.065661]
	
	Averaging 88 nets
	Average loss=0.0263064
	Average error=0.0511753

	Both the error and the loss are below the minimum net but they are
	not competitive.  If we had processed 88x100K instances
	sequentially we would have reached ~0.033 error.

2014-11-23    <dyuret@ku.edu.tr>

	* nnet15_test.m: TODO

	* nnet15: averaging running.  100 networks trained on
	100K random batches with hidden=500.

	- Read momentum paper.
	- Try caffe with its three training algorithms.
	- See if caffe better with deeper nets.
	- Must optimize init and momentum.
	- Cut/paste networks to pretrain.
	- Cut/paste networks to finetune?
	- Dynamic oracle sgd.  with optimized learning params from static
	sgd.
	- Beam search parser.

	* 36000: results of the 10 hour experiments (nnet??.out):  See
	graphs on email for more detailed comparisons based on time and
	number of instances seen (which may be less for larger models).

	deverr		arch
	------		----
	0.0353359	172
	0.0342687	300
	0.0327980	400
	0.0321602	500
	0.0318349	700
	0.0321993	1000
	0.0324205	2000
	0.0317828	1000,500
	0.0321082	1500,1000,500
	0.0338262	2000,1500,1000,500
	0.0370279	2500,2000,1500,1000,500
	0.0408673	3000,2500,2000,1500,1000,500
	
2014-11-22    <dyuret@ku.edu.tr>

	* ideas: 
	- 500 seems better than 172 or 1000, try 300 and 700.
	First batch for 172: 0.0703.
	First batch for 500: 0.0721.
	300: .0696 with rng(3), 100k epoch
	400: .0703 with rng(2), 100k epoch
	700: .0753 with rng(10), 75k epoch
	
	- Try initializing with 1300-500-1300 autoencoder (do we really
	need sparsity etc?)
	1300-500-1300: vloss=0.0029 with rng(1), 20k epoch: nnet13.{m,mat,out}
	- Julia, convnet, ufldl for finetuning: could do in matlab if I
	cut/paste layers.
	- Figure out convnet, see if faster.
	- Check out julia gpu extensions: not mature yet. caffe uses cudnn.

	* nnet14.m: To really do pretraining we need to copy first layer
	from nnet13.mat, freeze it, train second layer, than fine tune both
	layers...(TODO).

2014-11-21    <dyuret@ku.edu.tr>

	* nnet_minibatch_adaptive.m: adaptive schedule.  Do not slow down
	until you haven't made progress for a while.
	
	!!! Cancelling moves that do not improve dev seems to be a bad
	idea.  Let it roam around, while shrinking the step size.  Should
	we consider improvement from the best or improvement from the
	last?

	Should I make max_fail a function of successive improvements
	preceding?  Can we get a probabilistic derivation?

	* q: less memory?  single still doesn't work in 2014a with gpu.
	Works with cpu but too slow.

	* q: faster training? could try poslin, which i think means the
	rectified linear unit.  satlin has 45 degree in [0,1], 0 before
	and 1 after.  satlins is the same for [-1,1].  Does not seem to
	give as good results.  per-epoch slightly faster.  maybe doing it
	wrong.

	* q: autoencoder?  can we implement sparsity in the objective
	function?  should I just write my own code?  implement minibatch
	in ufldl?  Use Julia?  Theano, Torch, Convnet?

	* q: nnet01, nnet02, nnet03, nnet04 running.

	* TODO: averaging (easy), two layer (easy), deeper networks (need
	to keep weights in gpu), autoencoders (need sparsity in objective
	function), fine-tuning (need to backconvert inputs to one-hot and
	embeddings to weights)..
	- Learn convnet.
	- Run experiments nnet07 and nnet08 when license allows.
	
	
2014-11-20    <dyuret@ku.edu.tr>

	* convergence: need to find best training / convergence schedule.
	Keep hidden units fixed.  Keep data size so we can do batch
	training on gpu.  Then play with minibatches and training schedule
	/ averaging etc.  First, can we adjust regularization to use
	(w-wi)^2 as in Smola's
	http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf?

	% Run batch until full convergence.
	
	run1:
	>> net=patternnet(172);
	>> net.divideFcn='';
	>> tic;net=train(net, p_trn(:,1:100000), t_trn(:,1:100000), 'useGPU', 'yes');toc;
	Epoch 25/1000, Time 12.9436, Performance 0.069445/0, Gradient 0.046517/1e-06, Validation Checks 0/6
	Epoch 100/1000, Time 45.5907, Performance 0.02715/0, Gradient 0.0095009/1e-06, Validation Checks 0/6
	Epoch 375/1000, Time 165.235, Performance 0.0038649/0, Gradient 0.00086941/1e-06, Validation Checks 0/6
	Epoch 575/1000, Time 249.1637, Performance 0.0033475/0, Gradient 5.0178e-05/1e-06, Validation Checks 0/6
	Epoch 1000/1000, Time 424.796, Performance 0.0030821/0, Gradient 0.00031762/1e-06, Validation Checks 0/6
	
	% 172 gave good result with full dataset, will probably overfit 100K.
	% Has not converged fully in 1000 epochs.
	% What is the variance?  Rerun:

	run2:
	Epoch 25/1000, Time 10.667, Performance 0.072784/0, Gradient 0.033849/1e-06, Validation Checks 0/6
	Epoch 100/1000, Time 42.5621, Performance 0.030901/0, Gradient 0.014193/1e-06, Validation Checks 0/6
	Epoch 375/1000, Time 160.0595, Performance 0.0072034/0, Gradient 0.0005233/1e-06, Validation Checks 0/6
	Epoch 575/1000, Time 240.5866, Performance 0.0068664/0, Gradient 0.00012498/1e-06, Validation Checks 0/6
	Epoch 1000/1000, Time 415.7729, Performance 0.0065358/0, Gradient 4.2006e-05/1e-06, Validation Checks 0/6
	
	% The difference of 0.003 with the first run is preserved
	throughout!  Not much improvement after epoch 375.  Difference
	between epoch 25 and epoch 1000 is 0.0664 for run1, 0.0662 for
	run2, 0.0687 for run3.

	run3:
	Epoch 25/1000, Time 10.0876, Performance 0.077054/0, Gradient 0.054168/1e-06, Validation Checks 0/6
	Epoch 100/1000, Time 42.2755, Performance 0.029656/0, Gradient 0.016406/1e-06, Validation Checks 0/6
	Epoch 375/1000, Time 159.8604, Performance 0.0091682/0, Gradient 0.00013638/1e-06, Validation Checks 0/6
	Epoch 575/1000, Time 239.8406, Performance 0.0088202/0, Gradient 0.00010235/1e-06, Validation Checks 0/6
	Epoch 1000/1000, Time 412.1733, Performance 0.0083196/0, Gradient 2.6072e-05/1e-06, Validation Checks 0/6

	run4:
	Epoch 25/1000, Time 10.3772, Performance 0.069949/0, Gradient 0.032253/1e-06, Validation Checks 0/6
	Epoch 100/1000, Time 42.9895, Performance 0.02997/0, Gradient 0.012972/1e-06, Validation Checks 0/6
	Epoch 375/1000, Time 162.2832, Performance 0.0067342/0, Gradient 0.00040182/1e-06, Validation Checks 0/6
	Epoch 575/1000, Time 244.2793, Performance 0.0063362/0, Gradient 7.9733e-05/1e-06, Validation Checks 0/6
	Epoch 1000/1000, Time 421.4627, Performance 0.0059912/0, Gradient 0.00018068/1e-06, Validation Checks 0/6

	% so if the performance at 25 is an indicator of convergence
	point, we should be able to sample a bunch of networks and keep
	the ones that do well at 25 for further training.

	>> nets=cell(1,100);
	>> for i=1:100 nets{i}=patternnet(172);nets{i}.divideFcn='';end
	>> for i=1:100 nets{i}.trainParam.epochs=25; end
	>> for i=1:100 rng(i);nets{i}=train(nets{i}, p_trn(:,1:100000), t_trn(:,1:100000), 'useGPU', 'yes');end;
	
	25 epoch Results are in [0.064734:0.33333] (80% is less than 0.1).
	Best result at nets{56}: 0.064734.
	Train the best net:
	
	>> net56=patternnet(172);net56.divideFcn='';
	>> tic;rng(56);net56=train(net56, p_trn(:,1:100000), t_trn(:,1:100000), 'useGPU', 'yes');toc;
	Epoch 25/1000, Time 10.8775, Performance 0.064734/0, Gradient 0.054358/1e-06, Validation Checks 0/6
	Epoch 100/1000, Time 43.4348, Performance 0.028196/0, Gradient 0.013142/1e-06, Validation Checks 0/6
	Epoch 375/1000, Time 161.6486, Performance 0.0063895/0, Gradient 0.0001728/1e-06, Validation Checks 0/6
	Epoch 575/1000, Time 245.8455, Performance 0.006069/0, Gradient 7.4304e-05/1e-06, Validation Checks 0/6
	Epoch 1000/1000, Time 421.6591, Performance 0.0056706/0, Gradient 0.00010329/1e-06, Validation Checks 0/6

	% Never get as lucky as the first try :(
	% Validation stop around 150.

	>> net56a=patternnet(172);
	>> net56a.trainParam.max_fail=32;
	>> tic;rng(56);net56a=train(net56a, p_trn(:,1:100000), t_trn(:,1:100000), 'useGPU', 'yes');toc;
	Epoch 185/1000, Time 80.3111, Performance 0.01148/0, Gradient 0.010684/1e-06, Validation Checks 32/32

	dev loss/err: 0.0333/0.0623
	trn loss/err: 0.0211/0.0367

	% Full 100K data for 150 epochs:
	Epoch 150/150, Time 65.2703, Performance 0.019709/0, Gradient 0.0098602/1e-06, Validation Checks 0/6
	
	dev loss/err: 0.0340/0.0624
	trn loss/err: 0.0197/0.0343

	% Now let's try minibatch with 20K random batches, ~5 batches to
	cover the whole 100K dataset, with batchepochs at 5,10,15.  Record
	best performance in 60 seconds:

	>> nnet_minibatch_timed(172, 30, 20000, 60, 56, p_trn(:,1:100000), t_trn(:,1:100000), p_dev, t_dev)
	
	bepoch	deverr	batch
	5	0.1065	19/19
	10	0.0769	15/16
	15	0.0691	13/14
	20	0.0664	11/13
	25	0.0643	12/12
	30	0.0642	11/11
	35	0.0644	10/10
	40	0.0659	8/10
	45	0.0666	8/9
	50	0.0670	7/9		
	
	% These all tend to give the best results at last epoch, i.e. have
	not converged yet.  Note that the seed is kept the same so the
	same epochs should have the same data.  Note that we are spending
	time calculating trn/dev error after every batch, which works
	against small batches.  Try more time: 120 seconds:

	>> for i=5:5:50 [a,b]=nnet_minibatch_timed(172, i, 20000, 120, 56, p_trn(:,1:100000), t_trn(:,1:100000), p_dev, t_dev);fprintf('==> %d %g %g\n', i, a, b);end

	bepoch	deverr	batch
	5	0.0868	38/38
	10	0.0650	32/32
	15	0.0607	28/29
	20	0.0589	25/25
	25	0.0594	24/24
	30	0.0605	21/22
	35	0.0606	18/20
	40	0.0619	18/18
	45	0.0633	16/17
	50	0.0649	16/16	

	% Improvement across the board.  Still no convergence.  Best beats
	batch (0.0623)!  Best bepoch shifted from 30 to 20 when we went
	from 60 secs to 120 secs.  Suggests a slowing schedule.

	* q: Should bepoch be dynamically adjusted?  Smola2014 uses as sgd
	learning rate: n*sqrt(a/(a+t)) where n is the initial rate, a is a
	decay constant and t is the iteration number.  Convnet uses
	exponential or linear decay.  Estimating a mean with minibatch
	uses 1/t.

	With 30 seconds		With 60 seconds		With 120 seconds	With 180 seconds
	
	bepoch	deverr	batch	bepoch	deverr	batch	bepoch	deverr	batch	bepoch	deverr	batch
	5	0.1271	10	5	0.1065	19/19	5	0.0868	38/38	5	0.0779	55/56
	10	0.0917	9	10	0.0769	15/16	10	0.0650	32/32	10	0.0615	46/46
	15	0.0825	7	15	0.0691	13/14	15	0.0607	28/29	15	0.0589	40/42
	20	0.0755	7	20	0.0664	11/13	20	0.0589	25/25	20	0.0589	25/39
	25	0.0736	5	25	0.0643	12/12	25	0.0594	24/24	25	0.0589	36/36
	30	0.0696	6	30	0.0642	11/11	30	0.0605	21/22	30	0.0595	23/32
	35	0.0701	5	35	0.0644	10/10	35	0.0606	18/20	35	0.0603	23/28
	40	0.0716	5	40	0.0659	8/10	40	0.0619	18/18	40	0.0604	24/27
	45	0.0711	5	45	0.0666	8/9	45	0.0633	16/17	45	0.0599	24/25
	50	0.0700	4	50	0.0670	7/9	50	0.0649	16/16	50	0.0606	23/24

	Scheduled 180 seconds:
	(1) 1000/(25+t): gives 0.0587 in 36 epochs 
	(2) 66*sqrt(2.66/(2.66+t)): gives 0.0581 in 33 epochs
	The constants should be a function of batchsize.
	Will be another thing to optimize.
	Variance goes down as 1/N where N is the batch size.
	Our step size should scale with sqrt(N)?

	Results depend on random seed again.  561 blows up, 456 gives
	0.600.
	
	* nnet1120.log: Do another large experiment: 3600 seconds full dataset.

	[a,b]=nnet_minibatch_timed(172, 100, 100000, 3600, 56, p_trn, t_trn, p_dev, t_dev)
	deverr=0.0352 achieved at 193/254 batches.
	the batchepoch sizes went from 85 to 10.
	final terr=0.01968 above perceptron's 0.0026, we could use more nodes.

	* nnet1121.log: Larger hidden layer.

	[a,b]=nnet_minibatch_timed(500, 66, 100000, 24000, 7, p_trn, t_trn, p_dev, t_dev)
	deverr=0.0327 achieved at 1990/2289 batches.
	batchepoch sizes from 56 to 2.
	final terr=0.02553.
	best at 3600secs=0.0348 vs 172=>0.0352, so is it only longer training?
	
	* nnet1121b.log: 2000 layer.
	[a,b,n]=nnet_minibatch_timed(2000, 100, 40000, 3600, 5, p_trn, t_trn, p_dev, t_dev);
	deverr=0.0386 at batch 94 in 3600 secs.
	batches are slow.  but also convergence seems worse.  
	h172 was at 0.0366 at batch 94, h500 was at 0.0365.
	could also be bad initialization.
	
	TODO: Can we optimize load/unload to gpu in minibatch?
	TODO: Can we customize regularization?  Do we need to?
	TODO: How about other optimization algs?  Try again?
	
2014-11-18    <dyuret@ku.edu.tr>

	* ufldl/softmax_exercise: Pure softmax gives trn/dev accuracy of
	0.1110/0.1131 in 100 epochs, 441 seconds on balina.  The only
	adjustable parameter is lambda (1e-4).  We need to optimize
	lambda.  We could also use liblinear, or modify matlab perceptron.
	
	lambda=0.001 trnerr=0.132451 deverr=0.134498
	lambda=1e-05 trnerr=0.10041 deverr=0.104042 483.454480 seconds.
	lambda=1e-06 trnerr=0.0967901 deverr=0.100229 467.571426 seconds.
	lambda=1e-07 trnerr=0.0963716 deverr=0.100242 486.261751 seconds.
	lambda=0 trnerr=0.096822 deverr=0.100633 482.748054 seconds.
	
	lambda multiplies the regularization penalty:
	cost = (-1/numCases) * sum(log(sum(groundTruth .* h))) + ...
	       (lambda / 2) * norm(theta, 'fro')^2;

	liblinear takes 151 secs to train with dev data.  Not competitive.

	We can use patternnet([]) which has no hidden layer:
	>> nnet_minibatch_fn([], 40, 100, 200000, 1, p_trn, t_trn, p_dev, t_dev);
	best: ==> batch=83 tloss=0.157946 terr=0.092615 vloss=0.164955 verr=0.0979644
	It drops below 0.10 at batch=44.
	Each batch is a 200K sample for 40 epochs, takes 12 seconds.
	Seems to spend 10 secs extra / epoch for gpu transfers?
	So roughly the same amount of time to reach the same accuracy.
	How can it do better than softmax I don't know.
	
	net1119: try nnet with full data on balina.
	>> net=patternnet([]);
	>> net=train(net, p_trn, t_trn);
	31 seconds per epoch.
	Converged in 973 epochs.
	trn/dev = 0.0902/0.0946
	
	Once ready, try adding the random layer:
	With 2000 hidden units verr=0.0965, not that much better than softmax.
	The weights were standard normal, no biases, tansig transfer function.
	With 180 hidden units verr=0.20 much worse.
	Is ELM a big lie?
	Do I need some other weight distribution, bias, transfer function, final layer optimization?
	They have some ELM code we can try...
	Also this paper: http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-NC-2006.pdf
	They leave the output layer linear and solve with pseudo-inverse (pinv).
	The weights are rand*2-1, the biases are rand.
	Downloaded code for: Sample2 classification: elm('diabetes_train', 'diabetes_test', 1, 20, 'sig')
	From: http://www.ntu.edu.sg/home/egbhuang/elm_random_hidden_nodes.html
	All depends on the speed of pinv.
	done: check out their claims and code.
	
	* stat: test 3% model on dev data to see corresponding head accuracy.

		stat_pct		move_pct	      	head_pct	      	word_pct
	epoch	trn	dev	tst	trn	dev	tst   	trn	dev	tst   	trn	dev	tst   
	perc1	0.0142	0.0329	0.0346	0.0203	0.0409	0.0423	0.0590	0.1081	0.1134	0.0509	0.0946	0.0983
	perc6	0.0026	0.0302	0.0315	0.0044	0.0376	0.0392	0.0173	0.0993	0.1058	0.0156	0.0872	0.0916
	dyn17	0.0069	0.0364	0.0379	0.0019	0.0337	0.0340	0.0114	0.0898	0.0918	0.0110	0.0799	0.0819
	

	* run/nnet_minibatch_random3.m: Try more hidden units.  Single
	layer, [100:2000] range.  Batchsize 40000.  Best deverr with
	[10:200] range was: (0.0373663 172 38) with batchsize 100K.
	Running on ilac.  Still running the same number of batches may
	be unfair.  Best result: (0.038004 1766 42).
	
2014-11-17    <dyuret@ku.edu.tr>

	* crossentropy: The softmax experiment finished
	(net-90-30.mat,log).  The trn/dev combination was used (no
	minibatch).  Convergence in 403 iterations, 84109 secs on balina.
	
	0-1 error rate: trn/dev/tst: 0.0326/0.0378/0.0396.

	We are within .5 percent of the perceptron (1-epoch, see below for multi epoch): 
	trn/dev/tst: 0.0175/0.0329/0.0346.

	Best dev err from random search so far:
	==> best_err	0.0373663	172	38
	Single layer with 172 nodes and 38 batch-epochs per 100K random batch.

	Best dev err from two-layer random search:
	==> best_err	0.0387198	83	96	22
	i.e. two layers with 83 and 96 and 22 batch-epochs per 100K random batch.

	Try random network for optimization...
	2014a matlab works with single (in cpu, how about gpu?, k40?).

	* run141117: run perceptron until convergence:
	==> epoch=1 trnerr=0.0175111 deverr=0.03285
	==> epoch=2 trnerr=0.00896236 deverr=0.0309759
	==> epoch=3 trnerr=0.00558836 deverr=0.0305203
	==> epoch=4 trnerr=0.004031 deverr=0.03026
	==> epoch=5 trnerr=0.00310592 deverr=0.0302861
	==> epoch=6 trnerr=0.00255055 deverr=0.030247	***
	==> epoch=7 trnerr=0.00219074 deverr=0.0303251
	==> epoch=8 trnerr=0.00193145 deverr=0.0303251
	==> epoch=9 trnerr=0.00173259 deverr=0.0304813
	==> epoch=10 trnerr=0.00158098 deverr=0.0304813
	==> epoch=11 trnerr=0.00146397 deverr=0.0305073
	
	
2014-11-16    <dyuret@ku.edu.tr>

	* run/nnet_minibatch_random.m: Do random search of the
	hyperparameter space.  Single layer at biyofiz-4-0, two layer at
	biyofiz-4-2.  Range [10-200] for all params.  Also search the
	batchepochs.

	Results at nnet-minibatch-random.out and
	nnet-minibatch-random2.out.  The best results:

	deverr		hidden1	hidden2	batchepoch
	0.0373663	172	-	38
	0.0387198	83	96	22

	But: did it converge in 100 batches?  how much of this is noise?
	how about larger hidden layers?
	
	- how do we optimize:
	- batch-epochs
	- layers
	- units per layer
	- is this the right way to train?
	- is this the right objective function?
	- does averaging help?
	- does pre-training help?
	- going beyond 200?
	- can we optimize structure with random networks?  (ELM)
	
	
2014-11-15    <dyuret@ku.edu.tr>

	* run/nnet_minibatch.m: We are able to get dev cost to 0.0298138
	using 100K minibatches and the whole dataset with patternnet(25).
	
	==> epoch=1 batch=19 trn=0.030597 val=0.0304265
	==> epoch=2 batch=19 trn=0.0305259 val=0.0301641
	==> epoch=3 batch=19 trn=0.0304675 val=0.0300263
	==> epoch=4 batch=19 trn=0.0298717 val=0.0299105
	==> epoch=5 batch=19 trn=0.0300261 val=0.0298338
	==> epoch=6 batch=19 trn=0.030053 val=0.0298138
	==> epoch=7 batch=19 trn=0.030053 val=0.0298138	***
	
	* q: what is the trn/dev/test accuracy?
	0.0298 dev loss corresponds to 0.0574 prediction error
	0.0289 trn loss corresponds to 0.0560 prediction error
	
	This is consistent with out-of-sample test set:
	tst_out=net(p_tst);
	mean((tst_out(:)-t_tst(:)).^2) % 0.0301
	[~,tst_max]=max(tst_out);
	mean(tst_max ~= double(y_tst)) % 0.0587
	
	* q: less epochs per batch in the beginning help?
	
	Restrict each batch to max 10 epochs: net.trainParam.epochs=10;
	Seems to get stuck in a bad minimum:
	==> epoch=1 batch=19 trn=0.0364139 val=0.0372686
	==> epoch=2 batch=19 trn=0.0343426 val=0.034399
	==> epoch=3 batch=19 trn=0.0342088 val=0.0342703
	==> epoch=4 batch=19 trn=0.0340899 val=0.0341482
	
	net.trainParam.epochs=20;
	==> epoch=1 batch=19 trn=0.0322025 val=0.0322322
	==> epoch=2 batch=19 trn=0.0320921 val=0.0321144

	net.trainParam.epochs=40; % found sweet spot
	==> epoch=1 batch=19 trn=0.0295118 val=0.029725
	==> epoch=2 batch=19 trn=0.0292515 val=0.0294362	***
	
	net.trainParam.epochs=50; % very fickle and not convex (comp with full)
	==> epoch=1 batch=19 trn=0.0314811 val=0.0314628
	==> epoch=2 batch=19 trn=0.03133 val=0.0313066
	
	net.trainParam.epochs=30; % this is why i dont like nnets
	==> epoch=1 batch=19 trn=0.0307661 val=0.0311372
	==> epoch=2 batch=19 trn=0.0306145 val=0.0309189
	
	% we can't do regularization as given in the smola paper because
	it measures distance from previous point, not 0.
	
	* q: more hidden units/layers help? (prev results were with 25)
	% keeping net.trainParam.epochs=40; may not be ideal...

	net=patternnet(30);
	==> epoch=1 batch=19 trn=0.0297247 val=0.0293575
	==> epoch=2 batch=19 trn=0.0298361 val=0.029128	***
	
	net=patternnet(40);
	==> epoch=1 batch=19 trn=0.0296167 val=0.0286256
	==> epoch=2 batch=19 trn=0.0295002 val=0.0284329	***
	
	net=patternnet(50);	% this is good news, never worked for small data
	==> epoch=1 batch=19 trn=0.0281019 val=0.0270577
	==> epoch=2 batch=19 trn=0.0280043 val=0.0268897	***
	
	net=patternnet(60);
	==> epoch=1 batch=19 trn=0.0303423 val=0.0294406

	net=patternnet(70);	% not convex!
	==> epoch=1 batch=19 trn=0.0282054 val=0.0277522

	net=patternnet(80);
	==> epoch=1 batch=19 trn=0.0279242 val=0.0269423

	net=patternnet(90);	% gets stuck with rng(1), restart rng(2)
	==> epoch=1 batch=19 trn=0.027014 val=0.026036
	==> epoch=2 batch=19 trn=0.0271728 val=0.0259386
	==> epoch=3 batch=19 trn=0.0271773 val=0.0259049	
	==> epoch=4 batch=6 trn=0.0242783 val=0.0258576	***

	Error at this point:
	trn/dev/tst: 0.0485/0.0500/0.0514 (loss: 0.0253/0.0259/0.0265)
	Compare to: 0.0175/0.0329/0.0346 of 1-epoch perceptron

	net=patternnet(120);
	==> epoch=1 batch=19 trn=0.0284414 val=0.0271286 (rng(2))
	==> epoch=1 batch=19 trn=0.0275835 val=0.025687  (rng(1))  ***
	==> epoch=1 batch=19 trn=0.0275591 val=0.0260256 (rng(42))
	==> epoch=1 batch=19 trn=0.0282292 val=0.0270393 (rng(3))

	net=patternnet([90 60]);
	==> epoch=1 batch=19 trn=0.0280835 val=0.0270758 (rng(2))
	==> epoch=1 batch=19 trn=0.0275355 val=0.0264942 (rng(1))

	net=patternnet([120 30]);
	==> epoch=1 batch=19 trn=0.0281179 val=0.0277087 (rng(2))
	==> epoch=1 batch=19 trn=0.0281958 val=0.0268038 (rng(1))

	net=patternnet(150);
	==> epoch=1 batch=19 trn=0.027346 val=0.0264134 (rng(1))
	==> epoch=1 batch=19 trn=0.0278695 val=0.0269375 (rng(2))
	
	net=patternnet([90 30 30]);
	==> epoch=1 batch=19 trn=0.0269697 val=0.0261999 (rng(2))
	==> epoch=1 batch=19 trn=0.0270478 val=0.0257561 (rng(3))
	
	net=patternnet(180); % out of memory, reduce batchsize?
	==> epoch=1 batch=19 trn=0.0286053 val=0.0273498 (rng(42))
	
	net=patternnet([90 30]);
	==> epoch=1 batch=19 trn=0.0269882 val=0.0255406 (rng(1)) ***
	==> epoch=1 batch=19 trn=0.0261928 val=0.0254736 (rng(2)) ***

	How about if we don't use validation for the first epoch:
	Yes, that works!
	We just weren't learning much from the last 80% of train data!
	
	net=patternnet([90 30]);
	==> epoch=2 batch=1 trn=0.0178031 val=0.0237843
	
	Can we just switch test and validation?
	Find the minimum.  Then play with batch-epoch=40.
	That's like step size, maybe should reduce gradually.
	We need to reoptimize layers and hidden units.
	
	net=patternnet([90 30]);
	==> epoch=2 batch=1 trn=0.0150816 val=NaN tst=0.0246039
	==> epoch=2 batch=11 trn=0.0127859 val=NaN tst=0.0223863
	==> epoch=3 batch=1 trn=0.0133801 val=NaN tst=0.0236835
	==> epoch=4 batch=1 trn=0.0120749 val=NaN tst=0.0231974
	==> epoch=5 batch=1 trn=0.0114838 val=NaN tst=0.0235363	
	
	The last small epoch really screws us up (0.0268).  Pick epoch
	data randomly?  Find a way to adjust batch-epoch number.  Read up
	on SGD theory.
	
	net=patternnet([90 30]); with batch-epochs=10
	==> epoch=2 batch=1 trn=0.0327633 val=NaN tst=0.0333506
	==> epoch=3 batch=1 trn=0.0278755 val=NaN tst=0.0286027
	==> epoch=4 batch=1 trn=0.0255721 val=NaN tst=0.0263425
	==> epoch=5 batch=1 trn=0.0242439 val=NaN tst=0.0250586
	==> epoch=6 batch=1 trn=0.0228968 val=NaN tst=0.0238428
	==> epoch=7 batch=1 trn=0.0220123 val=NaN tst=0.0231609
	==> epoch=8 batch=1 trn=0.0214357 val=NaN tst=0.0224017
	==> epoch=9 batch=1 trn=0.0208732 val=NaN tst=0.0219181
	==> epoch=10 batch=1 trn=0.0206029 val=NaN tst=0.0217161
	==> epoch=10 batch=12 trn=0.0179716 val=NaN tst=0.0213889 ***

	Random sampling with batch-epochs=40 (each old epoch =~ 20xnew batch)
	loss measures mse, err measures 0-1 error rate.
	==> batch=20 tloss=0.0137094 terr=0.0235 vloss=0.0237919 verr=0.0461645
	==> batch=33 tloss=0.0113816 terr=0.01808 vloss=0.0232309 verr=0.0434834
	==> batch=40 tloss=0.0110373 terr=0.01823 vloss=0.0242332 verr=0.0462035
	==> batch=60 tloss=0.010301 terr=0.01627 vloss=0.0229451 verr=0.0427805
	==> batch=78 tloss=0.0102041 terr=0.01597 vloss=0.0228 verr=0.0418304
	==> batch=80 tloss=0.0101979 terr=0.0159 vloss=0.0229248 verr=0.042273
	==> batch=100 tloss=0.00889762 terr=0.01379 vloss=0.0237851 verr=0.043184
	
	Random sampling with batch-epochs=25 (each old epoch =~ 20xnew batch)
	==> batch=91 tloss=0.0127161 terr=0.02168 vloss=0.0206887 verr=0.0396699
	
	Random sampling with batch-epochs=10 (each old epoch =~ 20xnew batch)
	==> batch=100 tloss=0.0209893 terr=0.03985 vloss=0.0232879 verr=0.0454356

	
	* q: Can a single epoch of perceptron be this good?  What if we run it multiple epochs?
	
	>> m0 = struct('step', 1e5, 'batchsize', 1e3, 'kerparam', struct('type', 'rbf', 'gamma', 0.333140404105682));
	>> m1 = perceptron(x_trn, single(y_trn), m0);
	Using X batchsize=1000
	nd=1326 nx=1820392 nc=3 ns=0
	inst	err	nsv	batch	time	mem
	1820392	5.2815	96646	392	251.97	3.91e+08
	Saving 96326 unique SV.
	>> [~, trn_out, trn_aer] = perceptron(x_trn, single(y_trn), m1, 'update', 0);
	0.0175 % 0.0142 was on a 1000 sample
	>> [~, dev_out, dev_aer] = perceptron(x_dev, single(y_dev), m1, 'update', 0);
	0.0329 % confirmed manually as well
	>> [~, tst_out, tst_aer] = perceptron(x_tst, single(y_tst), m1, 'update', 0);
	0.0346 % so it is real!
	>> m2 = perceptron(x_trn, single(y_trn), m1);
	1820392	2.0441	133536	392	488.12	3.67e+08
	Saving 122579 unique SV.
	>> [~,dev_out,dev_aer] = perceptron(x_dev, single(y_dev), m2, 'update', 0);
	0.0310
	>> [~, tst_out, tst_aer] = perceptron(x_tst, single(y_tst), m2, 'update', 0);
	0.0328

	* q:  need better loss function (crossentropy)?

	The 2014a version of matlab has softmax final layer with
	crossentropy loss function as default for patternnet.  Here is a comparison:

	time	epoch	gpu	data	trnfcn	hidden	trn/val/tst loss
	327	127	n	dev	scg	10	0.0292/0.0424/0.0419 (2013a)
	149	83	n	dev	scg	10	0.1208/0.1558/0.1525 (2014a)

	Of course loss functions are incomparable.  We need to look at accuracy:
	dev/tst 0.0749/0.0904 for 2014a
	dev/tst 0.0597/0.0814 for 2013a

	Is this an optimization problem?  Bigger network?
	
	* q: So figure out how to train bigger networks?
	
	- remove validation set always taking n steps?
	- remove preprocessing and use logsig?
	- randomly sample each batch?
	- average each batch model?

2014-11-14    <dyuret@ku.edu.tr>

	* matlab-nnet:
	
	- there are default preprocessing fns, be careful.
	net.inputs{1}.processFcns, net.outputs{i}.processFcns
	we can remove them, but output should be logsig.
	
	- it automatically divides trn/dev/tst.
	divideFcn, divideParam, divideMode
	use rng(1) for repeatability.
	use net.divideFcn='' to cancel data division
	
	- it uses scg by default: ok
	trainFcn, trainParam	
	
	- warning: it may be using the wrong performFcn: 'mse'
	2013 does not have crossentropy.
	
	- the default transfer function is tansig (i.e.[-1,+1], not logsig=[0,1])
	help nntransfer, net.layers{i}.transferFcn
	
	- it may be using regularization?  not by default
	net.performFcn, net.performParam.regularization=0.
	btw, it applies regularization to biases as well.
	there is also normalization which is for multiple outputs with diff scales.
	
	- more properties at:
	http://www.mathworks.com/help/nnet/ug/neural-network-object-properties.html
	http://www.mathworks.com/help/nnet/ug/neural-network-subobject-properties.html
	
	- it is not using a softmax layer.
	- it cannot handle single with or without gpu.
	- it cannot use trainlm,br on gpu. bfg gets stuck.
	
	Default commands used:
	>> load 'mat/archybrid_conllWSJToken_wikipedia2MUNK-100_fv021a_xy.mat'
	>> t_dev = (full(sparse(double(y_dev), 1:numel(y_dev), 1)));
	>> p_dev = double(x_dev);
	>> net=patternnet;
	>> rng(1)
	>> [a,b]=train(net, p_dev, t_dev, 'useGPU', 'yes');
	
	time	epoch	gpu	data	trnfcn	hidden	trn/val/tst
	327	127	n	dev	scg	10	0.0292/0.0424/0.0419
	29	127	y	dev	scg	10	0.0292/0.0424/0.0419
	16.8	138	y	dev	rp	10	0.0369/0.0516/0.0507
	36.2	109	y	dev	cgb	10	0.0305/0.0441/0.0438
	35.4	107	y	dev	cgf	10	0.0355/0.0487/0.0484
	35.3	104	y	dev	cgp	10	0.0317/0.0442/0.0437
	102.3	329	y	dev	oss	10	0.0276/0.0403/0.0389
	21.4	176	y	dev	gdx	10	0.0625/0.0651/0.0659
	120.4	1000	y	dev	gdm	10	0.1439/0.1433/0.1449
	19	81	y	dev	scg	20	0.0277/0.0387/0.0399
	17.9	74	y	dev	scg	40	0.1178/0.1213/0.1231 ? convergence
	23.6	102	y	dev	scg	30	0.0226/0.0384/0.0384
	27.3	121	y	dev	scg	5	0.0385/0.0453/0.0443
	20.9	88	y	dev	scg	35	0.0265/0.0403/0.0408
	31.6	132	y	dev	scg	35	0.0214/0.0397/0.0412 ? min_grad=1e-10 max_fail=32
	25.4	110	y	dev	scg	25	0.0227/0.0379/0.0381
	23.9	100	y	tst	scg	25	0.0287/0.0379/0.0375
	30.2	126	y	tst	scg	25	0.0287/0.0379/0.0375

	Kernel perceptron after one epoch with full train:
	trn=0.0142, dev=0.0329, tst=0.0346
	
	25 hidden units seems close to optimal.
	
	dev:76834, tst:108536, trn:1820392 x1326
	
	trn gives memory error as expected. 19.3GB.  Since we don't have
	single available, we need to split train and do minibatch
	training.  Try 200K batches: 51 secs/128 epochs.  250K batches fail.
	
	time	epoch	gpu	data	trnfcn	hidden	trn/val/tst
	48.4	126	y	trn200	scg	25	0.0265/0.0334/0.0326
	63.4	160	y	trn200	scg	30	0.0234/0.0322/0.0338
	65.0	162	y	trn200	scg	35	0.0254/0.0344/0.0347

	Hidden units are up +5 when data doubled.
	
	This network has 0.08 error on real test data.  So what does
	0.0347 refer to?  Is it some other performance metric?  It seems
	it refers to the MSE on the 3xN matrix of answers.  Maybe this is
	not so surprizing as we are only using 10% of the training data.

	In the new version crossentropy is a possible performFcn but not
	in our version.  The first arg can be an array for multiple hidden
	layers.

	time	epoch	gpu	data	trnfcn	hidden	trn/val/tst
	61.6	123	y	trn200	scg	30,10	0.0242/0.0319/0.0318
	58.8	116	y	trn200	scg	30,20	0.0249/0.0328/0.0328
	72.8	146	y	trn200	scg	20,10	0.0246/0.0322/0.0318
	82.4	166	y	trn200	scg	30,5	0.0265/0.0336/0.0330
	60.0	118	y	trn200	scg	35,10	0.0258/0.0338/0.0340
	78.2	156	y	trn200	scg	25,10	0.0248/0.0316/0.0315

	Can we just remove the processFcns from the last net:
	>> net.inputs{1}.processFcns={};
	>> net.outputs{3}.processFcns={};
	>> rng(1);
	>> [a,b]=train(net, p_trn1, t_trn1, 'useGPU', 'yes');
	
	Gets stuck. Wrong sigmoid type incompatible with 0/1 output?
	Let's try on simpler architecture:
	
	>> net=patternnet(25);
	>> rng(1)
	>> [a,b]=train(net, p_trn1, t_trn1, 'useGPU', 'yes');
	0.0265/0.0334/0.0326	
	>> net.inputs{1}.processFcns={}
 	>> rng(1)
	>> [a,b]=train(net, p_trn1, t_trn1, 'useGPU', 'yes');
	0.0251/0.0312/0.0312
	>> net.outputs{2}.processFcns={};
	>> rng(1)
	>> [a,b]=train(net, p_trn1, t_trn1, 'useGPU', 'yes');
	0.0326/0.0367/0.0357 (1000 epochs)

	Having a hard time probably because of tansig:

	>> net.layers{2}.transferFcn='logsig'
	>> rng(1)
	>> [a,b]=train(net, p_trn1, t_trn1, 'useGPU', 'yes');
	0.0270/0.0322/0.0322

	Let's make it all logsig:

	>> net.layers{1}.transferFcn='logsig';
	>> rng(1)
	>> [a,b]=train(net, p_trn1, t_trn1, 'useGPU', 'yes');
	0.0249/0.0315/0.0312

	So we can use logsig everywhere and cancel pre/postprocessing.
	
	DONE:
	+ check the question marks above.
	+ try minibatch.
	+ try sophisticated optimizers.
	+ find cross-entropy cost, softmax layer, or write it.

	
	
2014-10-27  Deniz Yuret  <dyuret@ku.edu.tr>

$$ilac	* run_static_dynamic_1023_archybrid_conllWSJToken_wikipedia2MUNK_100_fv021a_0_333140404105682_ilac.out:
	First two epochs identical to 1017 run.  For conll07 use the 2014-10-10 result.
	
		stat_pct		move_pct	      	head_pct	      	word_pct
	epoch	trn	dev	tst	trn	dev	tst   	trn	dev	tst   	trn	dev	tst   
	1	0.0142	0.0329	0.0346	0.0203	0.0409	0.0423	0.0590	0.1081	0.1134	0.0509	0.0946	0.0983
	2	0.0138	0.0376	0.0389	0.0136	0.0373	0.0374	0.0400	0.0994	0.1023	0.0359	0.0881	0.0912
	3	0.0102	0.0364	0.0380	0.0081	0.0351	0.0355	0.0269	0.0934	0.0969	0.0246	0.0832	0.0860
	4	0.0088	0.0361	0.0378	0.0058	0.0348	0.0352	0.0209	0.0921	0.0955	0.0193	0.0819	0.0845
	5	0.0076	0.0360	0.0376	0.0043	0.0342	0.0349	0.0174	0.0910	0.0950	0.0162	0.0810	0.0839
	6	0.0071	0.0359	0.0375	0.0033	0.0341	0.0347	0.0150	0.0907	0.0940	0.0142	0.0807	0.0835
	7	0.0068	0.0359	0.0374	0.0025	0.0336	0.0346	0.0131	0.0894	0.0940	0.0123	0.0795	0.0835
	8	0.0067	0.0360	0.0374	0.0023	0.0336	0.0343	0.0125	0.0892	0.0934	0.0118	0.0792	0.0832
	9	0.0068	0.0359	0.0377	0.0023	0.0333	0.0342	0.0125	0.0888	0.0935	0.0119	0.0789	0.0833
	10	0.0068	0.0358	0.0376	0.0021	0.0333	0.0342	0.0121	0.0885	0.0933	0.0116	0.0786	0.0830
	11	0.0068	0.0360	0.0377	0.0021	0.0335	0.0341	0.0120	0.0891	0.0928	0.0116	0.0792	0.0827
	12	0.0068	0.0361	0.0378	0.0021	0.0335	0.0341	0.0119	0.0889	0.0925	0.0116	0.0790	0.0824
	13	0.0068	0.0359	0.0379	0.0020	0.0332	0.0339	0.0117	0.0884	0.0920	0.0114	0.0785	0.0821
	14	0.0069	0.0359	0.0380	0.0020	0.0333	0.0340	0.0116	0.0888	0.0921	0.0112	0.0788	0.0823
	15	0.0068	0.0360	0.0380	0.0019	0.0334	0.0340	0.0115	0.0888	0.0920	0.0111	0.0787	0.0820
	16	0.0068	0.0364	0.0380	0.0019	0.0338	0.0340	0.0116	0.0899	0.0919	0.0113	0.0798	0.0820
	17	0.0069	0.0364	0.0379	0.0019	0.0337	0.0340	0.0114	0.0898	0.0918	0.0110	0.0799	0.0819
	18	0.0069	0.0363	0.0380	0.0018	0.0336	0.0341	0.0115	0.0894	0.0922	0.0110	0.0795	0.0823
	19	0.0069	0.0365	0.0380	0.0018	0.0336	0.0341	0.0115	0.0893	0.0921	0.0109	0.0795	0.0822
	20	0.0069	0.0364	0.0380	0.0017	0.0337	0.0342	0.0113	0.0894	0.0927	0.0108	0.0796	0.0827
	

2014-10-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* beta-eval: If we eval the 1017 run using beta instead of beta2.
	Note that head accuracy below 90% without averaging.

		stat_pct		move_pct	      	head_pct	      
	epoch	trn	dev	tst	trn	dev	tst   	trn	dev	tst   
	1	0.0245	0.0423	0.0447	0.0349	0.0529	0.0538	0.0951	0.1393	0.1442
	2	0.0247	0.0482	0.0517	0.0255	0.0467	0.0488	0.0764	0.1285	0.1347
	3	0.0180	0.0457	0.0473	0.0168	0.0435	0.0436	0.0492	0.1128	0.1161
	4	0.0141	0.0438	0.0445	0.0119	0.0412	0.0417	0.0375	0.1106	0.1133
	5	0.0128	0.0428	0.0446	0.0097	0.0400	0.0410	0.0317	0.1065	0.1106
	6	0.0111	0.0412	0.0431	0.0073	0.0395	0.0395	0.0263	0.1058	0.1062
	7	0.0100	0.0416	0.0433	0.0064	0.0390	0.0396	0.0232	0.1029	0.1065
	8	0.0089	0.0412	0.0423	0.0067	0.0400	0.0394	0.0231	0.1049	0.1043
	9	0.0091	0.0410	0.0421	0.0049	0.0390	0.0388	0.0187	0.1031	0.1032
	10	0.0082	0.0410	0.0416	0.0043	0.0390	0.0381	0.0174	0.1033	0.1021
	11	0.0079	0.0406	0.0413	0.0044	0.0387	0.0383	0.0179	0.1021	0.1027
	12	0.0080	0.0396	0.0416	0.0029	0.0377	0.0369	0.0143	0.0999	0.1005

2014-10-23  Deniz Yuret  <dyuret@ku.edu.tr>

	* conllWSJ-debug: problem is with beta2 in single precision.
	- cache.rvec was single in log file.  should be double?
	- replicated 5->6 on biyofiz-4-3.  same bad result?
	- overflow in beta2?  m6 is better in beta.  m5 is better in beta2!
	- So let us eval with beta rather than beta2.
	- Then either change the way beta2 is computed or use double.

	stat_pct with beta	stat_pct with beta2
	epoch	trn	dev	trn	dev	tst   
	1	0.02451	0.04223	0.0142	0.0329	0.0346
	2	0.02466	0.04817	0.0138	0.0376	0.0389
	3	0.01802	0.04570	0.0102	0.0364	0.0380
	4	0.01415	0.04380	0.0088	0.0361	0.0379
	5	0.01282	0.04280	0.0083	0.0367	0.0381
	6	0.01109	0.04118	0.0264	0.0578	0.0592
	7	0.01002	0.04158	0.0470	0.0806	0.0819
	8	0.00893	0.04119	0.0924	0.1272	0.1277
	9	0.00901	0.04104	0.1231	0.1596	0.1611
	10	0.00822	0.04101	0.1489	0.1865	0.1886
	11	0.00794	0.04058	0.3828	0.4147	0.4139
	12	0.00801	0.03962	0.4992	0.5284	0.5252
	
$$b43	- evaluate all with beta...
	- restart: run_static_dynamic_1023_archybrid_conllWSJToken_wikipedia2MUNK_100_fv021a_0_333140404105682_ilac.out
$$b42	- do the 5->6 experiment with new vectorparser: gave error, just wait for ilac...
	
2014-10-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* conllWSJ: seems buggy after epoch 5.
	
$$ilac	run_static_dynamic_1017_archybrid_conllWSJToken_wikipedia2MUNK_100_fv021a_0_333140404105682_.out
	head_pct
	epoch	trn	dev	tst	nsv
	1	0.0590	0.1081	0.1134	96326
	2	0.0400	0.0994	0.1023	145946
	3	0.0269	0.0934	0.0969	176485
	4	0.0210	0.0919	0.0959	196535
	5	0.0197	0.0947	0.0963	210625
	6	0.0954	0.1750	0.1749	220682
	7	0.1857	0.2543	0.2524	228294
	8	0.3307	0.3836	0.3751
	9	0.4114	0.4529	0.4478
	10	0.4724	0.5165	0.5084
	Speed: 0.92x/s, 12h/epoch
	
	
	* conll07: Not better than trn/dev split?  Did we optimize
	features and gamma on trn/tst split?  Yes.  stat_pct for epoch 1
	is 0.0419.  Used to be 0.0459 with trn/dev.  head_pct is also
	better, it was 0.1455 for trn/dev, it starts at 0.1411 now.  All
	this tells us that the performance at the end of the first
	(static) epoch is not necessarily predictive of the eventual
	performance, which makes it difficult to optimize features /
	gamma...
	
$$ilac	run_static_dynamic_1016_archybrid_conll07EnglishToken_wikipedia2MUNK_100_fv022b_0_376747095368119_.out
	head_pct
	epoch	trn	dev	tst	nsv
	1	0.0696	0.0696	0.1411	57661
	2	0.0454	0.0454	0.1361	87989
	3	0.0254	0.0254	0.1263	105806
	4	0.0176	0.0176	0.1259	116869
	5	0.0138	0.0138	0.1263	123934
	6	0.0114	0.0114	0.1245	128757
	7	0.0108	0.0108	0.1255
	8	0.0100	0.0100	0.1273
	9	0.0095	0.0095	0.1279
	10	0.0095	0.0095	0.1269
	11	0.0092	0.0092	0.1301
	12	0.0091	0.0091	0.1317
`	13	0.0088	0.0088	0.1285
	14	0.0086	0.0086	0.1271
	15	0.0086	0.0086	0.1283
	16	0.0084	0.0084	0.1277
	17	0.0084	0.0084	0.1301
	18	0.0083	0.0083	0.1315
	19	0.0083	0.0083	0.1303
	20	0.0082	0.0082	0.1295
	Speed 1.2x/s, 4.3h/epoch

	
2014-10-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* features: only 11 common, marked by '+' below.
	
	>> c7 = load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_rbf376678_1014_cache.mat');
	>> c8 = load ('archybrid_conllWSJToken_wikipedia2MUNK-100_rbf333140_cache.mat');
	>> [c71, c72, c73] = run_sort_bestfeats(c7.cache);
	>> [c81, c82, c83] = run_sort_bestfeats(c8.cache);
	

	fv022b/conll07				fv021a/conllWSJ
	
	+[-208]    '-n0w'       '[0 0 4]'   	+[-157]    '-s0w'       '[-1 0 4]'  
	+[-152]    '-s0w'       '[-1 0 4]'  	+[-141]    '-n0w'       '[0 0 4]'   
	+[ -69]    '-s1w'       '[-2 0 4]'  	+[ -70]    '-s1w'       '[-2 0 4]'  
	+[ -68]    '-n1w'       '[1 0 4]'   	+[ -55]    '-n1w'       '[1 0 4]'   
	+[ -60]    '-n0l1w'     '[0 -1 4]'  	-[ -21]    '-s2w'       '[-3 0 4]'  
	+[ -50]    '-n1c'       '[1 0 -4]'  	+[ -21]    '-s0l1w'     '[-1 -1 4]' 
	+[ -40]    '-s0c'       '[-1 0 -4]' 	+[ -21]    '-s0c'       '[-1 0 -4]' 
	+[ -34]    '-s0r1w'     '[-1 1 4]'  	+[ -18]    '-n0l1w'     '[0 -1 4]'  
	+[ -33]    '-n0c'       '[0 0 -4]'  	+[ -18]    '-n1c'       '[1 0 -4]'  
	-[ -32]    '-s0r<'      '[-1 0 6]'  	-[ -16]    '-s1r<'      '[-2 0 6]'  
	-[ -31]    '-s1d>'      '[-2 0 7]'  	-[ -16]    '-s0r='      '[-1 0 2]'  
	+[ -31]    '-s0l1w'     '[-1 -1 4]' 	+[ -15]    '-n0c'       '[0 0 -4]'  
	-[ -28]    '-s1l1c'     '[-2 -1 -4]'	-[ -14]    '-n0l1-'     '[0 -1 -1]' 
	-[ -28]    '-s0l1-'     '[-1 -1 -1]'	-[ -13]    '-s2r1l='    '[-3 1 -2]' 
	+[ -28]    '-n0l1c'     '[0 -1 -4]' 	+[ -13]    '-s0r1w'     '[-1 1 4]'  
	-[ -28]    '-s2aw'      '[-3 0 8]'  	-[ -11]    '-s1r1l='    '[-2 1 -2]' 
	-[ -25]    '-n0l='      '[0 0 -2]'  	-[ -11]    '-s0l1r='    '[-1 -1 2]' 
	-[ -22]    '-s1r1+'     '[-2 1 1]'  	-[  -9]    '-s0l1c'     '[-1 -1 -4]'
	-[ -20]    '-s1c'       '[-2 0 -4]' 	-[  -9]    '-s0-'       '[-1 0 -1]' 
	-[ -19]    '-s0h-'      '[-1 0 -9]' 	-[  -7]    '-s0r1r='    '[-1 1 2]'  
	-[ -11]    '-s1h-'      '[-2 0 -9]' 	+[  -7]    '-n0l1c'     '[0 -1 -4]' 
	-[  -4]    '-s1l1r>'    '[-2 -1 5]' 
	
	
2014-10-16  Deniz Yuret  <dyuret@ku.edu.tr>

	* stop: run_static_dynamic_1014_archybrid_conllWSJToken_wikipedia2MUNK_100_fv021a_0_333140404105682_.out
	* start: run_static_dynamic_1017_archybrid_conllWSJToken_wikipedia2MUNK_100_fv021a_0_333140404105682_.out
$$b42	* start: run_static_dynamic_1017_archybrid_conllWSJToken_glove6B_100_fv021a_0_333140404105682_.out
	
	* Best points:
	
	corpus: conllWSJToken (trn02-21:39832s,950028w,1820392m;dev22:1700s,40117w,76834m;tst23:2416s,56684w,108536m)
	arctype: archybrid (TODO: can archybrid13 be the same?)
	embedding: wikipedia2MUNK-100
	feats: fv021a
	kernel: rbf
	gamma: 0.333140404105682
	devscore: 0.0325506937033084 (nsv=96326, single=0.0328500403467215)
	test: (TODO)
	
	corpus: conll07EnglishToken (trn:16588s,398439w,763702m;dev:1989s,48134w,92290m;trn+dev:18577s,446573w,855992m;tst:214s,5003w)
	arctype: archybrid
	embedding: wikipedia2MUNK-100 (TODO: other embeddings, bansal)
	feats: fv022b (TODO: compare with wsj feats)
	kernel: rbf
	gamma: 0.376747095368119
	tstscore: 0.0418668 (1-epoch full-trn/tst, nsv=57661)
	test: (TODO: debug run_static_dynamic)



2014-10-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* conllWSJ-fv021a: debugging slow vectorparser with single.  cache problem?
	a2=load('archybrid_conllWSJToken_wikipedia2MUNK-100_fv021a_dump.mat'); % double
	a1=convert_to_single(a2); % single
	load('archybrid_conllWSJToken_wikipedia2MUNK-100_fv021a_dump.mat_1014_00.mat');
	% initial model.
	% m1=perceptron(a1.trndump.x, a1.trndump.y, model);
	% first question: why still merge?  
	% Merge is triggered by a fixed size for svtr2, so ok.
	% second question: why is beta double?	
	% Fixed perceptron to create zeros with the right type.

	m1=perceptron(a1.trndump.x, a1.trndump.y, model); % 263.375s, 96326sv, single
	m2=perceptron(a2.trndump.x, a2.trndump.y, model); % 517.095s, 96638sv, double

	perceptron(a1.devdump.x, a1.devdump.y, m1, 'update', 0);
	Using X batchsize=2529
	nd=1326 nx=76834 nc=3 ns=96326
	inst	err	nsv	batch	time	mem
	76834	3.2850	96326	157	20.51	4.53e+08

	perceptron(a2.devdump.x, a2.devdump.y, m2, 'update', 0);
	Using X batchsize=2224
	nd=1326 nx=76834 nc=3 ns=96638
	inst	err	nsv	batch	time	mem
	76834	3.2551	96638	1779	37.73	4.81e+08

	% ok, confirmed that the single version is twice as fast and slightly worse.
	% now, what is wrong with vectorparser?
	% try it on a mini version of trn:
	a1.trn100 = a1.trn(1:100);
	[~,a1.trn100dump] = vectorparser(m1, a1.trn100, 'update', 0, 'predict', 0);
	% same with a2...
	a2.trn100 = a2.trn(1:100);
	[~,a2.trn100dump] = vectorparser(m2, a2.trn100, 'update', 0, 'predict', 0);
	% ok both a1 and a2 have double x,y in trn100dump; that's bad.  fixing...

	m1.x = a1.trn100dump.x;
	m1.y = [];
	m1a = vectorparser(m1, a1.trn100); % single, usecache, 1.58x/s.

	% m1a.beta is double again!  fixing compactify... done.  1.6x/s.

	m2.x = a2.trn100dump.x;
	m2.y = [];
	m2a = vectorparser(m2, a2.trn100); % double, usecache, 0.8x/s.

	% Back to normal with single faster.  Now without cache:

	m1.x = [];
	m1b = vectorparser(m1, a1.trn100); % single, nocache, 0.5x/s.	
	isequal(m1a,m1b) % 1

	m2.x = [];
	m2b = vectorparser(m2, a2.trn100); % double, nocache, 0.21x/s.		
	isequal(m2a,m2b) %  

	% And now with big cache:

	m1.x = a1.trndump.x;
	m1c = vectorparser(m1, a1.trn100); % single, bigcache, 0.58x/s.
	isequal(m1a,m1c) %

	m2.x = a2.trndump.x;
	m2c = vectorparser(m2, a2.trn100); % double, bigcache, 0.82x/s.
	isequal(m2a,m2c) %

	% Are the cache hits and cache types normal?
	% m1a.cache.rvec is double, fixing...
	size: 517800, rvec: [1x1326 single], mean: 3.6339, std: 2.0385, hit: 3877, miss: 1301, nkeys: 5144

	% Isn't this too high a miss rate?  Check m2a.cache: (0.83291x/s)
	size: 517800, rvec: [1x1326 double], mean: 4.2281, std: 2.1084, hit: 4034, miss: 1144, nkeys: 5156

	% Why are these numbers different and why is the miss rate so high?
	a1.trn100dump.x has 5178 instances.
	m1a.cache is initialized with 517800 empty cells.
	number of keys that actually make it in is 5144 (some were lost to clashes).
	Then we do m1a = vectorparser(m1, a1.trn100);
	This should have exactly the same x's.  No actually vectorparser
	modifies m1 and picks different parse paths.
	[~,a1.trn100dump] = vectorparser(m1, a1.trn100, 'update', 0, 'predict', 0);
	We get 3877 hits + 1301 misses in 5178 calls.  
	This should be almost all hits??
	Actually no, because vectorparser starts diverging from the original parse path.
	Wrote test_kernel_cache.m(a1.trn100dump.x, a1.trn100dump.y)
	5178 keys, 5175 unique, 5144 nkeys, 4941 hits?

	OK, numerical difference between matrix mult vs vector mult caused
	it.  After fixing:
        hit: 5156 miss: 22 nkeys: 5153 keys_orig: [1326x5178 single]
	keys_uniq: 5175

	Trying m1a again:
	m1.x = a1.trn100dump.x;
	m1.y = [];
	m1a = vectorparser(m1, a1.trn100); % single, usecache, 1.58x/s.
	hit: 4055 miss: 1123 nkeys: 5153	
	Improved from 3877/1301.  Speed 1.6 -> 1.8.

	Trying m2a again: not much difference, I guess this effected
	singles only.
	1st run: speed: 0.8326 hit: 4030 miss: 1148 nkeys: 5152 nsv: 96799
	2nd run: speed: 0.8322 hit: 4027 miss: 1151 nkeys: 5148 nsv: 96799

	Try m1c, m2c again:
	
	m1.x = a1.trndump.x;
	m1c = vectorparser(m1, a1.trn100); % single, bigcache, 0.58x/s -> 1.42x/s
	isequal(m1a,m1c) % 1

	% OK speed back to normal.
	% nocache: 0.5, bigcache: 1.42, smallcache: 1.8
	% nkeys: 1692875, hit: 3661, miss: 1517

	m2.x = a2.trndump.x;
	m2c = vectorparser(m2, a2.trn100); % double, bigcache, 0.82x/s. -> 0.81x/s
	isequal(m2a,m2c) % 1
	nkeys: 1781119, hit: 3998, miss: 1180

	% double still has more nkeys and better hit rate consistent with m2a.
	% single seems to have too many cache misses!  Debugging on a smaller case:

	m1.x = a1.trndump.x(:,1:100000);
	m1d = vectorparser(m1, a1.trn100); 
	isequal(m1a, m1d);
	% single, 100k cache, 1.7637x/s, hit:4039, miss:1139
	% single, 500k cache, 1.6840x/s, hit:3957, miss:1221
	% cache performance seems to decrease with cache size, bug?
	
	test_kernel_cache(a1.trndump.x(:,1:500000), a1.trn100dump.x);
	% single, 5178 cache, 5153 nkeys, 22 miss
	% single, 100k cache, 99208 nkeys, 49 miss
	% single, 500k cache, 487330 nkeys, 168 miss
	% single, 1m cache, 956955 nkeys, 327 miss
	% the percentage of nkeys is decreasing as more keys are used.
	
	% does double have the same problem?
	test_kernel_cache(a2.trn100dump.x, a2.trn100dump.x);
	test_kernel_cache(a2.trndump.x(:,1:100000), a2.trn100dump.x);
	% double, 5178 cache, 5148 nkeys, 27 miss
	% double, 100k cache, 99263 nkeys, 49 miss
	% double, 500k cache, 492707 nkeys, 47 miss
	% double, 1m cache, 982095 nkeys, 56 miss
	
	% can we fix it using double rvec?
	% with kernelcache_dbg where internal calculations are double:
	% single, 500k cache, 492605 nkeys, 43 miss
	% single, 1m cache, 982112 nkeys, 51 miss

	% bring a maximum to cache size?  it turns out the empty cells do
	not take that much space:
	1.8e6 * 100 * 8 = 1.44e9 ~ 1gb in single
	and the full cells (roughly 1.8m) take
	1.8e6 * 1326 * 4 = 1e10 = 10gb in single
	
	
2014-10-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* conllWSJ-fv021a: new best fv for conllWSJ, rerunning the experiment with single precision:
	waiting for the dump file on biyofiz-4-3.  script ready at run_static_dynamic_1014.m.
$$ilac	run_static_dynamic_1014_archybrid_conllWSJToken_wikipedia2MUNK_100_fv021a_0_333140404105682_.out

	DBG: static epoch gives nsv=96326, dev_stat_pct=0.0329
	Using X batchsize=2528
	nd=1326 nx=76834 nc=3 ns=96326
	inst	err	nsv	batch	time	mem
	76834	3.2850	96326	235	19.79	4.37e+08

	During featselect with double precision:
	gamma=0.33314
	Using X batchsize=2224
	nd=1326 nx=76834 nc=3 ns=96638
	inst	err	nsv	batch	time	mem
	76834	3.2551	96638	1779	36.24	4.81e+08

	Difference probably due to single/double precision.  Nice thing is
	single precision is also twice as fast!
	% Hmm, perceptron is faster but vectorparser is slower :(
	
	* conll07-reoptimize: conll07-full is worse than conll07-dev in spite of larger train set.
	Trying gamma/feature optimization again with full train:
$$ilac	run_featselect_rbf_1014_.out	
	gamma_before_optimize = 0.372063662109375 ==> f(0.372064)=0.044999
	% this is slightly different than 0.0443 which was trn/dev.
	gamma_after_optimize = 0.376678123543739 ==> f(0.376678)=0.0432241 (fv019)
	featselect best(22): 0.0422844
	fv022b = [-3 0 8;-2 -1 -4;-2 -1 5;-2 0 -9;-2 0 -4;-2 0 4;-2 0 7;-2 1 1;-1 -1 -1;-1 -1 4;-1 0 -9;-1 0 -4;-1 0 4;-1 0 6;-1 1 4;0 -1 -4;0 -1 4;0 0 -4;0 0 -2;0 0 4;1 0 -4;1 0 4]
	f(0.376752)=0.0420756
	f(0.376747)=0.0418668
	gamma_after_featselect = 0.376747095368119

	
	* conll07-full: with full training data.
$$ilac	msub "run_static_dynamic_1010('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv019', 0.372063662109375)"
        run_static_dynamic_1010_archybrid_conll07EnglishToken_wikipedia2MUNK_100_fv019_0_372063662109375_.out
	qstat -j 471140
	qdel 471140
	
	epoch	trn	dev	tst
	1	0.0700	0.0700	0.1483
	2	0.0447	0.0447	0.1435
	3	0.0269	0.0269	0.1345
	4	0.0176	0.0176	0.1353
	5	0.0151	0.0151	0.1271
	6	0.0124	0.0124	0.1293
	7	0.0111	0.0111	0.1293
	8	0.0102	0.0102	0.1279
	9	0.0097	0.0097	0.1257
	10	0.0092	0.0092	0.1271
	11	0.0093	0.0093	0.1281
	12	0.0091	0.0091	0.1299
	13	0.0091	0.0091	0.1297
	14	0.0090	0.0090	0.1305
	15	0.0087	0.0087	0.1287
	16	0.0086	0.0086	0.1279
	17	0.0085	0.0085	0.1273
	18	0.0083	0.0083	0.1279
	19	0.0083	0.0083	0.1281
	
	* Best points:
	
	corpus: conllWSJToken (trn02-21:39832s,950028w,1820392m;dev22:1700s,40117w,76834m;tst23:2416s,56684w,108536m)
	arctype: archybrid (TODO: can archybrid13 be the same?)
	embedding: wikipedia2MUNK-100
	feats: fv021a
	kernel: rbf
	gamma: 0.333140404105682
	devscore: 0.0325506937033084
	test: (TODO)
	
	corpus: conll07EnglishToken (trn:16588s,398439w,763702m;dev:1989s,48134w,92290m;trn+dev:18577s,446573w,855992m;tst:214s,5003w)
	arctype: archybrid
	embedding: wikipedia2MUNK-100 (TODO: other embeddings, bansal)
	feats: fv019 (tried featselect)
	kernel: rbf (tried poly)
	gamma: 0.372063662109375 (tried gamma_optimize) 
	devscore: 0.0443168273919168 (1-epoch stat_pct)
	test: (TODO: debug run_static_dynamic)


2014-10-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* conllWSJ: run/log2table.pl head_pct run_static_dynamic_1005_archybrid_conllWSJToken_wikipedia2MUNK_100_fv022a_0_339506144311523_.out 
	head_pct
	epoch	trn	dev	tst
	1	0.0567	0.1104	0.1140
	2	0.0373	0.0989	0.0996
	3	0.0243	0.0945	0.0954
	4	0.0188	0.0914	0.0939
	5	0.0154	0.0897	0.0936
	6	0.0141	0.0885	0.0937

	* conllWSJ:
	- Gave out of memory error after epoch 6.
	run_static_dynamic_1005_archybrid_conllWSJToken_wikipedia2MUNK_100_fv022a_0_339506144311523_.out
	- Suspicious epoch 1, optimize_gamma gives trn=1728102 nsv=91147:
	w136=load('archybrid_conllWSJToken_wikipedia2MUNK-100_fv136_dump.mat');
	[g1,f1,e1,o1] = run_optimize_gamma(0.339506144311523, w136.trndump, w136.devdump, fv.fv022a);
	- run_static_dynamic gives trn=1820392, nsv=95509:
	06-Oct-2014 10:34:43 Loading archybrid_conllWSJToken_wikipedia2MUNK-100_fv022a_dump.mat
	run_static_dynamic_1005_archybrid_conllWSJToken_wikipedia2MUNK_100_fv022a_0_339506144311523_.out
	- debugging on $$biyofiz-4-3: fv136_dump had a bad split.
	- rerunning featselect with the right split:
$$ilac	msub "run_featselect_rbf('archybrid','conllWSJToken_wikipedia2MUNK-100','fv136','fv022a',0.339506144311523)"
	run_featselect_rbf_archybrid_conllWSJToken_wikipedia2MUNK_100_fv136_fv022a_0_339506144311523_.out
$$ilac	DBG: f(0.33314)=0.032811, gamma_after_optimize = 0.333140404105682
	DBG: featselect_rbf with cache archybrid_conllWSJToken_wikipedia2MUNK-100_rbf333140_cache.mat
	DBG: 13-Oct-2014 00:48:45 # newbest(21)	0.0325507	s2w s2r1l= s1w s1r< s1r1l= s0l1c s0l1r= s0l1w s0c s0- s0r= s0w s0r1r= s0r1w n0l1c n0l1- n0l1w n0c n0w n1c n1w 

	
2014-10-10  Deniz Yuret  <dyuret@ku.edu.tr>

	* conll07-dev: trn/dev/tst results
	run_static_dynamic_1005_archybrid_conll07EnglishToken_wikipedia2MUNK_100_fv019_0_372063662109375_.out
	run/log2table.pl: extract table from log file:
	run/log2table.pl head_pct run_static_dynamic_1005_archybrid_conll07EnglishToken_wikipedia2MUNK_100_fv019_0_372063662109375_.out

	epoch	trn	dev	tst
	1	0.0644	0.1442	0.1455
	2	0.0408	0.1318	0.1403
	3	0.0211	0.1258	0.1317
	4	0.0148	0.1240	0.1305
	5	0.0108	0.1233	0.1269
	6	0.0087	0.1231	0.1267
	7	0.0073	0.1224	0.1235
	8	0.0065	0.1223	0.1235
	9	0.0063	0.1225	0.1235
	10	0.0060	0.1214	0.1219
	11	0.0060	0.1216	0.1219
	12	0.0056	0.1211	0.1201
	13	0.0056	0.1212	0.1219
	14	0.0055	0.1214	0.1221
	15	0.0053	0.1210	0.1217
	16	0.0052	0.1206	0.1229
	17	0.0050	0.1202	0.1241
	18	0.0050	0.1202	0.1243
	19	0.0048	0.1200	0.1239
	20	0.0049	0.1199	0.1251
	
	
2014-10-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* Testing:
	msub "run_static_dynamic_1005('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv019', 0.372063662109375)"
	msub "create_dumpfile_wsj('archybrid', 'conllWSJToken_wikipedia2MUNK-100', 'fv022a');"
	msub "run_static_dynamic_1005('archybrid', 'conllWSJToken_wikipedia2MUNK-100', 'fv022a', 0.339506144311523)"
	
	* vectorparser.m (compute_scores): Handle rbf kernel (TODO:
	compare with tparser)

	* perceptron.m (val_f): Handle rbf kernel.

	* run/run_static_dynamic_1005.m (run_static_dynamic_1005): This is
	for rbf kernel. and ready dump files.

	* run/create_dumpfile_wsj.m (create_dumpfile_wsj): This is for
	conllWSJToken, saves each test section separately.

2014-10-02  Deniz Yuret  <dyuret@ku.edu.tr>

	* Test-dump: compare vectorparser vs tparser.
	
	% create_dumpfile_07('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv019')
	m0.parser = eval(['@' parser]);
	m0.feats = eval(['f.' feats]);
	m0.step = 1e5; 
	[~,trndump] = vectorparser(m0, trn, 'update', 0, 'predict', 0);

	% create_dumpfile_07_dbg('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv019')
	m0 = tparser(eval(['@' parser]), eval(['f.' feats]), [], trn);
	m0.update = 0;
	m0.predict = 0;
	m0.output.feats = 1;
	m0.gparse(trn);
	trndump.x = m0.feats; trndump.y = m0.move;
	
	% Output comparison: ok.
	
	% Time comparison:
	vectorparser: 29 x/s, tparser: 24 x/s. 
	
	These lines take more time compared to corresponding lines in vectorparser:
	
	tparser:
	?  0.44    9578  255             if m.output.feats m.feats(:,end+1) = fcol; end 
	?  0.38    9578  265           mymove = gparse_pick_move(m, valid, mycost, myscore); 
	?  0.30    9578  266           if m.output.move m.move(end+1) = mymove; end 

	vectorparser:
	  0.14    9578   36       m.x(:,end+1) = ftr; 
	  0.07    9578   30       [mincost, mincostmove] = min(cost); 
	  0.01    9578   58       execmove = mincostmove; 
	  0.28    9578   70       update_dump(); 

	Using a local variable instead of a field is twice as fast:
	  0.18    9578  257             if m.output.feats myfeats(:,end+1) = fcol; end %DBG 

	Not calling a local function is three times faster:
	0.13 if we copy gparse_pick_move.
	
	* Test-static: compare perceptron, vectorparser, tparser.

	* Test-dynamic:

	* Test-conll07:

	* Test-conll-wsj:
	

2014-09-23  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_featselect_rbf-conll07:
	run_featselect_rbf('archybrid13', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv136', 'fv021', 0.372063662109375);
	fv023a: 0.0444685231336006
	w136=load('archybrid13_conll07EnglishToken_wikipedia2MUNK-100_fv136_dump.mat');
	fv=newFeatureVectors; [g1,f1,e1,o1] = run_optimize_gamma(0.372063662109375, w136.trndump, w136.devdump, fv.fv023a);
	
	% no change, fv023a-gamma local optimum, trying archybrid:
	run_featselect_rbf('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv136', 'fv023a', 0.372063662109375);
	fv023a/archybrid: 0.0452812 
	newbest(19)	0.0443168 => fv019
	
	% optimizing gamma:
	BUG: optimizing gamma always uses archybrid!!! (TODO: must streamline optimization)
$$b40	fv=newFeatureVectors; [g1,f1,e1,o1] = run_optimize_gamma(0.372063662109375, trndump, devdump, fv.fv019);	
	
	* conllWSJ:
$$b41	[g1,f1,e1,o1] = run_optimize_gamma(0.339506144311523, w136.trndump, w136.devdump, fv.fv022a);
	
	
	* features.m (features): Fixed nasty bug with feats 5,-5.  Need to
	do the dumps and featselect again.


2014-09-10  Deniz Yuret  <dyuret@yunus.hpc.ku.edu.tr>

	* compute_kernel.m (compute_kernel): Cannot understand the memory
	error.  Hard to debug when it happens in 130K secs.  Could fix
	kernelcache.  Compare performance with greedy on smaller training
	sets.  Could switch to Julia.
	
	
	* featselect_rbf: run conll07 archybrid13 featselect with rbf3721-fv021.
$$b41	run_featselect_rbf('archybrid13', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv136', 'fv021', 'rbf3721');
	08-Sep-2014 17:44:04 Loading archybrid13_conll07EnglishToken_wikipedia2MUNK-100_fv136_dump.mat
	08-Sep-2014 17:48:45 featselect_rbf with cache archybrid13_conll07EnglishToken_wikipedia2MUNK-100_rbf3721_cache.mat
	(22)	0.0443493  % name fv022

$$b41	% reoptimize gamma starting with 0.3721
	path('run',path);
	newFeatureVectors;
	w = load('archybrid13_conll07EnglishToken_wikipedia2MUNK-100_fv136_dump.mat');
	[gamma,fval,exitflag,output] = run_optimize_gamma(0.3721, w.trndump, w.devdump, fv022);
	% f(0.372063662109375) = 0.0442301

	CONLL07 BEST GREEDY SETTINGS:
	Features: fv022
	Kernel: rbf with gamma=0.372063662109375
	Arctype: archybrid13
	Embedding: conll07EnglishToken_wikipedia2MUNK-100
	1-Epoch-static-dev: 0.0442301
	
$$b41	% run train/dev pair to determine best dynamic epoch count (after one static epoch)
	
	m0.kerparam = struct('type', 'rbf', 'gamma', gamma);
	m0.parser = @archybrid13;
	m0.feats = fv022;
	m0.step = 1e7;
	m0.batchsize = 1e3;
	run_static_dynamic('archybrid13', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv022', m0);
	12-Sep-2014 19:13:16 Creating archybrid13_conll07EnglishToken_wikipedia2MUNK-100_fv022_dump.mat

	1. why so slow? ......... 1989/1989 (12793.27s 0.155472x/s)
	2. why not 0.0442301? move_pct: 0.0563
$$b41	need to debug tparser.
	compare with vectorparser_rbf?
	
	% run train0/test pair for final evaluation

	
	* featselect_rbf: with conllWSJToken.
	
	% run gamma optimization:
	[x,fval,exitflag,output] = fminsearch(@(x) f_optimize_gamma(x, x_tr, y_tr, x_te, y_te), 0.56);
	f(0.339502)=0.0343728
	% run featselect with rbf:
	m0.kerparam=struct('type','rbf','gamma',x);
	w130 = load('archybrid_conllWSJToken_wikipedia2MUNK-100_fv130_dump.mat');
	cachefile='archybrid_conllWSJToken_wikipedia2MUNK-100_rbf3395_cache.mat'
	featselect_rbf(m0, w130.trndump, w130.devdump, cachefile, fv017a);
	(17) 0.0343728
	(16) 0.0347242
	(18) 0.0339954
	(17) 0.0343077
	(19) 0.0335659
	(18) 0.0338522
	(20) 0.0335529
	(19) 0.0333056
	(18) 0.0330453
	(17) 0.0334097
	(19) 0.0328631
	(20) 0.032785
	(21) 0.0327719
	(22) 0.0326678
$$b42	[23] 0.0326158 => named fv023

	% run gamma optimization again.
$$b40	newFeatureVectors
$$b42	[gamma,fval,exitflag,output] = run_optimize_gamma(0.339502, w130.trndump, w130.devdump, fv023);
	f(0.339506)=0.0325637 (gamma=0.339506144311523)
	
	% try with other arctypes (archybrid13 is better in conll07).
$$b40	m0.kerparam=struct('type', 'rbf', 'gamma', gamma);
	m0.parser=@archybrid13;
	m0.step=1e7;
	cachefile='archybrid13_conllWSJToken_wikipedia2MUNK-100_rbf339506_cache.mat';
	featselect_rbf(m0, w130.trndump, w130.devdump, cachefile, fv023);
???	best(23): 0.0325637  % this is exactly the same as archybrid, suspicious.
	
	% compare featselect results of conll07 and conllWSJ.

2014-09-08  Deniz Yuret  <dyuret@yunus.hpc.ku.edu.tr>

	* performance: is a function of:
	-- corpus
	-- arctype
	-- embedding
	-- features
	-- kernel
	-- search (greedy vs beam, static vs dynamic, update type)

	* status:
	- On conll07:
	-- run_bparse_conll07 ended with memory error. $$b40. must debug.
	--- need to update kernelcache code even though the error above was without cache.
	-- run_5epoch_conll07_up3: ignoring single choice moves did not help. $$b43.
	-- run_conll07_ablation: arctype experiments: archybrid13 won.
	- On conllWSJToken_wikipedia2MUNK-100:
	-- featselect_rbf ended with gpu memory error.  $$b42. seems was fluke, ok now.
	
	
	* TODO:

	- Then try the best RBF multi-epoch on conll07 test set.  Try
	epoch number based on dev first.
	
	- kernelcache: Use a fixed size cache that keeps constantly
	updating.  No need for cachekeys.  Just recompute the values at
	the beginning of each epoch.

	- run_bparse_conll07: debug memory error.

	- featselect_rbf on wsj: debug memory error.

	- run_conll07_15epoch.m:
	+ If this doesn't work:
	+ Try other kernel?
	-- Check train vs test, overfitting?
	+ Try shuffling?
	+ Try other vectorparser update methods?
	+ Try p=0.9?
	- Try bansal?
	- Try pa
	+ Try to find better feature set?
	+ Try fv031a
	x Try starting featselect from best 14, 13, etc.

	- run/run_conll07_ablation.m:
	+ embedding: fselect bansal100 starting with fv015a+archybrid.
	No significant difference.
	bansal100 best(17): 0.046939
	wiki100   best(15): 0.047339 (fv015a)
	diff: -s0r1c(6) +s1l=(15) +s1r1r<(14) +s1r<(2)
	- features: anything between fv015a and fv031a? starting with 13,14?
	+ kernel, degree, rbf?
	- arctype: fselect starting with fv015a
	+ perceptron: with or without static training, p=0.9?, update rule?

	= beamparser.m:
	-- implement kernel cache
	-- try pa for faster convergence
	-- compatibility with eval_conll

	= rbf kernel with gpu?
	= Optimize and try rbf kernel calculation and fselect.
	= Other kernel degrees with featselect.
	- look at train/test performance to validate model and decide if
	higher orders are worth trying.

	- vectorparser.m: Debugging the update rule.  $$i30
	-- try vectorparser only ignore single choice as well, i.e. three options.
	-- no diff with small data, try bigger data or without static pre-training.

	- featselect_par: $$balina. left for later.

	- Implement arceasy.

	- remove model/dump distinction in perceptron etc.  just have one output.

	- Embeddings:
	-- test more on conll07.
	-- debug conllWSJToken_dep-100 (good train bad test)
	-- debug out-of-memory on levy, murphy50, mikolovGoogleNews300

	- write matlab gpu tutorial on blog
	OK, another matlab bug: if we have an SV matrix more than half the
	size of gpu memory, we can't add anything to it, we can't change
	it etc. etc. Everything I try (concat, subasgn etc.) ends up
	trying to create a copy.  This is the third bug I found in matlab
	(after max and skinny multiply problems), should send a bug
	report.

	- Old code:
	- trainparser_primal.m: broken, upgrade to use new code.
	- model_sparsify.m: broken, updgreade to use new code.

	$$i02 $$i03: still oom, do this on balina
	score:0.99 conllWSJToken_levy300 (out of memory at 1405179, sv:126177, nk:11, g:6.4e6)
	score:0.99 conll_levy300 (out of memory at 1596181, sv:165712, nk:263, g:9.8e7)
	score:0.99 conllWSJToken_mikolovGoogleNews300 (out of memory at 1111004, sv:123146, nk:19, g:4.1e6)
	score:0.99 conllWSJToken_murphy50 (out of memory at 1182126, sv:494871, nk:85, g:9.4e7)
	score:0.99 conllWSJToken_murphy50scaled01 (out of memory at 1289463, sv:494698, nk:85, g:9.4e7)




2014-08-28  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_bparse_conll07: Memory error, do we have too many SV?  Is there something in gpu
	we don't need that we are not clearing?  When training we have
	only one right path and one wrong path, SV should not matter.  We
	turned off the cache and that is not on gpu anyway.
	
$$b40	Started run_bparse_conll07 without cache.
	.......... 17000/18577 (132866.89s 0.127948x/s)
.Error using gpuArray/sum
Out of memory on device. To view more detail about available memory on the GPU,
use 'gpuDevice()'. If the problem persists, reset the GPU by calling
'gpuDevice(1)'.

Error in compute_kernel (line 78)
      s2 = sum(m.svtr .^ 2, 2);

Error in tparser/compute_score (line 95)
        score = compute_kernel(m1, x);

Error in tparser/bparse (line 399)
            scores = compute_score(m, fmatrix(:,1:nbeam));

Error in run_bparse_conll07 (line 23)
  tp.bparse(w.trn0);

	
2014-08-27  Deniz Yuret  <dyuret@ku.edu.tr>

	* tparser.m (bparse): testing make beamcell into handle to avoid
	copying need to keep the whole beam in memory compute max beam
	depth looking at corpus rename candidates beam have nbeam indexed
	by depth


	
	* run_conll07_ablation.m: Summary
	- baseline: wiki100   best(15): 0.047339 (fv015a)
	- up2, k=0, p=0.9, shuffling did not work.
	- embeddings: bansal100 best(17): 0.046939
	- kernel: (all results on trn->dev+trn split)

	d=2		d=3		d=4		d=5 		d=6             d=7 		d=8		d=9 $$b40/$$b43
	[20] 0.0566259	[15] 0.0473399	[17] 0.0455629	[18] 0.0449453	[19] 0.0456929	[17] 0.0455737	[19] 0.0451728	[17] 0.0455196

	rbf3721-fv021
	[21] 0.0445990

	- arctype: featselect starting from fv015a using fv136 in conll07EnglishToken_wikipedia2MUNK-100.

	archybrid	archybrid13	arceager	arceager13
	[15] 0.0473399	[19] 0.0464297	[19] 0.0486185	[18] 0.0495287	$$b41
	
	
	
	* fv031a-rbf: The only thing that significantly helps seems to be
	the kernel feature combination.  RBF always beats poly.  Try RBF
	featselect from fv031a.  

	w0784=load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv084_dump.mat');
	[a,b,c,d]=run_optimize_gamma(0.3721, w0784.trndump, w0784.devdump, fv031a);
	% f(0.3721)  =0.0476975
	% f(0.334781)=0.047069
	hp = struct('type', 'rbf', 'gamma', a);
	m0 = struct('parser', @archybrid, 'step', 1e6, 'kerparam', hp);
	cachefile = sprintf('archybrid_conll07EnglishToken_wikipedia2MUNK-100_rbf%d_cache.mat', round(a*10000));
$$b41	featselect_rbf(m0, w0784.trndump, w0784.devdump, cachefile, fv031a);
	
	(31)	0.047069	% compare to fv021: 0.0446
	(30)	0.0466248
	[29]	0.0459638	% yet another local minimum, not as good.
	(28)	0.046148
	(30)	0.0459638
	

	* featselect-wsj:
	- finish implementing beam parser and compare with ZN11.  First
	step: run featselect starting fv018 on conllWSJToken_wikipedia2MUNK-100.
	- other kernels? d=5 is better in conll07.  test rbf?

	>> run_featselect22('archybrid', 'conllWSJToken_wikipedia2MUNK-100', 'fv130', 'fv018');
	(18) 0.0361038	(16) 0.0357524
	[17] 0.0353489	(15) 0.0359997
	(16) 0.0357524	[17] 0.0353489
	(18) 0.0353750

	% Still same local maximum when started from (16) $$b42
	% Name this fv017a (it has a single feature diff from previous fv017).
	% fv017 was on conll07 fv017a is on conllWSJToken
	% fv017 was searched within fv084, fv017a within fv130.

	% Should try rbf:
	% set x_tr, y_tr etc. from 'archybrid', 'conllWSJToken_wikipedia2MUNK-100', 'fv008w'.
	[x,fval,exitflag,output] = fminsearch(@(x) f_optimize_gamma(x, x_tr, y_tr, x_te, y_te), 0.5);
	==> f(0.559377)=0.0386938

	% But why waste time at fv008w, should start at known best fv017a.
	% reset x_tr, y_tr etc. from 'archybrid', 'conllWSJToken_wikipedia2MUNK-100', 'fv017a'.
	% confirm we have the right data with poly3:
	m1 = perceptron(x_tr, y_tr, m0);
	[~,~,fval] = perceptron(x_te, y_te, m1, 'update', 0, 'average', 1); => 3.5349
	% run gamma optimization:
	[x,fval,exitflag,output] = fminsearch(@(x) f_optimize_gamma(x, x_tr, y_tr, x_te, y_te), 0.56);
	f(0.339502)=0.0343728
	% run featselect with rbf:
	m0.kerparam=struct('type','rbf','gamma',x);
	w130 = load('archybrid_conllWSJToken_wikipedia2MUNK-100_fv130_dump.mat');
	cachefile='archybrid_conllWSJToken_wikipedia2MUNK-100_rbf3395_cache.mat'
	featselect_rbf(m0, w130.trndump, w130.devdump, cachefile, fv017a);
	(17) 0.0343728
	(16) 0.0347242
	(18) 0.0339954
	(17) 0.0343077
	(19) 0.0335659
	(18) 0.0338522
	(20) 0.0335529
	(19) 0.0333056
	(18) 0.0330453
	(17) 0.0334097
	(19) 0.0328631
	(20) 0.032785
	(21) 0.0327719
	(22) 0.0326678
$$b42	[23] 0.0326158
	
	ERROR:
	
Training mode.
Using last model to predict.
Initializing empty model.
Initializing gpu. m=5.06e+08 .. 5.07e+08 done.
Using X batchsize=1000
nd=1625 nx=1820392 nc=3 ns=0
inst	err	nsv	batch	time	mem
10000	23.1700	2819	1000	4.45	3.59e+08
...
1060000	5.7413	61360	1000	494.89	2.92e+08
g:2.7e+08 merging sv blocks 0x1625 61495x1625
g:3.9e+08 done with merge
1070000	5.7407	61928	1000	509.42	2.63e+08
...	
1190224	5.6234	67433	364	604.41	6.81e+07
Error using gpuArray/mtimes
Out of memory on device. To view more detail about available memory on the GPU,
use 'gpuDevice()'. If the problem persists, reset the GPU by calling
'gpuDevice(1)'.

Error in perceptron_rbf>compute_scores (line 371)
    val_f = beta1 * exp(-hp.gamma * bsxfun(@plus, x_sq, bsxfun(@plus, s1sq, -2
    * (m.svtr1 * xij)))) + ...

Error in perceptron_rbf (line 75)
  score = compute_scores(m, X, i, j, opts); % score(nc,nk): scores for X(:,i:j)

Error in featselect_rbf/err (line 153)
  m1 = perceptron_rbf(x_tr, trn.y, m0);

Error in featselect_rbf (line 81)
      curr_e = err(curr_f);
 
 	Must have been a temporary situation (other processes?), rerunning
	did not generate the memory error.
	
	= reoptimize gamma:
	X run featselect with degree>3: do we really need to?  rbf is clearly better.


	
$$b43	* run_5epoch_conll07_up3.m: Try ignoring single choice moves.
	The original still looks better.

							k=1 p=1 no shuffle (original)                 
	epoch	stat	move	head	word	sent	epoch   stat    move    head    word    sent    
	1	0.0437	0.0587	0.1483	0.1340	0.7383	1       0.0437  0.0587  0.1483  0.1340  0.7383
	2	0.0543	0.0558	0.1389	0.1281	0.7336	2       0.0525  0.0522  0.1299  0.1215  0.7383
	3	0.0527	0.0505	0.1269	0.1156	0.7196	3       0.0508  0.0493  0.1255  0.1172  0.7477
	4	0.0528	0.0499	0.1249	0.1150	0.7290	4       0.0499  0.0487  0.1235  0.1154  0.7336
	5	0.0521	0.0501	0.1239	0.1134	0.7243	5       0.0504  0.0495  0.1245  0.1150  0.7196
	6	0.0527	0.0489 [0.1225]	0.1141	0.7103	6       0.0514  0.0488  0.1223  0.1147  0.7336
	7	0.0532	0.0492	0.1245	0.1154	0.7196	7       0.0511  0.0491  0.1217  0.1136  0.7430
	8	0.0540	0.0512	0.1273	0.1175	0.7243	8       0.0518  0.0503  0.1237  0.1152  0.7383
	9	0.0549	0.0517	0.1277	0.1177	0.7290	9       0.0520  0.0497 [0.1213] 0.1132  0.7243
	10	0.0550	0.0512	0.1267	0.1166	0.7336	10      0.0519  0.0501  0.1235  0.1152  0.7243
	11	0.0549	0.0513	0.1269	0.1172	0.7383	11      0.0522  0.0510  0.1249  0.1163  0.7243
	12	0.0545	0.0508	0.1255	0.1154	0.7430	12      0.0528  0.0504  0.1239  0.1161  0.7290
	13	0.0544	0.0507	0.1241	0.1143	0.7430	13      0.0532  0.0499  0.1219  0.1150  0.7290
	14	0.0546	0.0504	0.1233	0.1145	0.7430	14      0.0536  0.0499  0.1223  0.1154  0.7336
	15	0.0547	0.0501	0.1235	0.1141	0.7430	15      0.0537  0.0508  0.1251  0.1170  0.7336


2014-08-26  Deniz Yuret  <dyuret@ku.edu.tr>

	* tparser.m (gparse): Should we split bparser and gparser as two
	children of tparser?  Had decided not to because we can use the
	same model but parse/train in greedy mode or beam mode.  So it
	makes sense to keep model parameters in one object.

	* plan:
	+ debug alternative update rule
	- finish beam parser, start experiment
	- start conll07 rbf experiment
	- start rbf fv031a featselect
	- optimize rbf-gamma for dynamic oracle?


	* vectorparser_up2: Applying the perceptron.m update rule in
	vectorparser:

	- Confirm it is the same update rule:
	m10 = m0; m10.batchsize = 1;
	m10 = perceptron(w.devdump.x, w.devdump.y, m10);  % nsv=9302
	[m10a,d10a] = vectorparser_up2(m0, w.dev, 'predict', 0);
	% confirmed that m10a and m10 are the same
	- Now do dynamic vectorparser_up2 training:
	[m10b,d10b] = vectorparser_up2(m0, w.dev); % nsv=6077 vs 9302 for m10a
	- Compare the two on test set:
	[~,t10a] = vectorparser(m10a, w.tst, 'update', 0);
	e10a = eval_conll(w.tst, t10a);  % move_pct: 0.0828  head_pct: 0.2097 (ratio:2.53)
	[~,t10b] = vectorparser(m10b, w.tst, 'update', 0);
	e10b = eval_conll(w.tst, t10b);  % move_pct: 0.1479  head_pct: 0.4601 (ratio:3.11)

	So the second update rule does not work with dynamic oracle
	training.  Why?  And why does it have significantly fewer nsv?
	Assuming there are no score ties (which I confirmed is the case
	after the first move), the only difference between the two update
	rules comes from cost ties.  When there is a cost tie (multiple
	right answers), vectorparser does not update as long as one of the
	mincost moves has the maxscore.  Perceptron on the other hand,
	does update when the maxscoremove is not the same as the
	mincostmove (which is the lowest index move with mincost).  So,
	some correct moves still get punished.  This would suggest
	perceptron to have more sv, not less.  The vectorparser sv should
	be a subset of perceptron sv.  Confirm this with vectorparser
	static training:

	[m10c,d10c] = vectorparser(m0, w.dev, 'predict', 0); % 8354 sv
	all(ismember(m10c.SV', m10.SV', 'rows'))  % 0
	[~,t10c] = vectorparser(m10c, w.tst, 'update', 0);
	e10c = eval_conll(w.tst, t10c);  % move_pct: 0.0929  head_pct: 0.2307 (ratio:2.48)

	OK, perceptron has more SV than static vectorparser training, but
	there is no subset relation.  Once they start acquiring different
	SV sets, the mistakes diverge.  And as the 2014-08-05, the
	perceptron update works better with static training.  This may be
	because the model is forced to consistently pick the lowest index
	mincost move, which is also the way the gold path is generated.
	If we randomized the choice of mincost moves on the gold path, the
	results could have been different.  Test this:

	% add the following in vectorparser_rnd.m:
        mincostmove = find(cost == mincost);
        if numel(mincostmove) > 1
          mincostmove = randsample(mincostmove, 1);
        end
	% test the new gold path
	[~, w.devdumprnd] = vectorparser_rnd(m0, w.dev, 'predict', 0, 'update', 0);
	m10d = m0; m10d.batchsize = 1;
	m10d = perceptron(w.devdumprnd.x, w.devdumprnd.y, m10d); % nsv=10877 vs 9302 for m10a
	[~,t10d] = vectorparser(m10d, w.tst, 'update', 0);
	e10d = eval_conll(w.tst, t10d); % move_pct: 0.0809 head_pct: 0.2117
	% compare with e10a: move_pct: 0.0828  head_pct: 0.2097 (ratio:2.53)

	nsv increased as expected but there is not significant performance
	difference.  Try always picking the largest indexed mincost move:

	% add the following in vectorparser_lst.m:
        mincostmove = find(cost == mincost);
        if numel(mincostmove) > 1
          mincostmove = mincostmove(end);
        end
	% test the new gold path
	[~, w.devdumplst] = vectorparser_lst(m0, w.dev, 'predict', 0, 'update', 0);
	m10e = m0; m10e.batchsize = 1;
	m10e = perceptron(w.devdumplst.x, w.devdumplst.y, m10e); % nsv=9066 vs 9302 for m10a and 10877 for m10d
	[~,t10e] = vectorparser(m10e, w.tst, 'update', 0);
	e10e = eval_conll(w.tst, t10e); % move_pct: 0.0802 head_pct: 0.2107
	% compare with e10a: move_pct: 0.0828 head_pct: 0.2097 (ratio:2.53)
	% compare with e10d: move_pct: 0.0809 head_pct: 0.2117

	The nsv is lower than m10a?  The model learns to consistently pick
	the largest indexed correct move.  The difference between a, d, e
	is what is presented to the trainer as the "correct" move, the
	inputs are the same each time.  For (a) it is the first mincost
	move, for (e) it is the last mincost move, for (d) it is a random
	mincost move.  The test predictions are always made using the
	maxscore legal move.  The evaluation computes move_pct considering
	all mincost moves as correct.  So test and eval are not biased to
	pick one mincost move over another.

	This still does not explain why the perceptron update rule does
	not work with dynamic training.  There is no gold path where the
	lowest index mincost move is consistently picked.  During
	training, the maxscore legal move is always executed.
	vectorparser updates if this maxscoremove does not have mincost.
	vectorparser_up2 updates if the maxscoremove is not the lowest
	indexed mincost move.  Updating changes the weights, the weights
	change what is considered maxscore in future moves.

	[m10b,d10b] = vectorparser_up2(m0, w.dev); % nsv=6077 vs 9302 for static training
	[~,t10b] = vectorparser(m10b, w.tst, 'update', 0);
	e10b = eval_conll(w.tst, t10b);  % move_pct: 0.1479  head_pct: 0.4601 (ratio:3.11)

	[m10f,d10f] = vectorparser(m0, w.dev); % nsv=9860
	[~,t10f] = vectorparser(m10f, w.tst, 'update', 0);
	e10f = eval_conll(w.tst, t10f);  % move_pct: 0.0798  head_pct: 0.2059

	Finally found the bug: updating corrupted maxscoremove.  It didn't
	effect static training because execmove=mincostmove there.  Fixing
	vectorparser_up2.

	[m10g,d10g] = vectorparser_up2(m0, w.dev); % nsv=13705
	[~,t10g] = vectorparser(m10g, w.tst, 'update', 0);
	e10g = eval_conll(w.tst, t10g);  % move_pct: 0.0886  head_pct: 0.2287

	Summary of move_pct:
	vectorparser     static: 0.0929  dynamic: 0.0798
	vectorparser_up2 static: 0.0828  dynamic: 0.0886

	Restarting run_5epoch_conll07_up2
	Confirm same bug does not exist in tparser. done.

	Other update options: (DONE)
	- what to exec if maxscoremove is invalid. (exec the mincostmove vs next maxscoremove (I don't think this makes much difference from p=0.9))
	- what to update if maxscoremove is invalid. (update with invalid maxscoremove vs first legal maxscoremove (I think I tried this it didn't work))
	- what to do if there is a single valid move. (do not update?)
	- what to do if there is a tie among maxscore. (never happens after first update, done)
	- what to do if there is a tie among mincost. (pick lowest index vs consider all correct, done)


2014-08-25  Deniz Yuret  <dyuret@ku.edu.tr>

	* run/run_5epoch_conll07_up2.m: Testing the perceptron update rule
	on vectorparser, using vectorparser_up2.m.  The difference between
	the regular update rule used by vectorparser and the one
	perceptron uses is that sometimes perceptron will update a
	maxscoremove even when it has mincost but not equal to the
	mincostmove.  On the perceptron(cost) experiments we ran on
	2014-08-05 this determinism helped head prediction.

	* run/run_5epoch_conll07_k0.m: Just run dynamic oracle without
	static first epoch.

	k=1 p=1 no shuffle (original)                   k=0 p=1 (no static) $$b41		k=1 up2 (perceptron update rule #2) $$b40.
	epoch   stat    move    head    word    sent	stat    move    head    word    sent	stat    move    head    word    sent
	1       0.0437  0.0587  0.1483  0.1340  0.7383	0.0701	0.0565	0.1423	0.1320	0.7664	0.0437	0.0587	0.1483	0.1340	0.7383
	2       0.0525  0.0522  0.1299  0.1215  0.7383	0.0633	0.0513	0.1305	0.1215	0.7477	0.0599	0.0584	0.1503	0.1381	0.7897
	3       0.0508  0.0493  0.1255  0.1172  0.7477	0.0605	0.0502	0.1251	0.1152	0.7196	0.0568	0.0543	0.1393	0.1281	0.7804
	4       0.0499  0.0487  0.1235  0.1154  0.7336	0.0611	0.0494	0.1249	0.1152	0.7477	0.0558	0.0546	0.1363	0.1261	0.7710
	5       0.0504  0.0495  0.1245  0.1150  0.7196	0.0587	0.0493 [0.1241]	0.1134 	0.7290 	0.0545 	0.0539	0.1371	0.1261	0.7617
	6       0.0514  0.0488  0.1223  0.1147  0.7336	0.0583	0.0496	0.1241	0.1132	0.7336	0.0540	0.0536	0.1337	0.1238	0.7617
	7       0.0511  0.0491  0.1217  0.1136  0.7430	0.0589	0.0503	0.1251	0.1132	0.7477	0.0531	0.0539	0.1359	0.1249	0.7570
	8       0.0518  0.0503  0.1237  0.1152  0.7383	0.0591	0.0505	0.1257	0.1147	0.7523	0.0520	0.0531	0.1321	0.1215	0.7523
	9       0.0520  0.0497 [0.1213] 0.1132  0.7243	0.0589	0.0507	0.1257	0.1136	0.7523	0.0517	0.0535	0.1333	0.1206	0.7383
	10      0.0519  0.0501  0.1235  0.1152  0.7243	0.0587	0.0522	0.1299	0.1172	0.7570	0.0523	0.0528	0.1317	0.1200	0.7477
	11      0.0522  0.0510  0.1249  0.1163  0.7243	0.0584	0.0524	0.1313	0.1188	0.7523	0.0521	0.0532	0.1319	0.1202	0.7477
	12      0.0528  0.0504  0.1239  0.1161  0.7290	0.0582	0.0524	0.1315	0.1193	0.7523	0.0524	0.0534 [0.1313]	0.1193	0.7477
	13      0.0532  0.0499  0.1219  0.1150  0.7290	0.0577	0.0514	0.1277	0.1163	0.7430	0.0520	0.0526	0.1313	0.1195	0.7430
	14      0.0536  0.0499  0.1223  0.1154  0.7336	0.0572	0.0516	0.1285	0.1168	0.7383	0.0519	0.0525	0.1299	0.1177	0.7336
	15      0.0537  0.0508  0.1251  0.1170  0.7336	0.0572	0.0513	0.1277	0.1161	0.7383	0.0524	0.0538	0.1331	0.1209	0.7383



2014-08-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* compute_kernel.m: perceptron_dbg, vectorparser_dbg,
	beamparser_dbg and tparser call compute_kernel.

	* conll07: where are we?
	GN13 claims 12.38 average of 10 runs (shuffling), 15 epochs.
	+ Try other kernel: done, rbf always best, gamma needs to be optimized.
	poly3-fv015a best 12.37 at best epoch 5, 12.71 at epoch 15 (dev-stat: 4.7340).
	poly5-fv018a best 12.13 at best epoch 9, 12.51 at epoch 15 (dev-stat: 4.4945)
	We have not tried multi-epoch with rbf3721-fv021 (dev-stat: 4.4599)
	+ Try shuffling? did not help. $$b41
	+ Try p=0.9? did not help. $$b40
	= Try k=0? $$b41
	= Try other vectorparser update methods? $$b40
	- Try to find better feature set?
	-- Try fv031a: could try with rbf
	-- Try starting featselect from best 14, 13, etc.
	x Check train vs test, overfitting? but what to do with the result?  xval already tells us.
	x Try bansal?
	x Try pa

	* shuffle: It seems neither shuffling nor p=0.9 help.  This is on
	archybrid, poly5, fv018a, conll07 full train and test.  Will try
	k=0 but not hopeful.  Try better feature/kernel or update methods.

	k=1 p=1 no shuffle (original)			k=1 p=1 shuffle $$b41			k=1 p=.9 no shuffle $$b40
	epoch   stat    move    head    word    sent    stat    move    head    word    sent    stat    move    head    word    sent
	1       0.0437  0.0587  0.1483  0.1340  0.7383	0.0437	0.0587	0.1483	0.1340	0.7383	0.0437	0.0587	0.1483	0.1340	0.7383
	2       0.0525  0.0522  0.1299  0.1215  0.7383	0.0534	0.0531	0.1307	0.1213	0.7243	0.0541	0.0534	0.1323	0.1240	0.7617
	3       0.0508  0.0493  0.1255  0.1172  0.7477	0.0530	0.0513	0.1267	0.1184	0.7103	0.0530	0.0536	0.1323	0.1238	0.7523
	4       0.0499  0.0487  0.1235  0.1154  0.7336	0.0522	0.0513	0.1263	0.1175	0.7056	0.0518	0.0515	0.1271	0.1181	0.7430
	5       0.0504  0.0495  0.1245  0.1150  0.7196	0.0525	0.0517	0.1273	0.1177	0.7290	0.0524	0.0493	0.1239	0.1147	0.7523
	6       0.0514  0.0488  0.1223  0.1147  0.7336	0.0525	0.0519	0.1277	0.1177	0.7196	0.0527	0.0501	0.1271	0.1186	0.7523
	7       0.0511  0.0491  0.1217  0.1136  0.7430	0.0526	0.0511	0.1263	0.1175	0.7150	0.0523	0.0515	0.1297	0.1206	0.7383
	8       0.0518  0.0503  0.1237  0.1152  0.7383	0.0534	0.0515	0.1267	0.1175	0.7243	0.0527	0.0511	0.1267	0.1184	0.7336
	9       0.0520  0.0497 [0.1213] 0.1132  0.7243	0.0535	0.0516	0.1259	0.1170	0.7196	0.0524	0.0512	0.1265	0.1172	0.7383
	10      0.0519  0.0501  0.1235  0.1152  0.7243	0.0530	0.0508	0.1241	0.1147	0.7150	0.0531	0.0514	0.1271	0.1181	0.7383
	11      0.0522  0.0510  0.1249  0.1163  0.7243	0.0531	0.0510	0.1251	0.1154	0.7150	0.0534	0.0513	0.1275	0.1184	0.7383
	12      0.0528  0.0504  0.1239  0.1161  0.7290	0.0537	0.0514	0.1271	0.1170	0.7243	0.0541	0.0507	0.1271	0.1186	0.7430
	13      0.0532  0.0499  0.1219  0.1150  0.7290	0.0539	0.0505	0.1247	0.1145	0.7196	0.0535	0.0505	0.1267	0.1181	0.7383
	14      0.0536  0.0499  0.1223  0.1154  0.7336	0.0537	0.0507	0.1257	0.1156	0.7243	0.0536	0.0514	0.1297	0.1202	0.7336
	15      0.0537  0.0508  0.1251  0.1170  0.7336	0.0537	0.0501	0.1243	0.1150	0.7290


2014-08-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* compute_kernel: It is time to get this baby out of perceptron,
	vectorparser, beamparser and have it in one place optimized for
	various types of input, cache enabled etc.

	- can take care of batching of the input if perceptron testing.
	- must standardize model parameter names across.
	- suggest always keeping svtr in gpu.
	- always have two blocks: svtr,beta,beta2 and newsvtr, newbeta, newbeta2.
	- the cache, if it exists, only works for the non-new versions.
	- if x has one vector, use the bsxfun trick.
	- always check the cache, if x has multiple vectors, check for each.
	- transpose big stuff (sv and x) out of gpu if necessary.
	- implement rbf kernel as well.

	- shuffling and compactify should be out.

	* conll07: kernel-feature-results:
	dev: 1-epoch-dev-stat
	test: 1-epoch-test-stat
	head: best-epoch-test-head

	dev	test	head	model
	4.45985			rbf-372111-fv021 (gamma=0.372109375)
	4.49453	4.37	12.13	poly5-fv018a
	4.51186			rbf-372111-fv018a
	4.55629			poly4-(17)
	4.56929			poly6-(19)
	4.60613			poly7-(17)
	4.7340	4.69	12.37	poly3-fv015a
	4.76108			poly3-fv017
	4.82826			rbf-537494-fv018a
	4.8304			poly3-fv031a
	4.85			poly3-fv018 (same as fv808)
	4.8759			rbf-537494-(12)
	5.24108			rbf-537494-fv008w
	5.66259			poly2-(20)
	5.67			poly3-fv034
	7.99762			poly3-fv084

	Looks like we don't have much more left in feature/kernel selection.
	Try shuffling and other update rules.

	* conllWSJToken_wikipedia2MUNK-100: kernel-feature results:

	dev	test	head	model
	3.43728			rbf-339502-fv017a
	3.53489			poly3-fv017a
	3.61038			poly3-fv018 (same as fv808)
	3.86938			rbf-559377-fv008w


2014-08-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* gparser.m: object-oriented version of vectorparser.m with the
	various improvements discussed below.  eval_conll and compactify
	incorporated inside.  Probably should take out code common to
	beamparser.  Is not possible to have two multi-file classes in one
	directory.  So we'll use inheritence.  It does not make sense to
	inherit from archybrid etc, those only represent a single
	sentence.

	Designing the inheritence structure: the immutables are common.
	operations are not.  some output is.  model weights svs definitely
	common.  methods: init, set_model_parameters, compactify, eval,
	even compute_kernel may be common.  the question is, is there
	anything worth abstracting out of the greedy parser.  static
	train/test?  no predict, update, just dump?

	Two main differences between bparser and gparser:
	- how do we determine maxscorepath (beam vs greedy search)
	- when do we update (every token vs after sentence)

	Should we have a single parser that has the option of updating in
	sentence vs after sentence?  beam=1 could simulate vectorparser?
	update=2 for post sentence updating?  always compute both maxscore
	and mincost paths, or as necessary?  single code can emulate
	static, dynamic, beam etc.  The yparser!  Maybe just have bparse
	and gparse as two methods.


2014-08-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* gparser.m: We need to rethink the interface between various
	scripts.  Create a dbg set and work out the new interface:

	- Forget about dump and input options.  Take whatever information
	from the model, put whatever result into the model.

	- We have to figure out how to output multiple paths so both
	vectorparser and beamparser can be evaluated.  There is a
	mincostpath and a maxscorepath, beamparser follows both,
	vectorparser follows one or the other.  Predicted heads are always
	determined by the maxscorepath.  In fact vectorparser only follows
	mincostpath (predict=0) in dump or static-train modes.  But I
	guess we need possible preds as well.  To keep sane we should
	prefix all related to mincost as y, and all related to maxscore as
	z.  So we could possibly have yfeats, ymove, yscore, ycost, (one
	per move), yhead, ysumscore (one per sentence) and their z
	versions.  How about the mincostmove on a zpath or maxscoremove on
	a ypath?  Do not output those.  People can compute them from
	ymove, yscore etc.  The actual ymove, zmove should be the ones
	that are actually executed to get from one state to the next along
	the path.  Careful about feats, it is also used to specify which
	features to use.  Let's rename that fmatrix.

	- vectorparser has 6 steps, here are their prerequisites:
	1. valid:  m.parser
	2. cost:   m.parser, s.head
	3. feats:  m.parser, m.feats, s.wvec
	4. scores: m.SV, m.beta or  m.beta2, feats
	5. update: m.SV, m.beta and m.beta2, feats, cost, scores
	6. move:   m.parser, the rest depends on which move

	- following mincostpath needs 2, 6.
	- following maxscorepath needs 1, 3, 4, 6.
	- test uses maxscorepath.
	- eval also needs 2.
	- train also needs 5 (i.e. all).
	- dump follows mincostpath but also outputs feats (3).
	- 1-3 are on/off.
	- 4, 5 and 6 has different options.
	- 4 options: averaged (beta2) or final (beta) coefficients.
	- 5 options: no-update, and various ways of dealing with ties.
	- 6 options: follow mincost or maxscore or some other if maxscore is invalid.
	- we follow mincost for static train, test.
	- however it is more efficient to dump before static train, test and use perceptron.m.
	- that is one case when we need feats without scores.
	- kernel caching needs to store feats but has to recalculate scores.

	- need to design a number of options to determine the choices:
	- which way to move
	- whether/how to update
	- what to dump

	- ok well we could simplify things by dumping everything we use
	- (although for dump mode we don't use features so that has to be an option)
	- average is 0/1 (dogma model_predict argument)
	- move is captured well with predict=0,1 (no dogma equivalent)
	- update is right now 0/1, we could add more types (dogma also has model.update with different semantics)

	- update=1 means 5, which requires the rest, though average and predict can effect 4 and 6.
	- update=0 means no 5, means no 2,3,4.  predict determines 1,2,6 vs 1,3,4,6.
	- (another exception: eval needs cost which also should be an option when update=0).
	- just set whatever extra fields you want (feats,cost) computed extra to empty arrays and they get filled?

	update options (not all implemented and tried): (DONE)
	- no update for testing.
	- what to do if maxscoremove is invalid. (take the mincostmove vs next maxscoremove)
	- what to do if there is a single valid move. (do not update)
	- what to do if there is a tie among maxscore. (never happens)
	- what to do if there is a tie among mincost. (pick lowest index vs consider all correct)

	+ predict should be a probability! (p=0.9 predicts with prob 0.9)
	+ vectorparser should be a copyable class!  (probably called greedy or deterministic etc.)

	do similar analysis for beamparser to decide common names (DONE)

	- It is probably not a good idea to merge perceptron, leave it standalone.
	- Call the class tparser with gparse and bparse as two methods.


	* prioritize:
	- conll07
	-- try rbf featselect: $$b40  (later reoptimize gamma)
	-- try degree=5 fv018a 5epoch: $$b41
	-- try degree>5 featselect: $$b43 (see if better than d5,fv018a)
	- beamparser
	-- try wsjtoken featselect: $$b42 (later optimize kernel)
	-- finish code

	* run_5epoch_conll07: Try the better kernel fv018a with degree=5.
	Compare to GN13 hybrid-dynamic 87.62=12.38 and the prev d=3,fv015a run.
	This is using the full conll07 train and test sets.
	Improvement is possible...

	d=5,fv018a,$$b41				d=3,fv015a
	epoch	stat	move	head	word	sent	stat    move    head    word    sent
	1	0.0437	0.0587	0.1483	0.1340	0.7383	0.0469  0.0586  0.1477  0.1370  0.7664
	2	0.0525	0.0522	0.1299	0.1215	0.7383	0.0524  0.0512  0.1305  0.1229  0.7523
	3	0.0508	0.0493	0.1255	0.1172	0.7477	0.0508  0.0501  0.1259  0.1170  0.7430
	4	0.0499	0.0487	0.1235	0.1154	0.7336	0.0519  0.0503  0.1259  0.1175  0.7477
	5	0.0504	0.0495	0.1245	0.1150	0.7196	0.0513  0.0497 [0.1237] 0.1150  0.7430
	6	0.0514	0.0488	0.1223 	0.1147 	0.7336 	0.0523  0.0511  0.1271  0.1172  0.7523
	7	0.0511	0.0491	0.1217 	0.1136	0.7430	0.0532  0.0512  0.1259  0.1156  0.7477
	8	0.0518	0.0503	0.1237 	0.1152	0.7383	0.0535  0.0513  0.1273  0.1177  0.7570
	9	0.0520	0.0497 [0.1213]	0.1132	0.7243	0.0544  0.0511  0.1267  0.1170  0.7523
	10	0.0519	0.0501	0.1235	0.1152	0.7243	0.0549  0.0516  0.1257  0.1166  0.7477
	11	0.0522	0.0510	0.1249	0.1163	0.7243	0.0545  0.0512  0.1263  0.1168  0.7477
	12	0.0528	0.0504	0.1239	0.1161	0.7290	0.0544  0.0525  0.1301  0.1200  0.7383
	13	0.0532	0.0499	0.1219	0.1150	0.7290	0.0541  0.0513  0.1261  0.1163  0.7383
	14	0.0536	0.0499	0.1223	0.1154	0.7336	0.0551  0.0517  0.1267  0.1172  0.7383
	15	0.0537	0.0508	0.1251	0.1170	0.7336	0.0553  0.0519  0.1271  0.1179  0.7336

	* rbf_kernel: Modifying perceptron_dbg.m to use rbf kernel:

	  x_sq = sum(xij.^2, 1);
	  if ns2 == 0
	    s1sq = sum(m.svtr1.^2, 2);
	    val_f = beta1 * exp(-hp.gamma * bsxfun(@plus, x_sq, bsxfun(@plus, s1sq, -2 * (m.svtr1 * xij))));
	    clear s1sq;
	  elseif ns1 == 0
	    s2sq = sum(m.svtr2.^2, 2);
	    val_f = beta2 * exp(-hp.gamma * bsxfun(@plus, x_sq, bsxfun(@plus, s2sq, -2 * (m.svtr2 * xij))));
	    clear s2sq;
	  else
	    s1sq = sum(m.svtr1.^2, 2);
	    s2sq = sum(m.svtr2.^2, 2);
	    val_f = beta1 * exp(-hp.gamma * bsxfun(@plus, x_sq, bsxfun(@plus, s1sq, -2 * (m.svtr1 * xij)))) + ...
	            beta2 * exp(-hp.gamma * bsxfun(@plus, x_sq, bsxfun(@plus, s2sq, -2 * (m.svtr2 * xij))));
	    clear s1sq s2sq;
	  end

	Run one epoch:

	w0715 =	load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv015a_dump.mat')
	m0 = struct('kerparam', struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
	m0.parser = @archybrid;
	m0.feats = fv015a;
	m0.step = 1e4;
	m0.batchsize = 1e3;
	m1 = m0;
	m1.kerparam = struct('type', 'rbf', 'gamma', 0.1);
	[m0a,d0a] = perceptron(w0715.trndump.x, w0715.trndump.y, m0);
	nd=1401 nx=763702 nc=3 ns=0
	inst	err	nsv	batch	time	mem
	763702	7.5355	58050	702	132.87	4.49e+08
	Finding unique SV in 58050...
	Saving 57809 unique SV.
	[m1a,d1a] = perceptron_dbg(w0715.trndump.x, w0715.trndump.y, m1);
	inst	err	nsv	batch	time	mem
	763702	17.5595	134603	171	331.48	3.33e+08
	Finding unique SV in 134603...
	Saving 133915 unique SV.

	%%% 132 vs 331 seconds, 2.5x slow down in training not too bad.

	[~,~,e0a] = perceptron(w0715.devdump.x, w0715.devdump.y, m0a, 'update', 0)
	inst	err	nsv	batch	time	mem
	92290	4.7340	57809	426	27.84	4.51e+08
	e0a = 0.0473
	[~,~,e1a] = perceptron_dbg(w0715.devdump.x, w0715.devdump.y, m1a, 'update', 0)
	inst	err	nsv	batch	time	mem
	92290	9.8386	133915	1150	61.50	4.22e+08
	e1a = 0.0984

	%%% 61 vs 28 seconds, 2.2x slow down in testing.
	%%% Result a lot worse, but neither gamma nor features have been optimized.

	%%% May be worth running featselect for rbf:
	start with archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv084_dump.mat
	use fv008w for initial starting point
	modified featselect_dbg to call perceptron_dbg.
	In fact let's rename these featselect_rbf and perceptron_rbf.

	m0 = struct('kerparam', struct('type','rbf','gamma',0.1), 'parser', @archybrid, 'step', 1e6);
	cachefile = 'archybrid_conll07EnglishToken_wikipedia2MUNK-100_rbf_cache.mat';
	w0784 = load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv084_dump.mat');
	featselect_rbf(m0, w0784.trndump, w0784.devdump, cachefile, fv008w);

	(8) 0.122104
	(7) 0.125561
	(9) 0.108809
	(8) 0.109524
	(10) 0.106891

	- This looks kind of hopeless given that poly3 gives 0.0546 in
	epoch=1 with fv008w.  gamma problem?
	- First confirm 0.0546.
	19-Aug-2014 23:02:45 # starting(8)	0.0543613	s1w s1r1w s0l1w s0w s0r1w n0l1w n0w n1w
	- Then optimize gamma using fv008w.

	[x,fval,exitflag,output] = fminsearch(@(x) f_optimize_gamma(x, x_tr, y_tr, x_te, y_te), 1.0);
	x = 0.537493896484375
	fval = 0.0524108787517608
	exitflag = 1
	output:
	iterations: 20
	funcCount: 48
	algorithm: 'Nelder-Mead simplex direct search'
	message: Optimization terminated:
	  the current x satisfies the termination criteria using OPTIONS.TolX of 1.000000e-04
	  and F(X) satisfies the convergence criteria using OPTIONS.TolFun of 1.000000e-04

	- Then rerun featselect if there is hope.
	!rm archybrid_conll07EnglishToken_wikipedia2MUNK-100_rbf_cache.mat
	m0.kerparam.gamma = x
	featselect_rbf(m0, w0784.trndump, w0784.devdump, cachefile, fv008w);

	7	    8           9            10         11          [12]        13
	0.055271    0.052411    0.051338     0.05033    0.049507    0.048759    0.048759

	Stuck at a local minimum at 12.

	- Try restart at fv018a which was best at d=5,0.0449.
	18	   17
	0.0482826  0.0471015

	- No, first do a gamma optimize at fv018a.
	newFeatureVectors
	% reset x_tr, x_te vs.
	% confirm we have the right data
	m0p5.kerparam=struct('type','poly','degree',5,'gamma',1,'coef0',1);
	m1 = perceptron(x_tr, y_tr, m0p5);
	[~,~,tmp] = perceptron(x_te, y_te, m1, 'update', 0) => 0.044945
	% optimize gamma for rbf
	[x,fval,exitflag,output] = fminsearch(@(x) f_optimize_gamma(x, x_tr, y_tr, x_te, y_te), 0.55);
	f(0.372111)=0.0451186
	- Do a featselect_rbf starting with fv018a using the right gamma (wipe cachefile first or make gamma part of name).
	m0.kerparam.gamma = x;
	cachefile='archybrid_conll07EnglishToken_wikipedia2MUNK-100_rbf3721_cache.mat';
	featselect_rbf(m0, w0784.trndump, w0784.devdump, cachefile, fv018a);
	==> fv018a is a local minimum. with 0.0451186
	- Reoptimize gamma.
	No need, fv018a did not move.
	- Then do a final check with fv136: It does make a difference!
	w07136=load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv136_dump.mat');
	featselect_rbf(m0, w07136.trndump, w07136.devdump, cachefile, fv018a);
	(18) 0.0448586
	(19) 0.0448369
	(20) 0.0446094
	[21] 0.0445985 => named fv021
	- Reoptimize gamma
	no change, gamma=0.372109375
	$$b40

	* run/run_conll07_ablation.m:
	= kernel: fselect d=2..9 starting with fv015a+archybrid.
	d=2		d=3		d=4		d=5 		d=6             d=7 		d=8		d=9 $$b40/$$b43
	(15) 0.0607758	[15] 0.047340	(15) 0.0465598	(15) 0.0474591	(15) 0.0490844  (15) 0.0511756	(15) 0.0532127	(15) 0.0558782
	(14) 0.0613284	(14) 0.047925	(14) 0.0466464	(14) 0.0475133	(13) 0.0489327	(14) 0.0508506	(14) 0.0527359	(14) 0.0552498
	(15) 0.0592914	(16) 0.047340	(15) 0.0464406	(15) 0.0467981	(14) 0.0477625	(13) 0.0509264	(13) 0.0531477	(13) 0.055759
	(16) 0.0587713  (17) 0.047611	(16) 0.0455954	(16) 0.0459096	(15) 0.0471665	(15) 0.0465598	(15) 0.0470365	(15) 0.0487377
	(17) 0.0578719			[17] 0.0455629	(17) 0.0454437	(16) 0.0464406	(14) 0.0474808	(14) 0.0473399	(14) 0.0487485
	(18) 0.0575144			(18) 0.0455629	[18] 0.0449453	(17) 0.0462022  (16) 0.0462347	(16) 0.0463539	(16) 0.0469823
	(19) 0.056745 					(19) 0.0449453	(18) 0.0459313  (15) 0.0464189	(15) 0.0466031	(15) 0.0478600
	[20] 0.0566259							[19] 0.0456929	[17] 0.0455737	(17) 0.0457254	(17) 0.0463430
									(20) 0.0456929	(16) 0.0459963	(16) 0.0463322	(16) 0.0467331
													(18) 0.0453679	(18) 0.0456387
													[19] 0.0451728	[17] 0.0455196
															(16) 0.0465706


	ARCTYPE EXPERIMENTS: featselect starting from fv015a in fv136
	archybrid: (15) 0.047340

	archybrid13:
	(15) 0.0487268
	(14) 0.0486293
	(13) 0.0490736
	(15) 0.0474049
	(14) 0.0479359
	(16) 0.0470907
	(17) 0.0467981
	(18) 0.0467331
	-- continued above


2014-08-18  Deniz Yuret  <dyuret@ku.edu.tr>

2014-08-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* beamparser_dbg.m:
	+ implement early stopping
	+ implement path construction
	+ implement perceptron update
	+ implement dump update
	+ check beamparser_init
	- implement kernel cache
	- try pa for faster convergence
	- compatibility with eval_conll

	Debugging:

 	w = load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv015a_dump.mat')
	% w.trn: 763702 inst, 398439 word, 16588 sent
	newFeatureVectors;
	m0 = struct('kerparam', struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3),...
                    'parser', @archybrid, 'feats', fv015a, 'step', 1e5, 'batchsize', 1e3);
	m1 = perceptron(w.trndump.x, w.trndump.y, m0);   % 223 secs 57809 unique sv
	m1nox=rmfield(m1,{'x','y'});
	[m2,d2] = vectorparser(m1nox, w.trn(1:200));  % 3.85x/s with cache, 2.65 without, the rest without cache
	[m2a,d2a] = beamparser_dbg(m1nox, w.trn(1:200), 'beam', 1);

	% They do not match because of early stopping!
	% Also beamparser updates at sentence end.
	% Need to compare them in test mode.  The following all match:
	% However beamparser_dbg is much slower, should look into it.

	% Testing with averaged model:
	[m3,d3] = vectorparser(m1nox, w.trn(1:200), 'update', 0, 'predict', 1);  % 3.01x/s
	[m3a,d3a] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 1, 'beam', 1);  % 1.74x/s, 3.01x/s after bsxfun hack

	% This is not identical due to larger beam:
	[m3b,d3b] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 1, 'beam', 10);  % 0.46x/s

	% Testing with final model:
	[m4,d4] = vectorparser(m1nox, w.trn(1:200), 'update', 0, 'predict', 1, 'average', 0);  % 3.06x/s
	[m4a,d4a] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 1, 'beam', 1, 'average', 0);  % 1.66x/s

	% Dumping features:
	[m5,d5] = vectorparser(m1nox, w.trn(1:200), 'update', 0, 'predict', 0);  % 29.2x/s
	[m5a,d5a] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 0, 'beam', 1);  % 17.2x/s

	% This should match (no effect of beam if we are not predicting) but doesn't:
	[m5b,d5b] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 0, 'beam', 10); % 1.47x/s

	% It turns out beam=10 finds better solutions to non-projective sentences.
	% Sentences 10:35 are projective (0 cost) and these match:
	% We shoud output pred in dump mode!
	[m7,d7] = vectorparser(m1nox, w.trn(10:35), 'update', 0, 'predict', 0);  % 39.5x/s
	[m7a,d7a] = beamparser_dbg(m1nox, w.trn(10:35), 'update', 0, 'predict', 0, 'beam', 1);  % 15.1x/s
	[m7b,d7b] = beamparser_dbg(m1nox, w.trn(10:35), 'update', 0, 'predict', 0, 'beam', 10); % 2.04x/s

	% Static oracle training:
	% These should match but don't:
	[m6,d6] = vectorparser(m1nox, w.trn(1:200), 'update', 1, 'predict', 0);  % 2.67x/s
	[m6a,d6a] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 1, 'predict', 0, 'beam', 1);  % 1.55x/s

	% Actually they shouldn't: vectorparser updates after every move,
	beamparser waits until the end of the sentence.  So their training
	is not comparable.  I did confirm however that everything except
	score matches.

	% This doesn't match at all because it picks different oracle y to follow:
	[m6b,d6b] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 1, 'predict', 0, 'beam', 10); % 0.40x/s

	% Starting profile
	[m5,d5] = vectorparser(m1nox, w.trn(1:200), 'update', 0, 'predict', 0);
	[m5a,d5a] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 0, 'beam', 1);

	DONE: profile
	DONE: use struct array instead of cell array?
	DONE: profile beam=10 testing
	====: make eval_conll work again
	====: kernelcache



	* run_conll07_15epoch.m: Final version of vectorparser with kernel
	caching gets 6 sentences or 153 words per second, which means one
	epoch should be less than an hour!  Now let's run this for 15
	epochs and see what happens.  Until now we always stopped when the
	improvement stopped for one epoch.  Compare to GN13 hybrid-dynamic
	goes to 87.62=12.38 head.

	epoch	stat	move	head	word	sent	nsv(fin-uniq)	time(*)
	1	0.0469	0.0586	0.1477	0.1370	0.7664	63668-63359	255s (static)
	2	0.0524	0.0512	0.1305	0.1229	0.7523	113001-102704	14509s (dynamic)
	3	0.0508	0.0501	0.1259	0.1170	0.7430	143697-129767	13400s
	4	0.0519	0.0503	0.1259	0.1175	0.7477	165416-150682	13881s
	5	0.0513	0.0497	0.1237	0.1150	0.7430	182066-167586	13707s
	6	0.0523	0.0511	0.1271	0.1172	0.7523	195499-181295	13799s
	7	0.0532	0.0512	0.1259	0.1156	0.7477	206538-193049	13616s
	8	0.0535	0.0513	0.1273	0.1177	0.7570	215773-202974	13453s
	9	0.0544	0.0511	0.1267	0.1170	0.7523	223486-211556	13007s
	10	0.0549	0.0516	0.1257	0.1166	0.7477	229936-218751	12937s
	11	0.0545	0.0512	0.1263	0.1168	0.7477	236054-225312	12703s
	12	0.0544	0.0525	0.1301	0.1200	0.7383	240978-230814	8823s
	13	0.0541	0.0513	0.1261	0.1163	0.7383	245332-235830	9401s
	14	0.0551	0.0517	0.1267	0.1172	0.7383	248927-240056	9781s
	15	0.0553	0.0519	0.1271	0.1179	0.7336	252237-243952	10152s

	- No improvement after epoch 5.

	(*) $$b41.  Note that the times given are on a half busy gpu,
	around 1.5 sent/sec.  On an empty machine it should run at twice
	this speed.

	- run_conll07_15epoch.mat: models
	- run_conll07_data.mat: data

	+ If this doesn't work:
	- Try other kernel?
	- Try shuffling?
	- Try other vectorparser update methods?
	- Try p=0.9?
	- Try to find better feature set?
	- Try bansal?
	- Try pa
	- Try fv031a
	- Check train vs test, overfitting?
	- Try starting featselect from best 14, 13, etc.


2014-08-16  Deniz Yuret  <dyuret@ku.edu.tr>

	* conllWSJToken:
	- test-split: the 02 directory in conllWSJToken files contain:
	sec	lines			tokens	sentences
	00	48372 (1-48372)		46451	1921 (1-1921)
	01	49626 (48373-97998)	47633	1993 (1922-3914)
	23	59100 (98999-157098)	56684	2416 (3915-6330)
	24	34199 (157099-191297)	32853	1346 (6331-7676)

	- the 00 directory contains contents of 02-21
	- the 01 directory contains contents of 22
	sec	lines	tokens	sentences
	02-21	989860	950028	39832
	22	41817	40117	1700


	* ZN11: Zhang and Nivre, ACL 2011 report 92.9.
	- 02-21 training, 22 devel, 23 testing.
	- using Penn2Malt for dependency conversion.
	- beamsize: 64.
	- sec22 UAS:93.14 LAS:?    UEM:50.12
	- sec23 UAS:92.9  LAS:91.8 UEM:48.0
	- best cited K&C10 model 1: UAS:93.0


	* run_conll07_15epoch.m: Final version of vectorparser with kernel
	caching gets 6 sentences or 153 words per second, which means one
	epoch should be less than an hour!  Now let's run this for 15
	epochs and see what happens.  Until now we always stopped when the
	improvement stopped for one epoch.  Compare to GN13 hybrid-dynamic
	goes to 87.62=12.38 head.

	epoch	stat	move	head	word	sent	nsv(fin-uniq)	time(*)
	1	0.0469	0.0586	0.1477	0.1370	0.7664	63668-63359	255s (static)
	2	0.0524	0.0512	0.1305	0.1229	0.7523	113001-102704	14509s (dynamic)
	3	0.0508	0.0501	0.1259	0.1170	0.7430	143697-129767	13400s
	4	0.0519	0.0503	0.1259	0.1175	0.7477	165416-150682	13881s
	5	0.0513	0.0497	0.1237	0.1150	0.7430	182066-167586	13707s
	6	0.0523	0.0511	0.1271	0.1172	0.7523	195499-181295	13799s
	7	0.0532	0.0512	0.1259	0.1156	0.7477	206538-193049	13616s

	(*) Note that the times given are on a half busy gpu, around 1.5
	sent/sec.  On an empty machine it should run at twice this speed.

	- run_conll07_15epoch.mat: models
	- run_conll07_data.mat: data


2014-08-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* vectorparser.m: Implement kernel cache and test on vectorparser.
	Static training should set most of the SVs and leave a cache for
	the oracle path states using those SVs.  At the beginning of each
	epoch, vectorparser can use the states visited in the last epoch
	and the SVs from the last epoch to construct a new cache in 300
	secs.  The new SVs go into a different matrix
	(use svtr2 again).  Whenever there is a cache hit no need to
	recalculate the scores with the big matrix.  model.X should store
	the instances from the last epoch.  Use mat2str of X vectors for
	keys?

	Before kernel cache:
	w = load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv015a_dump.mat')
	% w.trn: 763702 inst, 398439 word, 16588 sent
	newFeatureVectors;
	m0 = struct('kerparam', struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3),...
                       'parser', @archybrid, 'feats', fv015a, 'step', 1e5, 'batchsize', 1e3);
	m1 = perceptron(w.trndump.x, w.trndump.y, m0);   % 223 secs 57809 unique sv
	m2 = vectorparser(m1, w.trn(1:200));  % 97.58 secs for the first 100 sentences

	After kernel cache:
	>> m1.X = w.trndump.x;
	>> m2b = vectorparser_dbg(m1, w.trn(1:200));
	% 319.54 secs to run the perceptron and compute 763702 cache scores
	% Hopelessly slow trying to cache with mat2str keys.

	For faster cache, use dot product with a random vector:
	>> r = rand(1, 1401);
	>> a = r * w.trndump.x;  % 1,1401 * 1401,763702
	>> size(a)  % 1,763702
	>> min(a)   % -6.9922
	>> max(a)   % 8.9943
	>> mean(a)  % 0.7074
	>> std(a)   % 1.7632 and the distribution is almost gaussian
	>> ux = unique(w.trndump.x', 'rows');  % 756727,1401
	% almost all x vectors are unique
	% how do you map gaussian to uniform?
	>> b = normcdf(a, mean(a), std(a)); % now b is uniform in [0,1]
	>> c = round(b*1e6); % c is uniform int in [0,1e6]
	>> d = unique(c);  % gives 530540 unique values
	>> numel(unique(round(b*1e6)))  % 530540 (70.11%)
	>> numel(unique(round(b*1e7)))  % 728396 (96.26%)
	>> numel(unique(round(b*1e8)))  % 753765 (99.61%)
	>> numel(unique(round(b*1e9)))  % 756470 (99.97%)
	>> numel(unique(round(b*1e10))) % 756710
	>> numel(unique(round(b*1e11))) % 756727 (100.00%)

	Trying again:
	>> [m2c,tmp] = vectorparser_dbg(m1, w.trn(1:200));
	% 326 seconds to compute kernel cache scores
	% 12 seconds to initialize the hash table
	% 65.31 seconds for the first 100 sentences
	% need to profile on an empty machine:
	m2 = vectorparser(m1, w.trn(1:200));  % 36.87 secs for the first 100 sentences
	[m2c,tmp] = vectorparser_dbg(m1, w.trn(1:200));  % 19.65 secs for first 100
	tmp.cache.hit  % 6636
	tmp.cache.miss % 3234

	% it turns out score1 is best computed on the gpu (in case of cache miss)
	% and score2 on the cpu.  At 200 sentences profiling shows:
	score1(gpu,3236): 18.294  score1(cpu):82.748
	score2(cpu,9870): 2.095   score2(gpu):10.794

	% and here is mtimes vs bsxfun:
	score1(gpu,bsxfun): 18.294  score1(gpu,mtimes): 34.101
	score2(cpu,bsxfun): 2.095   score2(cpu,mtimes): 2.179

	% After 12K/18K it is at half speed.  Maybe we should have kept
	score2 in the gpu as well.  How many new SVs are we getting?
	20-50K per epoch.  At that size gpu will win.  Putting everything
	back on gpu.  In a more realistic experiment where svtr1 has 100K
	and svtr2 has 30K support vectors we have for 200 sentences:

	bsxfun:
	score1(gpu,3067):26.591		score1(cpu):181.869
	score2(gpu,9870):34.035		score2(cpu):164.859
	mtimes:
	score1(gpu,3059):52.574 	score1(cpu,3070):396.305
	score2(gpu,9870):59.453 	score2(cpu,9870):547.821

	- no cache: 135.78s for 200sent, nsv: 130242-130067
	- with cache: 78.07s.

	* DONE:
	+ featselect finished, check archybrid_conll07EnglishToken_wikipedia2MUNK-100_cache.mat
	+ check run_conll07_ablation on b40: kernel started.


2014-08-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* GN13: Fig 1 shows that running too many static training
	iterations is not good, they only do it for k=1 epoch.  They do a
	total of 15 epochs (14 dynamic).  Also in dynamic oracle training
	they pick the model move with p=0.9.  Finally they shuffle
	sentences every iteration. (TODO)
	[actually GN13 suggests 1 epoch]
	[Yoav says k=0 and p=1 is also ok, which corresponds to no static]


2014-08-13  Deniz Yuret  <dyuret@ku.edu.tr>

	* beamparser.m: Training. (TODO)
	- We do not update until parse is over.
	- At that point we update if the heads do not match gold.
	- We have the (wrong) z move sequence with its features.
	- We need to compute the correct y sequence with its features.
	- If a state belongs to both y and z and the moves are different we do the usual update.
	- If a state is only in y or in z do we update?  Do we check what the model would have done?
	- Yue says he does update if only in y or z.  Try different variations.

	- early stopping?

	- graph based parser: score agenda states rather than candidate moves?  difference?

	- pa may be worth looking at again for fast convergence.

2014-08-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* beamparser.m: profiling:
	% http://www.mathworks.com/help/matlab/matlab_prog/profiling-for-improving-performance.html
	>> !taskset -pc 0 18893  % set affinity to single cpu
	>> profile -timer real
	>> profile -timer cpu
	% modify score function to return random matrix
	% calling without gpu
	>> profile on
	>> [a,b]=beamparser(m1, wiki.dev(1:10), 'update', 0, 'beam', 10, 'gpu', 0);
	>> profile off
	>> p=profile('info')
	>> profsave(p)
	% profile info saved in html under profile_results.

	* beamparser.m: Computing candidate scores in bulk rather than one
	at a time:

	For 10 sentences, 189 words, 358 moves, 95686 sv, 1401 xdims:
	beam	old	new	wps
	0	8.5	-	22	(vectorparser)
	1	15	12	15.75
	2	23	26	7.27
	5	46	28	6.75
	8	-	29	6.52
	10	86	31	6.10
	16	-	33	5.73
	32	-	40	4.73
	64	475	54	3.50



2014-08-11  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_featselect.m: It does not look like we are going to get a
	better feature set with backward elimination.

	conll07b with bansal100 and fv808 ended with:
	epoch	gmove	move	head	word	sent
	1	0.0492	0.0591	0.1523	0.1374	0.7411	first-static-epoch on dev
	8	0.0425	0.0526	0.1362	0.1235	0.7049	best-static-epoch on dev
	10	0.0458	0.0487	0.1257	0.1157	0.6933	best-dynamic-epoch on dev
	1tst	0.0444	0.0567	0.1417	0.1324	0.7383	first-dynamic-epoch on tst
	0tst	0.0484	0.0540	0.1347	0.1270	0.7383	best-dynamic-epoch on tst

	- GN13 hybrid-static is 86.43=13.57 head.
	- GN13 hybrid-dynamic goes to 87.62=12.38 head.
	There must be better feature/embedding combinations.
	However note also that my experiments are not using the whole training set.

	>> load 'archybrid_conll07EnglishToken_wikipedia2MUNK-100_cache.mat'
	>> [a,b,c] = run_sort_bestfeats(cache);
	best(15): 0.0473399
	>> [find(isfinite(b')) 100*b(isfinite(b))']

	11.0000    5.2476
	12.0000    4.9539	izvlcazmsj (start:5.4892)
	13.0000    4.9149
	14.0000    4.7925
        15.0000    4.7340	izvlcazmsj (switch to diarnxtnht)

	16.0000    4.7340	diarnxtnht (last)
	17.0000    4.7611
	18.0000    4.7611	aefsjihmbv (start:4.8532,done)
	19.0000    4.7925

	28.0000    5.0005
	29.0000    5.0601	vkuvhmnnsm (switch to bxofpzrjcr)
	30.0000    4.8521
	31.0000    4.8304	lvmbqppcab (last)
	32.0000    4.8304
	33.0000    4.8857
	34.0000    4.9409	vkuvhmnnsm (start:5.6658)

	35.0000    5.0406
	36.0000    5.0406
	37.0000    5.0515
	38.0000    5.2281
	39.0000    5.3007	lvmbqppcab (start:5.3007)

	80.0000    7.4093	nslylpeu (last)
	81.0000    7.5729
	82.0000    7.6433
	83.0000    7.8448
	84.0000    7.9976	nslylpeu (start:7.9976)

	* fv015a: is a local minimum.

	See if we can improve over fv015a by testing other features from
	fv130. It turns out not: Numbers below show 1/100 percentage
	points lost (-) or gained (+) from deleting (-) or adding (+) a
	feature to fv015a.
	archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv130_fs.log

	>> load 'archybrid_conll07EnglishToken_wikipedia2MUNK-100_cache.mat'
	>> run_sort_bestfeats(cache) best(15): 0.0473399

	[-201]    '-n0w'   [   0]    '+s0h+'   [  -7]    '+s0ac'   [ -10]    '+s1l1l>' [ -13]    '+s1ac'   [ -17]    '+s0r1-'  [ -22]    '+s2d='   [ -35]    '+s2r1l<' [ -59]    '+n0l1l<'
	[-139]    '-s0w'   [   0]    '+s1h+'   [  -7]    '+n0l1l>' [ -10]    '+s1r1l>' [ -13]    '+s2r>'   [ -17]    '+s1r1c'  [ -24]    '+n0l1-'  [ -37]    '+s1+'    [ -68]    '+s0l1r<'
	[ -71]    '-s1w'   [   0]    '+s2h+'   [  -8]    '+s0l1c'  [ -10]    '+s0r1r>' [ -14]    '+n0l1l=' [ -17]    '+s2l1r=' [ -24]    '+n2w'    [ -37]    '+s1h-'   [ -75]    '+s1l1l<'
	[ -48]    '-n1w'   [  -1]    '+s0r1r=' [  -8]    '+s1l1c'  [ -10]    '+n0l1r>' [ -14]    '+s2l1l=' [ -17]    '+s2l1w'  [ -25]    '+s1l1+'  [ -37]    '+s2+'    [ -80]    '+s1l1r<'
	[ -46]    '-n0l1w' [  -2]    '+s0l1r>' [  -8]    '+s1-'    [ -11]    '+s0aw'   [ -14]    '+s2ac'   [ -17]    '+s0r1r<' [ -25]    '+s0l1-'  [ -37]    '+s2h-'   [ -83]    '+s2l1l<'
	[ -35]    '-s0l1w' [  -2]    '+s0r1l>' [  -8]    '+s2r1w'  [ -11]    '+s2r1c'  [ -14]    '+s2r1l>' [ -17]    '+s1l>'   [ -25]    '+s1r>'   [ -39]    '+n1+'    [ -85]    '+s2l1r<'
	[ -32]    '-s1r1+' [  -3]    '+s0r>'   [  -9]    '+s1r1r=' [ -11]    '+s2r1r=' [ -14]    '+s2r1r>' [ -18]    '+s1aw'   [ -25]    '+s1r1l<' [ -39]    '+s1d>'   [-110]    '+s2l<'
	[ -26]    '-s0r1w' [  -4]    '+s1r1w'  [  -9]    '+s2l1l>' [ -11]    '+s2l1c'  [ -15]    '+s1l1l=' [ -18]    '+s2r='   [ -28]    '+s0l='   [ -39]    '+s2r1r<' [-131]    '+s1l<'
	[ -25]    '-n1c'   [  -5]    '+n0l>'   [  -9]    '+s0l1l>' [ -11]    '+n2-'    [ -15]    '+s2l1+'  [ -18]    '+s1d='   [ -28]    '+s2l>'   [ -43]    '+n0+'    [-134]    '+s2r<'
	[ -19]    '-n0l1c' [  -6]    '+s1l1r=' [ -10]    '+s0-'    [ -12]    '+n1-'    [ -15]    '+s2-'    [ -19]    '+s0r='   [ -29]    '+s2l1-'  [ -43]    '+n2+'    [-145]    '+s1r<'
	[ -18]    '-s2aw'  [  -6]    '+s1l1w'  [ -10]    '+n0l1+'  [ -12]    '+s2l='   [ -15]    '+s1l1r>' [ -20]    '+s2r1l=' [ -29]    '+s1r1r<' [ -44]    '+s0+'    [-157]    '+s0l<'
	[ -17]    '-n0c'   [  -6]    '+s0l1r=' [ -10]    '+n0-'    [ -12]    '+n2c'    [ -16]    '+s0l1+'  [ -20]    '+s0r1l<' [ -32]    '+s1r1-'  [ -44]    '+s0d>'   [-163]    '+n0l<'
	[ -16]    '-s1c'   [  -6]    '+s0r1+'  [ -10]    '+s1r1l=' [ -12]    '+s1r1r>' [ -16]    '+s2r1+'  [ -21]    '+s1l1-'  [ -34]    '+s2d>'   [ -44]    '+s0h-'   [-171]    '+s0r<'
	[ -15]    '-s0c'   [  -6]    '+n0l1r=' [ -10]    '+s2c'    [ -13]    '+s1l='   [ -16]    '+s2l1r>' [ -21]    '+n0l='   [ -34]    '+s2r1-'  [ -50]    '+n0l1r<' [-238]    '+s2d<'
	[  -6]    '-s0r1c' [  -7]    '+s0r1l=' [ -10]    '+s2w'    [ -13]    '+s0l1l=' [ -16]    '+s0d='   [ -21]    '+s0l>'   [ -35]    '+s1r='   [ -58]    '+s0l1l<' [-267]    '+s1d<'
																				       [-300]    '+s0d<'


	* fv031a: is also a local minimum
	run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv130', 'fv031a');
	13-Aug-2014 08:36:39 Loading archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv130_dump.mat


	* beamparser: can we gpu parallelize the beam score calculation?
	we only compute move scores for parent, not the children!?  can we
	do kernel hashing?  Experimenting with conll07b_m2 and dev (fv808
	bansal encoding trn split run_conll07_dbg.m):

	>> load conll07b_m2.mat
	epoch	stat	move	head	word	sent
	10	0.0458	0.0487	0.1257	0.1157	0.6933
	>> load conll07b_dev.mat
	>> tic;[a,b] = beamparser(m2, dev(1:10), 'update', 0, 'beam', 10);toc;
	Elapsed time is 106.003019 seconds.
	>> tic;[c,d] = vectorparser(m2, dev(1:10), 'update', 0);toc;
	Elapsed time is 16.087695 seconds.

	First version of beamparser: can only parse with existing model, no training.

	Way too slow!
	Approx linear with beam size.  Approx eq to vectorparser at beam=1.

	Make it output scores and moves. done.

	Comparison on 100 sentences of dev with vectorparser:
	beamparser made more than twice as many errors, must be bug?

	Looking at the first 10 sentences.  beamparser finds better
	scoring move sequences for 8 of them with beam=10.

	s (len)	vparser(16s)	b1(17s)		b2(28s)		b5(58s)		b10(107s)	b64(604s)
	1 (49)	4.73e+14	4.73e+14	5.65e+14	6.08e+14	6.01e+14	6.31e+14
	2 (5)	2.92e+13	2.92e+13	2.92e+13	2.92e+13	2.92e+13	2.92e+13
	3 (20)	3.72e+14	3.72e+14	3.80e+14	3.85e+14	3.94e+14	3.96e+14
	4 (12)	1.30e+14	1.30e+14	1.30e+14	1.30e+14	1.30e+14	1.34e+14
	5 (29)	5.12e+14	5.12e+14	4.65e+14	5.24e+14	5.28e+14	5.57e+14
	6 (15)	3.75e+14	3.75e+14	4.18e+14	4.18e+14	4.26e+14	4.26e+14
	7 (9)	2.15e+14	2.15e+14	2.15e+14	2.15e+14	2.15e+14	2.15e+14
	8 (8)	1.67e+14	1.67e+14	1.91e+14	2.22e+14	2.22e+14	2.22e+14
	9 (17)	3.86e+14	3.86e+14	3.93e+14	3.90e+14	3.95e+14	4.06e+14
	10 (25)	3.94e+14	3.94e+14	3.99e+14	4.02e+14	4.06e+14	4.06e+14
	head	0.1164		0.1164		0.1693		0.2434		0.2698		0.3175

	+ vectorparser and beam=1 are equal, good.
	+ generally sentence scores increase with beam size, good.
	+ beam=2 is worse than beam=1 in sent=5?
	+ beam=10 is worse than beam=5 in sent=1?
	+ beam=5 is worse than beam=2 in sent=9?
	+ it is totally normal for larger beams to lose the correct answer.
	+ only inf beam guaranteed higher score.

	+ higher beams have progressively worse head error.  unless there
	  is a bug, seems we can't use a model not trained with beamsearch.

	- would it matter if we used only static training?

	>> m1 = run_static('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv015a');
	epoch	nsv	trnerr		deverr
	1	57809	0.0753553	0.0473399
	2	74079	0.0447845	0.0438292
	3	82786	0.0343079	0.0429624
	4	88578	0.0275631	0.0424423
	5	92618	0.0229998	0.0423773
	6	95686	0.0193832	0.0421606	best
	7	98266	0.0169831	0.0422256
	>> wiki = load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv015a_dump.mat')
	>> eval_model(m1, wiki.dev, wiki.devdump)
	epoch	stat	move	head	word	sent
	6	0.0422	0.0523	0.1350	0.1203	0.7084
	% compare with conll07b_m2 (bansal100,dynamic10,fv808) stat:4.58 head:12.57
	>> tic;[m10,e10] = beamparser(m1, wiki.dev(1:10), 'update', 0, 'beam', 10);toc;
	Elapsed time is 85.803560 seconds.
	>> eval_conll(wiki.dev(1:10), e10);
	head_pct: 0.2116  sum(e10.s1):1.6775e+09
	>> tic;[m5,e5] = beamparser(m1, wiki.dev(1:10), 'update', 0, 'beam', 5);toc;
	Elapsed time is 46.477789 seconds.
	>> eval_conll(wiki.dev(1:10), e5);
	head_pct: 0.2328  sum(e5.s1):1.6575e+09
	% Beam search helped this time?  It was a fluke.  Here is the full table:
	>> dmp = {e0,e1,e2,e5,e10,e64};
	>> for i=1:10 fprintf('%d',i); for j=1:6 dj=dmp{j}; fprintf('\t%.2e', sentence_score(dj,i)); end; fprintf('\n'); end

	s(len)	vparser(13s)	b1(15s)		b2(23s)		b5(46s)		b10(86s)	b64(475s)
	1	3.67e+08	3.67e+08	3.83e+08	4.17e+08	4.19e+08	4.21e+08
	2	2.69e+07	2.69e+07	2.69e+07	2.69e+07	2.69e+07	2.69e+07
	3	1.97e+08	1.97e+08	2.04e+08	1.99e+08	2.04e+08	2.19e+08
	4	8.10e+07	8.10e+07	8.10e+07	8.10e+07	8.10e+07	8.10e+07
	5	2.73e+08	2.73e+08	2.73e+08	2.81e+08	2.86e+08	2.97e+08
	6	1.45e+08	1.45e+08	1.45e+08	1.45e+08	1.45e+08	1.58e+08
	7	6.80e+07	6.80e+07	7.24e+07	7.24e+07	7.24e+07	7.24e+07
	8	7.01e+07	7.01e+07	7.01e+07	7.01e+07	7.01e+07	7.01e+07
	9	1.10e+08	1.10e+08	1.18e+08	1.18e+08	1.20e+08	1.23e+08
	10	2.26e+08	2.26e+08	2.12e+08	2.47e+08	2.52e+08	2.58e+08
	head	0.1429		0.1429		0.1958		0.2328		0.2116		0.2751

	- figure out how to make it faster.


	* multi-epoch-featselect:
	>> run_static('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv034');
	Try and see if multiple epochs will get rid of irrelevant features. $$b42

	This is fv034: (34 features)
	epoch	stat	move	head	word	sent
	1	0.0567	0.0722	0.1842	0.1662	0.7883
	2	0.0490	0.0632	0.1609	0.1448	0.7557
	3	0.0462	0.0600	0.1529	0.1385	0.7401
	4	0.0451	0.0587	0.1498	0.1362	0.7305
	5	0.0446	0.0582	0.1485	0.1346	0.7275	best-epoch
	6	0.0446	0.0583	0.1486	0.1345	0.7285

	Compare to fv808: (18 features)
	1	0.0485	0.0597	0.1518	0.1360	0.7506
	2	0.0453	0.0563	0.1430	0.1284	0.7230
	3	0.0442	0.0549	0.1398	0.1248	0.7119
	4	0.0437	0.0542	0.1375	0.1228	0.7114
	5	0.0435	0.0538	0.1370	0.1224	0.7144

	Doing featselect based on first epoch may not be a good idea!

	Will the larger feature set win with more epochs?
	Another multi-epoch experiment comparing:
        15.0000    4.7340	fv015a
	31.0000    4.8304	fv031a

	fv015a
	epoch	stat	move	head	word	sent
	1	0.0473	0.0580	0.1490	0.1332	0.7441
	2	0.0438	0.0543	0.1399	0.1244	0.7260
	3	0.0430	0.0528	0.1362	0.1209	0.7200
	4	0.0424	0.0524	0.1351	0.1198	0.7114
	5	0.0424	0.0528	0.1360	0.1211	0.7104
	6	0.0422	0.0523	0.1350	0.1203	0.7084	best stat
	7	0.0422	0.0518	0.1347	0.1201	0.7104

	fv031a
	epoch	stat	move	head	word	sent
	1	0.0483	0.0621	0.1587	0.1433	0.7547
	2	0.0448	0.0588	0.1507	0.1357	0.7366
	3	0.0434	0.0562	0.1443	0.1296	0.7240
	4	0.0425	0.0555	0.1424	0.1282	0.7210
	5	0.0421	0.0552	0.1413	0.1272	0.7134	best stat
	6	0.0424	0.0549	0.1416	0.1274	0.7169

	- As anticipated, the larger feature set starts a bit worse and
	closes the gap in multi-epoch, but the advantage is too small to
	be worth trying multi-epoch feature selection.

2014-08-10  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	+ write script to run featselect on short.q.  job id can be random so we can check.
	+ try backward elimination instead of forward selection.
	+ run_static should take corpus, parser name and feature name as args.
	+ check to see if it can load existing mat files.
	+ also compare bansal and wiki on fv808.

	= beam search training may not need dynamic oracle (which cannot be minibatch'ed)!  needs code.
	+ Beam search parser: read paper.  read redshift.  minibatch?
	+ we can start by writing a beam decoder to parse with existing models.  no training.
	+ how often does the gold sequence have a total score better than greedy sequence?


2014-08-09  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_static.m: take corpus, parser name and feature name as args.
	also compare bansal and wiki (and others) on fv808.  running on
	split conll07 trn.

	>> run_static('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv808');

	First Epoch
	epoch	stat	move	head	word	sent	parser
	1	0.0485	0.0597	0.1518	0.1360	0.7506	archybrid
	1	0.0487	0.0592	0.1506	0.1353	0.7536	archybrid13
	1	0.0505	0.0689	0.1649	0.1469	0.7622	arceager13
	1	0.0512	0.0697	0.1682	0.1501	0.7717	arceager

	Fifth Epoch
	epoch	stat	move	head	word	sent	parser
	5	0.0435	0.0538	0.1370	0.1224	0.7144	archybrid (unfinished)
	5	0.0433	0.0539	0.1380	0.1234	0.7174	archybrid13 (unfinished)
	5	0.0454	0.0634	0.1548	0.1379	0.7310	arceager (unfinished)
	5	0.0456	0.0633	0.1527	0.1356	0.7285	arceager13 (unfinished)

	Conclusion: no significant difference with 13 versions.  archybrid
	better but we are not using s0h.  We should try arceager with s0h.

	>> run_static('archybrid', 'conll07EnglishToken_bansal100', 'fv808')
	epoch	stat	move	head	word	sent
	1	0.0492	0.0591	0.1523	0.1374	0.7411
	2	0.0448	0.0551	0.1424	0.1285	0.7134
	3	0.0438	0.0538	0.1386	0.1251	0.7054
	4	0.0431	0.0536	0.1380	0.1246	0.7054
	5	0.0430	0.0531	0.1373	0.1241	0.7044
	6	0.0430	0.0532	0.1374	0.1245	0.7059

	* run_featselect.m: Try backward feature selection.
	Running on split conll07 trn.

	>> run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv084', 'fv018'); aefsjihmbv:done
	>> run_featselect('archybrid', 'conll07EnglishToken_bansal100', 'fv084', 'fv018'); lnaumcolef:stopped
	>> run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv084', 'fv084'); nslylpeu
	>> run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv084', 'fv039'); lvmbqppcab
	>> run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv084', 'fv012'); izvlcazmsj
	>> run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv084', 'fv034'); vkuvhmnnsm

	The first backtrack immediately gives a relative order of
	features.  However we can get misleading results for redundant
	features.  We need to pick one of +-1 (exists/doesn't), +-9 (head
	exists/doesn't), +-{2,5,6} encodings for children, {3,7,-7}
	encodings for distance, skip 0 and -3 (token features, which are
	concatenations of word+context).  We can try alternatives later.
	fv084 picks = encoding for children and >= encoding for distances,
	but has both bits for 1 and 9.

	The fv084 starting features on wiki100 give 0.0799762 for conll07
	trn split into sec02 dev and the rest trn with archybrid.  This is
	under 0.05 for best feature combination.  A lot of starting
	features have negative effect.

	Here is fv084 (all) vs fv017 (best):			(bansal100)
	best(84): 0.0799762	    best(17): 0.0476108		best(16): 0.0473616
	[-196]    'n0w'   	    [-203]    'n0w'   		[-217]    'n0w'       '[0 0 4]'
	[-119]    's0w'   	    [-125]    's0w'   		[-161]    's0w'       '[-1 0 4]'
	[ -73]    'n1w'   	    [ -59]    's1w'   		[ -74]    's1w'       '[-2 0 4]'
	[ -43]    's1w'   	    [ -59]    'n1w'   		[ -50]    'n1w'       '[1 0 4]'
	[ -20]    'n0l1w' 	    [ -41]    'n0l1w' 		[ -43]    'n0l1w'     '[0 -1 4]'
								[ -26]    's0c'       '[-1 0 -4]'
	[ -20]    'n0c'   	    [ -17]    'n0c'   		[ -23]    's0l1w'     '[-1 -1 4]'
	[ -19]    'n1c'   	    [ -15]    'n1c'   		[ -20]    's1ac'      '[-2 0 -8]'
	[ -16]    's0c'   	    [ -16]    's0c'   		[ -20]    's1r1l='    '[-2 1 -2]'
	[ -11]    's0l1w' 	    [ -21]    's0l1w' 		[ -20]    's0r1w'     '[-1 1 4]'
	[ -10]    's1c'   	    [ -13]    's1c'   		[ -16]    'n0c'       '[0 0 -4]'
								[ -15]    's1c'       '[-2 0 -4]'
	[  -8]    's2l1c' 	    				[ -15]    'n0l1c'     '[0 -1 -4]'
	[  -8]    's0l1c' 	    [ -12]    's0l1c' 		[ -13]    'n1c'       '[1 0 -4]'
	[  -8]    'n0l1c' 	    [ -11]    'n0l1c' 		[  -9]    's0d>'      '[-1 0 7]'
	[  -7]    's2l1r='	    				[  -2]    's0r1c'     '[-1 1 -4]'
	[  -7]    's2aw'
	[  -7]    's1r1+'
	[  -7]    's0ac'
	[  -7]    's0d>'
	[  -6]    's0l1+'
	[  -6]    's0aw'
	[  -5]    's2c'
	[  -5]    's2w'   	    [ -12]    's2w'
	[  -5]    's1l1c'
	[  -5]    's0r1w' 	    [ -21]    's0r1w'
	[  -4]    's2ac'
	[  -4]    'n0l1+'
	[  -3]    's2-'
	[  -3]    's2d>'
	[  -3]    's2r1w'
	[  -3]    's1l1w'
	[  -3]    's1aw'
	[  -3]    's1r1r='
	[  -2]    's1l1l='
	[  -2]    's0l1l='
	[  -1]    's1ac'
	[  -1]    's0l1r='
	[  -1]    's0r1c' 	    [ -12]    's0r1c'
	[   0]    's2r1c'
	[   0]    's0r='
	[   1]    's2l1w'
	[   1]    's2h+'
	[   1]    's2r1+'
	[   1]    's1r1c'
	[   1]    's1r1w'
	[   1]    's0-'
	[   1]    's0r1l='	    [ -19]    's0r1l='
	[   1]    's0r1r='
	[   1]    'n0-'
	[   2]    's2r1l='
	[   2]    's2r1r='
	[   2]    's1l1r='
	[   2]    's1-'
	[   2]    's1d>'
	[   2]    's1h+'
	[   2]    's0h+'
	[   3]    's2l1+'
	[   3]    's2l='
	[   3]    's0l1-'
	[   3]    's0r1+'
	[   3]    'n0l1l='
	[   3]    'n0l1r='
	[   3]    'n1-'
	[   4]    's1l1-'
	[   4]    's1l1+'
	[   5]    's2l1l='
	[   5]    's2l1-'
	[   5]    'n0l='
	[   7]    's1r1l='		[ -30]    's1r1l='
	[   7]    's1r1-'
	[   8]    's2r='
	[   8]    's1r='
	[   9]    's1l='
	[   9]    's0l='
	[   9]    'n0l1-'
	[  11]    's0h-'
	[  11]    's0+'
	[  11]    's0r1-'
	[  12]    'n0+'
	[  14]    's2r1-'
	[  14]    's1+'
	[  14]    'n1+'
	[  15]    's2h-'
	[  15]    's2+'
	[  15]    's1h-'


2014-08-08  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_static: testing other embeddings on conll07 using archybrid
	and fv008w.  Using full conll07 trn (no dev split) and tst in
	multi-epoch static training.  wiki is better than bansal?!

	epoch	stat	move	head	word	sent	corpus
	5	0.0470	0.0574	0.1481	0.1333	0.7523	conll07EnglishToken_wikipedia2MUNK-100
	7	0.0470	0.0603	0.1501	0.1372	0.7897	conll07EnglishToken_rcv1UNK100
	4	0.0477	0.0603	0.1505	0.1395	0.7430	conll07EnglishToken_bansal100
	4	0.0481	0.0594	0.1503	0.1372	0.7617	conll07EnglishToken_wikipedia2MUNK-50
	6	0.0517	0.0639	0.1573	0.1465	0.7617	conll07EnglishToken_stratos100k5000scaled01
	4	0.0545	0.0656	0.1679	0.1560	0.7664	conll07EnglishToken_cw100scaled
	4	0.0578	0.0696	0.1763	0.1594	0.7570	conll07EnglishToken_hlbl100scaled
	9	0.0607	0.0755	0.1909	0.1821	0.8411	conll07EnglishToken_stratos100k200scaled01
	11	0.0640	0.0797	0.2029	0.1878	0.8318	conll07EnglishToken_hpca100scaled01

	* run_max_beam_score.m: We can calculate an upper bound on beam
	parser for an existing model by comparing the total score of the
	correct tree with the total score of the predicted tree.  Using
	the last static model from mxrep.

	>> load mxrep_m1
	>> load mxrep_dev
	% confirmed stat=3.1627 on dev.
	>> [~,devparse] = vectorparser(m1, dev, 'update', 0);
	>> eval_conll(dev, devparse);
	% confirmed move=3.85 head=10.09
	>> [~,devdump.score] = perceptron(devdump.x, devdump.y, m1, 'update', 0);
	% mxrep_devdump does not have scores
	>> [sy,sz,sumy,sumz] = run_max_beam_score(dev, devdump, devparse);
	>> numel(find(sy>sz))/numel(sy) % 0.4895: pct of moves where gold score is better than guess
	>> numel(find(sy==sz))/numel(sy) % 0.0023
	>> numel(find(sy<sz))/numel(sy) % 0.5082
	>> numel(find(sumy>sumz))/numel(sumy) % 0.4465: pct of sentences where total gold score is better than guess
	>> numel(find(sumy==sumz))/numel(sumy) % 0.0082
	>> numel(find(sumy<sumz))/numel(sumy) % 0.5453

	This means our answers would change (at least) in 44% of the
	sentences, possibly for the better.

	* vectorparser.m: Debugging the update rule. $$i30

	- trainparser_gpu also have score(c=inf)=-inf, why did that not hurt?
	c = p.oracle_cost(h);
	scores(c==inf) = -inf;
	[~,move] = max(scores);
	[mincost, bestmove] = min(c);
	if c(move) > mincost % update

	- vectorparser has:
	cost = p.oracle_cost(h);
	[mincost, mincostmove] = min(cost);
	[maxscore, maxscoremove] = max(score);
	if cost(maxscoremove) > mincost % update

	- Experiment with conll07 dev vs tst for speed.  Using fv808.
	- Static training.  13-epoch, 6.6820 tst err.
	- vectorparser Dynamic training.  1-epoch:
	epoch	stat	move	head	word	sent
	1	0.0685	0.0776	0.1935	0.1782	0.7991
	- vectorparser_dbg (same as trainparser_gpu), 1-epoch:
	epoch	stat	move	head	word	sent
	1	0.0689	0.0782	0.1949	0.1798	0.7991
	- confirm same as trainparser_gpu... done.
	- vectorparser 5-epochs: (no help at all?)
	epoch	stat	move	head	word	sent
	0	0.0668	0.0764	0.1903	0.1769	0.7850
	1	0.0685	0.0776	0.1935	0.1782	0.7991
	2	0.0690	0.0787	0.1951	0.1800	0.7991
	3	0.0680	0.0783	0.1937	0.1794	0.7991
	4	0.0683	0.0780	0.1941	0.1800	0.7944
	5	0.0687	0.0784	0.1951	0.1807	0.8037
	- vectorparser_dbg 5-epochs:
	epoch	stat	move	head	word	sent
	0	0.0668	0.0764	0.1903	0.1769	0.7850
	1	0.0689	0.0782	0.1949	0.1798	0.7991
	2	0.0687	0.0781	0.1941	0.1791	0.7991
	3	0.0686	0.0790	0.1963	0.1810	0.7991
	4	0.0688	0.0787	0.1955	0.1805	0.8037
	5	0.0684	0.0780	0.1943	0.1789	0.8037

	- At this size dynamic oracle training does not seem to help.  To
	see the difference we need a bigger size problem.



==> run_max_beam_score.m <==
function [sy, sz, sumy, sumz] = run_max_beam_score(dev, devdump, devparse)
y = devdump.y;
z = devparse.z;
sy = devdump.score(sub2ind(size(devdump.score), y, 1:numel(y)));
sum_sy = sum(sy)
sz = devparse.score(sub2ind(size(devparse.score), z, 1:numel(z)));
sum_sz = sum(sz)

ns = numel(dev)
nw = 0;
nt = 0;
for s=1:ns
  w = numel(dev{s}.head);
  t = 2*w-2;
  at = nt + 1;
  bt = nt + t;
  sumy(s) = sum(sy(at:bt));
  sumz(s) = sum(sz(at:bt));
  nw = nw + w;
  nt = nt + t;
end



==> run_sort_bestfeats.m <==
function fsorted = run_sort_bestfeats(cache, imin)

nfeats = 199;
bestfeats = cell(1,nfeats);
besterror = inf(1,nfeats);
cachekeys = keys(cache);
for i=1:numel(cachekeys)
  fstr = cachekeys{i};
  ferr = cache(fstr);
  flen = size(eval(fstr), 1);
  if ferr < besterror(flen)
    besterror(flen) = ferr;
    bestfeats{flen} = fstr;
  end
end

if nargin < 2
  [emin, imin] = min(besterror);
end
emin = besterror(imin);
fmin = eval(bestfeats{imin});
fprintf('best(%d): %g\n', imin, emin);

for i=1:imin
  fsorted{i,1} = 0;
  fsorted{i,2} = fsymbol(fmin(i,:));
  fnew = fmin;
  fnew(i,:) = [];
  fk = mat2str(sortrows(fnew));
  if isKey(cache, fk)
    ferr = cache(fk);
    fsorted{i,1} = round(10000 * (emin - ferr));
  end
end

fsorted = sortrows(fsorted, 1);

end


2014-08-07  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_fs_conll07_arceager13.m: featselect with arceager13 starting
	at fv008 using fv136 (head features), conll07 trn split into trn
	and dev, bansal100 encoding. ($$b43)


	* run_fs_conll07_aug7: featselect stopped and restarted from a
	better initial state: fv008.  Still using conll07a_ variables
	(conll07 trn split into dev and trn, bansal100 token encoding).
	Get the new head features in there: fv136.  Eliminate features
	never found useful from fv130: decided not to.  Try eager/eager13
	as well.  Remove single-choice moves from training:done. Speed is
	about 100 secs per feature.  We can try 864 features a day.  That
	decides roughly six features a day.  Started at Aug 7 10:00AM.
	($$b42)

	dev 1989 sent -> 92290 move -> 87202 multi-choice move
	trn 16588 sent -> 763702 move -> 720961 multi-choice move

	- Compare basic feature set with other encoding and other
	arctypes.  (maybe with the addition of s0r=(helps .1%) and s0h(not
	for hybrid) and the constant 1(not helpful)).  Maybe best to wait
	and do an actual fselect.

	- NOTE: we do not have a constant feature but we do have features
	that are always on that will serve the same purpose: s0h- for
	archybrid, s0+ are some of them.  It is probably not a good idea
	to rely on these and we should include them explicitly.  Tried
	adding s0+ to fv008, did not improve.

	- NOTE: fv008 gave 0.0591 on conll07.  fv808 gave 0.0492 first
	epoch on conll07.  It gave 0.0371 on its original data
	conllToken_wikipedia2MUNK-50.  bansal5k experiments used 5k
	sentences from conllWSJToken_bansal100 and got 0.0504 on its own
	dev set. fv808 actually has 1608 dims in this case because
	bansal100 has 100 dim vectors and it includes the context vectors
	for each.  In addition to the 6 basic words in fv008, it has s0l1
	and s2, all the contexts, plus two counts: s1r1l and s0r1l.

	- NOTE: The first feature picked to add to fv008 is s1r1w, which
	was not picked in the previous featselect (but was part of fv804).
	It got rid of both n0c and s0c and replaced them with word
	features s1r1w and s0l1w.  Hopefully this will discover a good
	combo for conll07.

	>> load 'fs_conll07_aug7.mat' (archybrid)
	>> run_sort_bestfeats(cache)
	best(11): 0.0515011   	best(12): 0.0509621	best(13): 0.0502282	best(14): 0.0498842
	[-367]    'n0w'       	[-360]    'n0w'  	[-359]    'n0w'
	[-238]    's0w'       	[-229]    's0w'  	[-230]    's0w'
	[-106]    's1w'       	[-100]    's1w'  	[-104]    's1w'
	[ -94]    'n1w'       	[ -94]    'n1w'  	[ -82]    'n1w'
	[ -81]    'n0l1w'     	[ -63]    'n0l1w'	[ -65]    'n0l1w'
	[ -39]    's1r1w'     	[ -41]    's0r1w'	[ -45]    's1r1w'
	[ -26]    's0r1w'     	[ -36]    's1r1w'	[ -31]    's0l1w'
	[ -26]    'n0c'       	[ -25]    's1c'  	[ -30]    'n0c'
	[ -25]    's1c'       	[ -20]    's0l1w'	[ -28]    's0r1w'
	[ -24]    's0l1w'     	[ -16]    'n0c'  	[ -26]    's1c'
	[  -8]    's0r1l<'    	[ -12]    's2l=' 	[ -15]    's2l='
				[ -12]    'n0l>' 	[  -8]    'n0l>'
							[  -7]    's0r1l<'

	>> load 'fs_conll07_arceager13.mat'
	>> run_sort_bestfeats(cache)
	best(9): 0.0560545	best(10): 0.0542881	best(11): 0.0538064	best(12): 0.0534164	best(13): 0.0526593
	[-494]    'n0w'   	[-510]    'n0w'   	[-502]    'n0w'   	[-507]    'n0w'
	[-324]    's0w'   	[-337]    's0w'   	[-313]    's0w'   	[-268]    's0w'
	[-181]    'n1w'   	[-187]    'n1w'   	[-189]    'n1w'   	[-184]    'n1w'
	[-179]    's1r1r<'	[-177]    's1r1r<'	[-175]    's1r1r<'	[-155]    's1r1r<'
	[ -71]    'n0l1w' 	[ -54]    'n0l1w' 	[ -54]    's1w'   	[ -45]    'n0l1w'
	[ -36]    's1w'   	[ -53]    's1w'   	[ -45]    'n0l1w' 	[ -32]    's0r1w'
	[ -32]    's0r1w' 	[ -42]    's0r1w' 	[ -41]    's0l1w' 	[ -31]    'n2c'
	[ -15]    's0l1w' 	[ -36]    'n2c'   	[ -39]    's0r1w' 	[ -28]    's0l1w'
	[ -15]    'n2c'   	[ -25]    's0l1w' 	[ -35]    'n2c'   	[ -24]    's1w'
				[ -18]    'n0l1r<'	[ -12]    'n0l1r<'	[ -16]    'n0l1r<'
							[  -5]    'n0l1c' 	[ -14]    'n0l1c'
										[  -4]    's1c'


	* conll07b: rerun conll07 experiments with fv808 and new code.  bansal100 encoding.
	- try scode instead of bansal?
	- g&n13 hybrid-static is 86.43=13.57 head.
	- g&n13 hybrid-dynamic goes to 87.62=12.38 head.
	- feature optimization for conll07 & difft arctypes skipping single-choice.
	- fix featselect so it doesn't backup shorter than initfeats.
	- use the whole training set after determining epochs and feats.
	$$b41

	Dynamic oracle:
	epoch	gmove	move	head	word	sent
	(8)	0.0425	0.0526	0.1362	0.1235	0.7049 (last static epoch)
	1tst	0.0444	0.0567	0.1417	0.1324	0.7383 (1st dynamic epoch on test set)
	1	0.0451	0.0535	0.1389	0.1262	0.7205 rest of the results on dev set
	2	0.0451	0.0522	0.1351	0.1229	0.7094
	3	0.0456	0.0518	0.1337	0.1221	0.7084
	4	0.0456	0.0510	0.1320	0.1205	0.7074
	5	0.0454	0.0504	0.1303	0.1193	0.7019
	6	0.0458	0.0502	0.1295	0.1189	0.7014
	7	0.0459	0.0500	0.1288	0.1183	0.7024
	8	0.0458	0.0495	0.1277	0.1174	0.6983
	9	0.0455	0.0490	0.1265	0.1164	0.6948
	10	0.0458	0.0487	0.1257	0.1157	0.6933	best-dynamic-epoch
	11	0.0460	0.0489	0.1266	0.1166	0.6928
	0	0.0484	0.0540	0.1347	0.1270	0.7383	final-testing

	* mxrep: replicate mx experiments with new code.  Note that the
	difference in the last static model is due to the older experiment
	removing single choice moves from training.  In dynamic training
	older code ignored scores of impossible moves completely.  One
	epoch takes about 7 hours which is consistent with before. $$b40.

	Dynamic oracle:
	epoch	gmove	move	head	word	sent
	(4)	0.0316	0.0385	0.1009	0.0887	0.6165 (last static epoch)
	1	0.0384	0.0376	0.0997	0.0883	0.6300
	2	0.0382	0.0368	0.0977	0.0865	0.6200
	3	0.0383	0.0359	0.0955	0.0843	0.6165
	4	0.0382	0.0350	0.0930	0.0822	0.6118
	5	0.0382	0.0346	0.0925	0.0816	0.6100
	6	0.0381	0.0344	0.0917	0.0808	0.6065
	7	0.0382	0.0344	0.0916	0.0811	0.6076
	8	0.0383	0.0342	0.0904	0.0800	0.6071
	9	0.0381	0.0339	0.0897	0.0792	0.6059
	10	out-of-memory

	Compare to original mx results:
	$$i11			TEST				TRAIN
	model	nsv	time	gtrans	ptrans	phead	whead	ptrans	phead	whead
	mx5	169135	963	3.34	3.76	9.83	8.63	-	-
	mx5d1	204369	22472	4.07	3.78	10.02	8.83	3.02	8.37	7.42
	mx5d2	232151	?	4.02	3.63	9.59	8.47	2.73	7.60	6.77
	mx5d3	254535	?	3.99	3.49	9.23	8.16	2.45	6.88	6.13
	mx5d4	273226	?	4.00	3.47	9.20	8.12	2.23	6.28	5.61
	mx5d5	289077	?	3.99	3.39	9.02	7.98	2.02	5.73	5.11
	mx5d6	302335	?	3.99	3.39	8.96	7.92	1.83	5.24	4.67
	mx5d7	314292	?	4.00	3.37	8.90	7.88	1.71	4.92	4.40

==> run/run_conll07_dbg.m <==
% function run_conll07_dbg(parser, feats, trn, dev, tst, savefile)
function run_conll07_dbg(savefile)

if 0
fprintf('%s saving arguments\n', datestr(now));
save([savefile '_parser'], 'parser', '-v7.3');
save([savefile '_feats'], 'feats', '-v7.3');
save([savefile '_trn'], 'trn', '-v7.3');
save([savefile '_dev'], 'dev', '-v7.3');
save([savefile '_tst'], 'tst', '-v7.3');

fprintf('%s initializing model\n', datestr(now));
m0.kerparam = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
m0.parser = parser;
m0.feats = feats;
m0.step = 1e5;
m0.batchsize = 1e3;
save([savefile '_m0'], 'm0', '-v7.3');

fprintf('%s preparing dumps.\n', datestr(now));
[~,trndump] = vectorparser(m0, trn, 'predict', 0, 'update', 0);
save([savefile '_trndump'], 'trndump', '-v7.3');
[~,devdump] = vectorparser(m0, dev, 'predict', 0, 'update', 0);
save([savefile '_devdump'], 'devdump', '-v7.3');
[~,tstdump] = vectorparser(m0, tst, 'predict', 0, 'update', 0);
save([savefile '_tstdump'], 'tstdump', '-v7.3');

fprintf('%s static oracle training.\n', datestr(now));
fprintf('epoch\tgmove\tmove\thead\tword\tsent\n');
m1 = m0;
e1 = inf;
for epoch=1:100
  m = perceptron(trndump.x, trndump.y, m1);
  [e, f] = eval_model(m, dev, devdump, epoch);
  if e < e1
    e1 = e;
    f1 = f;
    m1 = m;
    save([savefile '_m1'], 'm1', '-v7.3');
  else
    break;
  end
end

else % if 0
% load([savefile '_parser']);
% load([savefile '_feats']);
load([savefile '_trn']);
load([savefile '_dev']);
load([savefile '_tst']);
% load([savefile '_m0']);
% load([savefile '_trndump']);
load([savefile '_devdump']);
load([savefile '_tstdump']);
end % if 0

fprintf('%s dynamic oracle training.\n', datestr(now));
load([savefile '_m2']);
[e2, f2] = eval_model(m2, dev, devdump, 1);

for epoch=2:100
  m2save = m2;
  m2 = vectorparser(m2, trn);
  save([savefile '_m2'], 'm2', '-v7.3');
  [e, f] = eval_model(m2, dev, devdump, epoch);
  if f.head_pct < f2.head_pct           % this can go bad first iteration
    f2 = f;
  else
    break;
  end
end

m2 = m2save;
save([savefile '_m2'], 'm2', '-v7.3');

fprintf('%s final testing.\n', datestr(now));
eval_model(m2, tst, tstdump, 0);

end % main

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [e, f] = eval_model(m, dev, devdump, epoch)
[~,s] = perceptron(devdump.x, devdump.y, m, 'update', 0);
[~,z] = max(s);
e = numel(find(z ~= devdump.y))/numel(z);
[~,mdev] = vectorparser(m, dev, 'update', 0);
f = eval_conll(dev, mdev)
fprintf('%d\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\n', epoch, e, ...
        f.move_pct, f.head_pct, f.word_pct, f.sent_pct);
end % eval_model


==> run/run_sort_bestfeats.m <==
function fsorted = run_sort_bestfeats(cache)

nfeats = 199;
bestfeats = cell(1,nfeats);
besterror = inf(1,nfeats);
cachekeys = keys(cache);
for i=1:numel(cachekeys)
  fstr = cachekeys{i};
  ferr = cache(fstr);
  flen = size(eval(fstr), 1);
  if ferr < besterror(flen)
    besterror(flen) = ferr;
    bestfeats{flen} = fstr;
  end
end

[emin, imin] = min(besterror)
fmin = eval(bestfeats{imin});

for i=1:imin
  fsorted{i,1} = 0;
  fsorted{i,2} = fsymbol(fmin(i,:));
  fnew = fmin;
  fnew(i,:) = [];
  fk = mat2str(sortrows(fnew));
  if isKey(cache, fk)
    ferr = cache(fk);
    fsorted{i,1} = round(10000 * (emin - ferr));
  end
end

fsorted = sortrows(fsorted, 1);

end


==> run_fs_conll07_arceager13.m <==
load mat/conll07a_trn.mat
load mat/conll07a_dev.mat
whos trn dev

newFeatureVectors;
load mat/conll07a_m0.mat
m0.feats = fv136;
m0.parser = @arceager13;

[~,devdump] = vectorparser(m0, dev, 'update', 0, 'predict', 0)
dev_finite_count = sum(isfinite(devdump.cost));
devdump.x = devdump.x(:,dev_finite_count > 1);
devdump.y = devdump.y(:,dev_finite_count > 1);
devdump.cost = devdump.cost(:,dev_finite_count > 1)
save('run_fs_conll07_arceager13_devdump', 'devdump', '-v7.3');

[~,trndump] = vectorparser(m0, trn, 'update', 0, 'predict', 0)
trn_finite_count = sum(isfinite(trndump.cost));
trndump.x = trndump.x(:,trn_finite_count > 1);
trndump.y = trndump.y(:,trn_finite_count > 1);
trndump.cost = trndump.cost(:,trn_finite_count > 1)
save('run_fs_conll07_arceager13_trndump', 'trndump', '-v7.3');

featselect_gpu(m0, trndump, devdump, 'fs_conll07_arceager13.mat', fv008);


==> run_fs_conll07_aug7.m <==
load mat/conll07a_trn.mat
load mat/conll07a_dev.mat
whos trn dev

newFeatureVectors;
load mat/conll07a_m0.mat
m0.feats = fv136;

[~,devdump] = vectorparser(m0, dev, 'update', 0, 'predict', 0)
dev_finite_count = sum(isfinite(devdump.cost));
devdump.x = devdump.x(:,dev_finite_count > 1);
devdump.y = devdump.y(:,dev_finite_count > 1);
devdump.cost = devdump.cost(:,dev_finite_count > 1)
save('run_fs_conll07_aug7_devdump', 'devdump', '-v7.3');

[~,trndump] = vectorparser(m0, trn, 'update', 0, 'predict', 0)
trn_finite_count = sum(isfinite(trndump.cost));
trndump.x = trndump.x(:,trn_finite_count > 1);
trndump.y = trndump.y(:,trn_finite_count > 1);
trndump.cost = trndump.cost(:,trn_finite_count > 1)
save('run_fs_conll07_aug7_trndump', 'trndump', '-v7.3');

featselect_gpu(m0, trndump, devdump, 'fs_conll07_aug7.mat', fv008);



2014-08-06  Deniz Yuret  <dyuret@ku.edu.tr>

	* mxrep: replicate mx experiments with new code. $$b40.  Static oracle
	results are better in gmove but .25 worse in parsing performance.
	Difference?  Not using single option moves for training?  Some
	other change from dumpfeatures/trainparser to vectorparser?

	Static oracle:
	epoch	gmove	move	head	word	sent
	1	0.0363	0.0431	0.1129	0.1002	0.6624
	2	0.0334	0.0399	0.1048	0.0929	0.6329
	3	0.0322	0.0389	0.1021	0.0898	0.6241
	(4)	0.0316	0.0385	0.1009	0.0887	0.6165
	5	0.0318	0.0386	0.1012	0.0887	0.6176

	Compare to the original mx Static Oracle:
	model	aer	time	nsv	gtrans	ptrans	phead	whead	notes
	mx1	6.09	314	104757	3.75	4.26	11.18		fv808 feature-set batch=1000
	mx2	4.89	727	133856	3.50	3.95	10.36		all mx are fv808 batch=1000
	mx3	4.11	860	150207	3.38	3.80	 9.96
	mx4	3.54	930	161208	3.36	3.80	 9.88
	mx5	3.21	963	169135	3.34	3.76	 9.83	8.63	best epoch for fv808: gtrans:3.34 phead:9.83 whead:7.60

	- try removing single choice moves, results should be identical
	-- instances go from 1820392 to 1720986
	-- first epoch ends with 103524 sv (103062 unique).
	-- dev error is 5.3180?
	-- eliminate single choice moves from dev as well. 76834 -> 72555.
	-- recovered gtrans=3.7392.  (did we do this on cost based perceptron? doesn't matter it screwed up on parsing)
	-- vectorparser on dev: move:4.26 head:11.20 word:9.84 sent:65.88.

	- this works because vectorparser is not asking the perceptron
	anything for single choice moves.  so perceptron would be wasting
	its capacity on useless instances.  for multichoice moves it
	shouldn't matter what score is assigned to inf cost moves, since
	vectorparser is not going to ask about them either?

	-- ok try to replicate the above result with cost based.
	perceptron ignoring the single-choice moves itself.  Gives us
	103286 sv.  3.77 gtrans.  It won't be exactly the same because
	minibatch borders will shift.  We can use the 1720986 trn to get
	exactly identical result.  It does.  Now add ignoring scores for
	inf cost moves.  102510 sv.  6.86 gtrans.  Of course this is not
	eliminating the inf moves like in training.  Parser does, so
	vectorparser comparison more meaningful.  vectorparser on dev:
	move:4.29 (+0.03) head:11.36 (+0.16) word:9.90 (+0.06) sent:66.41
	(+0.53).  OK it gets worse, don't ask again.

	Conclusion: it gains a bit to ignore single choice moves.  it does
	not gain to ignore the score given to inf cost moves in the
	multi-choice case.


	* conll07b: rerun conll07 experiments with fv808 and new code.  $$b41.
	>> run_conll07_dbg('conll07b');

	Static oracle:
	epoch	gmove	move	head	word	sent
	1	0.0492	0.0591	0.1523	0.1374	0.7411
	2	0.0448	0.0551	0.1424	0.1285	0.7134
	3	0.0438	0.0538	0.1386	0.1251	0.7054
	4	0.0431	0.0536	0.1380	0.1246	0.7054
	5	0.0430	0.0531	0.1373	0.1241	0.7044
	6	0.0430	0.0532	0.1374	0.1245	0.7059
	7	0.0426	0.0529	0.1367	0.1238	0.7039
	(8)	0.0425	0.0526	0.1362	0.1235	0.7049
	9	0.0426	0.0526	0.1362	0.1235	0.7029


	* rerun: featselect for conll07, archybrid, fv130, new train/dev
	split.  ($$b42) Note that fv130 does not include the new head
	feature but that is not useful for archybrid.

	* DONE:
	- copy run scripts to ChangeLog.
	+ move run_* to logs. split log/run/mat.
	+ stop featselect.
	+ test 10-epoch best model for parsing
	+ start dynamic oracle training
	+ figure out conll07 diff,
	+ rerun featselect.
	+ rinse and repeat.

	* DONE:
	- severe connections with dogma:
	- remove model.n_cla and other useless fields from vectorparser.
	- edit perceptron to do testing as well.

	* DONE: add a head feature for arceager and redo feature selection.

	* DONE: correct perceptron update rule when there can be more than
	one maxscore and more than one mincost answer.

	* DONE: perceptron.m:
	+ experiment with cost vs answer training.
	+ skipping single choice answers?
	? arceager vs archybrid parse error - move error relation?
	+ write testing function for perceptron instead of model_predict_gpu.
	+ run_arctype_comparison should dump first instead of training.

	* DONE:
	x sparsification: isn't necessary, compactify is enough.
	+ multi-epoch: ~10-12 static epochs seem sufficient, 5 epochs for fv808.
	+ features: fv808 converges faster but same static performance.
	+ dynamic oracle: it starts working after epoch 2, best at 5-10 epochs
	+ embeddings: bansal seems best.
	+ arctype
	? beam-search
	? kernel-type

	* DONE: test if saving single-choice moves in dump makes a big
	difference in static oracle training, if it does find a way to
	filter the multi-choice instances.

	* DONE:
	+ Let $$i00 finish so we can use the dumps.
	- Verify the dumps.
	- Run regular perceptron training.
	- Figure out the difference between PTB and conll.
	- Figure out if we need separate featselect for conll.

	* DONE: in fact, why don't we use the scaled averaged beta during
	training?  If we know it is better it will make less mistakes,
	accumulate fewer support vectors.  (Try this in perceptron.m after
	debugging). -- did not work.


==> run/run_conll07_debug.m <==
% % function run_conll07(parser, feats, trn, dev, tst, savefile)

%% savefile = 'conll07a';

% fprintf('%s saving arguments\n', datestr(now));
% save([savefile '_parser'], 'parser', '-v7.3');
% save([savefile '_feats'], 'feats', '-v7.3');
% save([savefile '_trn'], 'trn', '-v7.3');
% save([savefile '_dev'], 'dev', '-v7.3');
% save([savefile '_tst'], 'tst', '-v7.3');

% fprintf('%s initializing model\n', datestr(now));
% path('dogma',path);
% m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
% m0.parser = parser;
% m0.feats = feats;
% m0.step = 1e5;
% m0.batchsize = 1e3;
% tmp = feval(parser, 1);
% m0.n_cla = tmp.NMOVE;
% clear tmp;
% save([savefile '_m0'], 'm0', '-v7.3');

%% load([savefile '_m0']);

% fprintf('%s preparing dumps.\n', datestr(now));
% [~,trndump] = vectorparser(m0, trn, 'predict', 0, 'update', 0);
% save([savefile '_trndump'], 'trndump', '-v7.3');
% [~,devdump] = vectorparser(m0, dev, 'predict', 0, 'update', 0);
% save([savefile '_devdump'], 'devdump', '-v7.3');
% [~,tstdump] = vectorparser(m0, tst, 'predict', 0, 'update', 0);
% save([savefile '_tstdump'], 'tstdump', '-v7.3');

%% load([savefile '_trndump']);
%% load([savefile '_devdump']);
%% load([savefile '_dev']);

% m1 = k_perceptron_multi_train_gpu(trndump.x, trndump.y, m0);
m2 = perceptron(trndump.x, trndump.y, m0);
% m3 = perceptron(trndump.x, trndump.cost, m0);
% m4 = perceptron_dbg(trndump.x, trndump.y, m0);
% m5 = perceptron_dbg(trndump.x, trndump.cost, m0);
% m6 = perceptron2_dbg(trndump.x, trndump.y, m0);
% m7 = perceptron2_dbg(trndump.x, trndump.cost, m0);

% save('run_conll07_debug_models.mat', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', '-v7.3');

% [g1,c1,e1] = run_conll07_debug_eval(m1, devdump, dev);
[g2,c2,e2] = run_conll07_debug_eval(m2, devdump, dev);
% [g3,c3,e3] = run_conll07_debug_eval(m3, devdump, dev);
% [g4,c4,e4] = run_conll07_debug_eval(m4, devdump, dev);
% [g5,c5,e5] = run_conll07_debug_eval(m5, devdump, dev);
% [g6,c6,e6] = run_conll07_debug_eval(m6, devdump, dev);
% [g7,c7,e7] = run_conll07_debug_eval(m7, devdump, dev);

fprintf('gtrans\tctrans\tptrans\tphead\twhead\tsent\tmodel\n');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g1, c1, e1.move_pct, e1.head_pct, e1.word_pct, e1.sent_pct, 'k_perceptron_multi_train');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g2, c2, e2.move_pct, e2.head_pct, e2.word_pct, e2.sent_pct, 'perceptron(y)');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g3, c3, e3.move_pct, e3.head_pct, e3.word_pct, e3.sent_pct, 'perceptron(cost)');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g4, c4, e4.move_pct, e4.head_pct, e4.word_pct, e4.sent_pct, 'perceptron_dbg(y)');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g5, c5, e5.move_pct, e5.head_pct, e5.word_pct, e5.sent_pct, 'perceptron_dbg(cost)');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g6, c6, e6.move_pct, e6.head_pct, e6.word_pct, e6.sent_pct, 'perceptron2_dbg(y)');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g7, c7, e7.move_pct, e7.head_pct, e7.word_pct, e7.sent_pct, 'perceptron2_dbg(cost)');

% fprintf('%s static oracle training.\n', datestr(now));
% m1 = m0;
% e1 = inf;
% for epoch=1:100
%   m = perceptron(trndump.x, trndump.cost, m1);
%   p = model_predict_gpu(devdump.x, m, 1);
%   c = devdump.cost(sub2ind(size(devdump.cost),p,1:numel(p)));
%   [cmin,imin] = min(devdump.cost);
%   e = numel(find(c~=cmin))/numel(c);
%   fprintf('%s Epoch %d dev gtrans error: %g\n', datestr(now), epoch, e);
%   if e < e1
%     e1 = e;
%     m1 = m;
%     save([savefile '_m1'], 'm1', '-v7.3');
%     save([savefile '_p'], 'p', '-v7.3');
%   else
%     break;
%   end
% end

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% fprintf('%s Evaluating m1\n', datestr(now));
% [~,m1dev] = vectorparser(m1, dev, 'update', 0);
% save([savefile '_m1dev'], 'm1dev', '-v7.3');
% e1dev = eval_conll(dev, m1dev)
% save([savefile '_e1dev'], 'e1dev', '-v7.3');

% fprintf('%s dynamic oracle training.\n', datestr(now));
% m2 = m1;
% e2 = e1dev.head_pct;
% for epoch=1:100
%   m = vectorparser(m2, trn);
%   [~,mdev] = vectorparser(m, dev, 'update', 0);
%   edev = eval_conll(dev, mdev)
%   e = edev.head_pct;
%   fprintf('%s Epoch %d dev phead error: %g\n', datestr(now), epoch, e);
%   if e < e2
%     e2 = e;
%     m2 = m;
%     save([savefile '_m2'], 'm2', '-v7.3');
%     save([savefile '_mdev'], 'mdev', '-v7.3');
%     save([savefile '_edev'], 'edev', '-v7.3');
%   else
%     break;
%   end
% end

% fprintf('%s final testing.\n', datestr(now));
% [~,m2tst] = vectorparser(m2, tst, 'update', 0);
% save([savefile '_m2tst'], 'm2tst', '-v7.3');
% e2tst = eval_conll(tst, m2tst)
% save([savefile '_e2tst'], 'e2tst', '-v7.3');


==> run/run_conll07.m <==
function run_conll07(savefile, opts)

parser = getval('parser', @()(opts.parser));
feats = getval('feats', @()(opts.feats));
trn = getval('trn', @()(opts.trn));
dev = getval('dev', @()(opts.trn));
tst = getval('tst', @()(opts.trn));

trndump = getval('trndump', @()(getdump(m0,trn)));
devdump = getval('devdump', @()(getdump(m0,dev)));
tstdump = getval('tstdump', @()(getdump(m0,tst)));

m0 = getval('m0', @()(struct('kerparam', struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3),...
                             'parser', parser, 'feats', feats, 'step', 1e5, 'batchsize', 1e3)));

m3 = getval('m3', @()[]);
if isempty(m3)

  m2 = getval('m2', @()[]);
  if isempty(m2)

    m1 = getval('m1', @()[]);
    if isempty(m1)
      m1 = m0;
      e1 = inf;
      epoch1 = 1;
    else
      e1 = getval('e1', @()(error('No e1')));
      epoch1 = getval('epoch1', @()(error('No epoch1')));
    end

    fprintf('%s static oracle training.\n', datestr(now));
    fprintf('epoch\tgmove\tmove\thead\tword\tsent\n');

    while epoch1 < 100
      m = perceptron(trndump.x, trndump.y, m1);
      [e, f] = eval_model(m, dev, devdump);
      if e < e1
        e1 = e;
        f1 = f;
        m1 = m;
        save([savefile '_m1'], 'm1', '-v7.3');
        save([savefile '_e1'], 'e1', '-v7.3');
        save([savefile '_f1'], 'f1', '-v7.3');
        save([savefile '_epoch1'], 'epoch1', '-v7.3');
      else
        break;
      end
    end % while epoch1 < 100
    m2 = m1;
    f2 = f1;
    epoch2 = 1;
  end % if isempty(m2)

  f2 = getval('f2', @()(error('No f2')));
  epoch2 = getval('epoch2', @()(error('No epoch2')));

  fprintf('%s dynamic oracle training.\n', datestr(now));
  while epoch2 < 100
    m = vectorparser(m2, trn);
    [e, f] = eval_model(m, dev, devdump);
    if f.head_pct < f2.head_pct
      f2 = f;
      m2 = m;
      save([savefile '_m2'], 'm2', '-v7.3');
      save([savefile '_f2'], 'f2', '-v7.3');
      save([savefile '_epoch2'], 'epoch2', '-v7.3');
    else
      break;
    end
  end % while epoch2 < 100

  m3 = m2;
  f3 = f2;
end % if isempty(m3)

fprintf('%s final testing.\n', datestr(now));
[e3, f3] = eval_model(m3, tst, tstdump);


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [e, f] = eval_model(m, dev, devdump)
[~,s] = perceptron(devdump.x, devdump.y, m, 'update', 0);
[~,z] = max(s);
e = numel(find(z ~= devdump.y))/numel(z);
[~,mdev] = vectorparser(m, dev, 'update', 0);
f = eval_conll(dev, mdev)
fprintf('%d\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\n', epoch, e, ...
        f.move_pct, f.head_pct, f.word_pct, f.sent_pct);
end % eval_model

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function v = getval(varname, lambda)
fprintf('Getting %s\n', varname);
filename = [savefile '_' varname '.mat'];
if (exist(varname))
  v = eval(varname);
elseif (exist(filename, 'file'))
  load(filename);
  v = eval(varname);
else
  v = feval(lambda);
end
if ~exist(filename, 'file')
  save filename varname
end

end % getval

end % main


==> run_fs_conll07_aug6.m <==
load mat/conll07a_trn.mat
load mat/conll07a_dev.mat
whos trn dev

newFeatureVectors;
load mat/conll07a_m0.mat
m0.feats = fv136;


2014-08-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* run/run_conll07.m: 5 epochs of static oracle training best,
	nsv=71496, gtrans=.0352.  However bad ptrans error.  Let's compare
	with our old mx runs:

	mx run:
	corpus:ptb3
	encoding:conllToken_wikipedia2MUNK-50
	feats:fv808
	parser:ArcHybrid
	static-learner:k_perceptron_multi_train_gpu (run_multi_epoch2.m)
	dynamic-learner:trainparser_gpu (run_dynamic_oracle2.m)

	Static Oracle:
	model	aer	time	nsv	gtrans	ptrans	phead	whead	notes
	mx1	6.09	314	104757	3.75	4.26	11.18		fv808 feature-set batch=1000
	mx2	4.89	727	133856	3.50	3.95	10.36		all mx are fv808 batch=1000
	mx3	4.11	860	150207	3.38	3.80	 9.96
	mx4	3.54	930	161208	3.36	3.80	 9.88
	mx5	3.21	963	169135	3.34	3.76	 9.83	8.63	best epoch for fv808: gtrans:3.34 phead:9.83 whead:7.60

	Dynamic Oracle:
	$$i11			TEST				TRAIN
	model	nsv	time	gtrans	ptrans	phead	whead	ptrans	phead	whead
	mx5	169135	963	3.34	3.76	9.83	8.63	-	-
	mx5d1	204369	22472	4.07	3.78	10.02	8.83	3.02	8.37	7.42
	mx5d2	232151	?	4.02	3.63	9.59	8.47	2.73	7.60	6.77
	mx5d3	254535	?	3.99	3.49	9.23	8.16	2.45	6.88	6.13
	mx5d4	273226	?	4.00	3.47	9.20	8.12	2.23	6.28	5.61
	mx5d5	289077	?	3.99	3.39	9.02	7.98	2.02	5.73	5.11
	mx5d6	302335	?	3.99	3.39	8.96	7.92	1.83	5.24	4.67
	mx5d7	314292	?	4.00	3.37	8.90	7.88	1.71	4.92	4.40

	conll07 run:
	corpus:conll07 (trn nx:763702 nd:1384)
	encoding:bansal100
	feats:best of mat/bansal5k_archybrid_fv130_cache.mat
	parser:archybrid
	static-learner:perceptron (run_conll07.m)
	dynamic-learner:vectorparser (run_conll07.m)
	evaluation:gtrans measured using cost, not correct answer!

	Note: in static oracle gtrans not equivalent to prev exp, it is
	cost based (let's call this ctrans), prev was answer based
	(gtrans).

	Note: in dynamic oracle ptrans=move_pct, phead=head_pct,
	whead=word_pct, and the new sent_pct measures exact sentence match
	error.

	Static Oracle: (run_conll07_experiment1.out)
	epoch	ctrans
	1	4.02
	2	3.72
	3	3.57
	4	3.53
	5	3.52 (nsv:71496 gtrans:5.38 ptrans:7.20 phead:18.30)

	Dynamic Oracle: (run_conll07_experiment2.out $$i01)
	epoch	move	head	word	sent
	0	7.20	18.30	16.04	76.97
	1	5.14	13.39	12.31	70.69
	2	4.99	13.01	11.92	70.54
	3	4.92	12.76	11.75	70.04
	4	4.89	12.68	11.67	69.88
	5	4.85	12.55	11.57	69.33
	6	4.82	12.50	11.51	69.48
	7	4.79	12.43	11.45	69.08
	8	4.75	12.36	11.37	68.78

	Summary: Static oracle is not easy to compare.  In mx, correct
	answer based gtrans went from 3.75 to 3.34 in 5 epochs.  In
	conll07 cost based gtrans went from 4.02 to 3.52 (which
	corresponds to 5.38 answer based).  So we are doing 2% worse in
	static oracle.  Theories:
	1. smaller training corpus
	1a. different dev set (02 may be more difficult than 22)
	2. bansal vs scode encoding
	3. fv808 vs new features
	4. bug-fix in archybrid?
	5. perceptron vs k_perceptron_multi_train_gpu.

	In dynamic oracle, mx starts with 3.34 gtrans translating to a
	close 3.76 ptrans, which gives the x2.6 9.83 phead.  whead is
	about 1% better.  conll07 starts with 5.38 gtrans translating to a
	2-point worse 7.20 ptrans, which gives the x2.6 18.30 phead.  At
	epoch 4 mx improves ptrans 0.3 to 3.47 and phead x2.6 is 9.20.  At
	epoch 4 conll07 improves ptrans 2.31 to 4.89 and phead x2.6 gives
	12.68.

	Static oracle is leaving a model 2.04% gtrans worse (which could
	be explained by data size etc.) but the gtrans/ptrans difference
	at that point is huge (1.82% vs 0.42% for mx)!  That is partly
	compensated by dynamic oracle improvement (2.31% vs 0.29% for mx).

		gtrans0	gtrans1	ptrans1	ptrans4	phead4
	mx	3.75	3.34	3.76	3.47	9.20
	conll07	?	5.38	7.20	4.89	12.68

	Theories:
	1. find out the answer based gtrans0 for conll07.
	2. problem definitely in static oracle training.
	3. 1-epoch perceptron vs k-perceptron, ctrans vs gtrans.
	4. Look into perceptron update rule for cost/score ties.


	** 1-epoch-experiment: Compare the effect of different update
	rules.

	1. k_perceptron:
	Yi = gpuArray(int32(Y(i:j)) + model.n_cla*int32(0:ij-1)); % 1018us
	tmp=val_f;                            % 922us
	tmp(Yi)=-inf;                         % 1200us
	[mx_val,idx_mx_val]=max(gather(tmp));         % 983us
	tr_val = val_f(Yi);                   % 996us
	updates = find(tr_val <= mx_val);     % 1219us

	2. perceptron(Y):
	First converts Y into cost matrix and then applies the rule below:

	3. perceptron(cost):
	score = compute_scores();           % score(nc,nk): scores for X(:,i:j)
	costij = Y(:,i:j);                  % costij(nc,nk): costs for X(:,i:j)
	[maxscore, maxscore_i] = max(score); % compare the cost of maxscore answers
	[mincost, mincost_i] = min(costij); % to the mincost answers
	mycost = costij(sub2ind(size(costij), maxscore_i, 1:nk)); % cost of maxscore answers
	updates = find(mycost > mincost);

	4. vectorparser:
	[mincost, bestmove] = min(cost);
	[maxscore, maxmove] = max(score);
	update if cost(maxmove) > mincost

	4.1 But maxmove is arbitrary in case of tie?
	4.2 It is not used for transition if its cost is inf.
	4.3 All ties are broken picking the lowest numbered move.

	First experiment shows that using the cost for training
	(naturally) gives best result in cost, but significantly worse in
	guessing "correct" answer. (run_conll07_debug.m)

$$i02	gtrans	ctrans	ptrans	phead	whead	sent	model
	0.0471	0.0426	0.0605	0.1549	0.1397	0.7285	k_perceptron_multi_train
	0.0476	0.0428	0.0612	0.1576	0.1418	0.7466	perceptron(y)
	0.0581	0.0402	0.0752	0.1899	0.1668	0.7969	perceptron(cost)

	How do we decide "correct" other than arbitrarily during dump?
	[mincost, bestmove] = min(cost);

	So the fact that we do not match arbitrary correct answer should
	not hurt the head accuracy.  But it seems less able to generalize
	to the dynamic parsing situation, i.e. ptrans is a lot worse.

	Trying a different update rule: (perceptron_dbg.m)
	maxscore = max(score, [], 1);       % maxscore answers according to model
	minycost = min(ycost, [], 1);       % mincost answers according to data
	dbeta = bsxfun(@eq, ycost, minycost) - bsxfun(@eq, score, maxscore);
	updates = find(any(dbeta));         % nonzero columns of dbeta

	gtrans	ctrans	ptrans	phead	whead	sent	model
	0.0471	0.0426	0.0605	0.1549	0.1397	0.7285	k_perceptron_multi_train
	0.0476	0.0428	0.0612	0.1576	0.1418	0.7466	perceptron(y)
	0.0581	0.0402	0.0752	0.1899	0.1668	0.7969	perceptron(cost)
	0.0481	0.0433	0.0619	0.1584	0.1424	0.7436	perceptron_dbg(y)
	0.0713	0.0533	0.0887	0.2324	0.2095	0.8421	perceptron_dbg(cost)

	Try to normalize the updates: (perceptron2_dbg.m) This makes sense
	if we assume one pair of mincost,maxscore entries is picked at
	random and we are looking at the expected update.

	score = compute_scores();           % score(nc,nk): scores for X(:,i:j)
	costij = cost(:,i:j);               % costij(nc,nk): costs for X(:,i:j)
	maxscore = max(score);              % maxscore in each column of score
	mincost = min(costij);              % mincost in each column of costij
	incbeta = bsxfun(@eq, costij, mincost); % +1 beta for mincost answers
	decbeta = bsxfun(@eq, score, maxscore); % -1 beta for maxscore answers
	incbeta = bsxfun(@rdivide, incbeta, sum(incbeta));   % normalize in case multiple mincost answers
	decbeta = bsxfun(@rdivide, decbeta, sum(decbeta));   % normalize in case multiple maxscore answers
	dbeta = incbeta - decbeta;          % this should cancel out if y_mincost = y_maxscore
	updates = find(any(dbeta));         % nonzero columns of dbeta are the updates

	gtrans	ctrans	ptrans	phead	whead	sent	model
	0.0471	0.0426	0.0605	0.1549	0.1397	0.7285	k_perceptron_multi_train
	0.0476	0.0428	0.0612	0.1576	0.1418	0.7466	perceptron(y)
	0.0581	0.0402	0.0752	0.1899	0.1668	0.7969	perceptron(cost)
	0.0481	0.0433	0.0619	0.1584	0.1424	0.7436	perceptron_dbg(y)
	0.0713	0.0533	0.0887	0.2324	0.2095	0.8421	perceptron_dbg(cost)
	0.0481	0.0433	0.0619	0.1584	0.1424	0.7436	perceptron2_dbg(y)
	0.0587	0.0408	0.0733	0.1864	0.1642	0.7924	perceptron2_dbg(cost)

	This version gives the same answers in y mode.  Why?  Different
	beta, same SV?  Look at non-compactified version.

	However ptrans and the rest still bad, even worse when using cost
	matrix.

	Theories:
	- Look at this cost matrix and make sure its contents are good.
	- What kinds of instances have multiple correct answers or score ties?
	- Can we just emulate k_perceptron_multi_train?
	- Compare resulting beta with k_perceptron_multi_train?
	- How do cost based algorithms in dogma do it?
	- Does it have to do with move ordering (min picking the lowest)?
	- If we sort them differently does the regular k_perceptron screw up?
	- If it is important do we have the right ordering for other arctypes?
	- Replicate the mx results with perceptron.m.

	CONCLUSION: Even though costs and scores can be tied, in reality
	the oracle or the parser only picks one move!  When the learner
	behaves as if other moves could have been chosen it does not gain
	any advantage.  So we should get back to the original perceptron
	update rule.  The actual moves that are made in vectorparser
	should be compatible with learner:

    if opts.predict
      zscore = score;
      zscore(~valid) = -inf;
      [~,zmove] = max(zscore);
      p.transition(zmove);
    else
      [mincost, bestmove] = min(cost);
      p.transition(bestmove);
    end


==> run/run_conll07_debug_eval.m <==
function [gtrans, ctrans, ev] = run_conll07_debug_eval(model, dump, corpus)

p = model_predict_gpu(dump.x, model, 1);
gtrans = numel(find(p~=dump.y))/numel(p);
c = dump.cost(sub2ind(size(dump.cost),p,1:numel(p)));
[cmin,imin] = min(dump.cost);
ctrans = numel(find(c~=cmin))/numel(c);

[~,d] = vectorparser(model, corpus, 'update', 0);
ev = eval_conll(corpus,d);

end


2014-08-04  Deniz Yuret  <dyuret@ku.edu.tr>

	* fsymbol.m: get feature name from feature vector f(1,3).

	* conll07bansal.mat: we need conll07 dev for featselect and epoch
	decisions.  let us use sec02 for dev: top 50123 lines, 1989
	sentences, 48134 tokens.  We'll save it as split.  Just concat dev
	and trn to get original data.

	* run_arctype_comparison2.m: $$i00: Determine which arctype is
	best.  The scores *c are cost based, *a are answer based.  trn and
	tst are from conll07, dev is from conllWSJToken.  All after one
	epoch cost-based training with perceptron.m with features
	optimized on conllWSJToken trn(1:5000)/dev.

	NOTE: these used a buggy version of perceptron (cost based)
	update, a bad dev set and possibly bad features (because they were
	optimized on the bad dev set albeit with the correct
	k_perceptron_multi_train).  Need to rerun.

	1-epoch results: (run_arctype_comparison2.m)
	arc	trnc	trna	devc	deva	tstc	tsta	feats
	eager	2.76	4.69	7.78	9.14	4.23	6.41	17
	hybrid	2.75	4.46	7.30	8.55	4.23	6.30	27
	eag13	2.76	4.87	7.62	9.22	4.08	6.43	23
	hyb13	2.58	4.31	7.43	8.66	4.24	6.25	27

	10-epoch results: (run_arctype_comparison3.m)
	arc	trnc	trna	devc	deva	tstc	tsta	feats
	eager	0.35	2.32	7.70	9.13	3.85	6.06	17
	hybrid	0.39	2.18	6.99	8.32	3.65	5.76	27
	eag13	0.36	2.43	7.52	9.12	3.71	5.96	23
	hyb13	0.30	2.08	7.02	8.33	3.78	5.89	27

==> run/run_arctype_comparison2.m <==
% from previous script:
% save -v7.3 conll07bansal trnconll devconll tstconll
% savefile = ['conll07bansal_' arc];
% save(savefile, 'model', 'trndump', 'devdump', 'tstdump', 'deveval', 'tsteval', '-v7.3');

path('dogma',path);

fprintf('%s initializing model\n', datestr(now));
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
m0.step = 1e5;
m0.batchsize = 1e3;
% m0.parser = parser;
% m0.feats = feats;

fprintf('%s loading conll07bansal\n', datestr(now));
load conll07bansal;

for arctype={'arceager', 'archybrid', 'arceager13', 'archybrid13'}
  arc = arctype{1};
  m0.parser = eval(['@' arc]);
  savefile = ['conll07bansal_' arc];
  fprintf('%s loading %s\n', datestr(now), savefile);
  load(savefile);
  m0.feats = trndump.feats;
  fprintf('%s training\n', datestr(now));
  m1 = perceptron(trndump.x, trndump.cost, m0)

  fprintf('%s trn eval\n', datestr(now));
  p1 = model_predict_gpu(trndump.x, m1, 1);
  c1=trndump.cost(sub2ind(size(trndump.cost),p1,1:numel(p1)));
  [min1,imin1] = min(trndump.cost);
  trn_cost_eval = numel(find(c1~=min1))/numel(c1)
  trn_answer_eval = numel(find(p1~=trndump.y))/numel(p1)

  fprintf('%s dev eval\n', datestr(now));
  [~,devdump] = vectorparser(m0, devconll, 'predict', 0, 'update', 0);
  p2 = model_predict_gpu(devdump.x, m1, 1);
  c2=devdump.cost(sub2ind(size(devdump.cost),p2,1:numel(p2)));
  [min2,imin2] = min(devdump.cost);
  dev_cost_eval = numel(find(c2~=min2))/numel(c2)
  dev_answer_eval = numel(find(p2~=devdump.y))/numel(p2)

  fprintf('%s tst eval\n', datestr(now));
  [~,tstdump] = vectorparser(m0, tstconll, 'predict', 0, 'update', 0);
  p3 = model_predict_gpu(tstdump.x, m1, 1);
  c3=tstdump.cost(sub2ind(size(tstdump.cost),p3,1:numel(p3)));
  [min3,imin3] = min(tstdump.cost);
  tst_cost_eval = numel(find(c3~=min3))/numel(c3)
  tst_answer_eval = numel(find(p3~=tstdump.y))/numel(p3)

end

==> run/run_arctype_comparison3.m <==
% from previous script:
% save -v7.3 conll07bansal trnconll devconll tstconll
% savefile = ['conll07bansal_' arc];
% save(savefile, 'model', 'trndump', 'devdump', 'tstdump', 'deveval', 'tsteval', '-v7.3');

path('dogma',path);

fprintf('%s initializing model\n', datestr(now));
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
m0.step = 1e5;
m0.batchsize = 1e3;
% m0.parser = parser;
% m0.feats = feats;

fprintf('%s loading conll07bansal\n', datestr(now));
load conll07bansal;

for arctype={'arceager', 'archybrid', 'arceager13', 'archybrid13'}
  arc = arctype{1};
  m0.parser = eval(['@' arc]);
  savefile = ['conll07bansal_' arc];
  fprintf('%s loading %s\n', datestr(now), savefile);
  load(savefile);
  m0.feats = trndump.feats;

  fprintf('%s training\n', datestr(now));
  m1 = m0;
  for i=1:10
    m1 = perceptron(trndump.x, trndump.cost, m1);
  end
  m1_10_epoch = m1

  fprintf('%s trn eval\n', datestr(now));
  p1 = model_predict_gpu(trndump.x, m1, 1);
  c1=trndump.cost(sub2ind(size(trndump.cost),p1,1:numel(p1)));
  [min1,imin1] = min(trndump.cost);
  trn_cost_eval = numel(find(c1~=min1))/numel(c1)
  trn_answer_eval = numel(find(p1~=trndump.y))/numel(p1)

  fprintf('%s dev eval\n', datestr(now));
  [~,devdump] = vectorparser(m0, devconll, 'predict', 0, 'update', 0);
  p2 = model_predict_gpu(devdump.x, m1, 1);
  c2=devdump.cost(sub2ind(size(devdump.cost),p2,1:numel(p2)));
  [min2,imin2] = min(devdump.cost);
  dev_cost_eval = numel(find(c2~=min2))/numel(c2)
  dev_answer_eval = numel(find(p2~=devdump.y))/numel(p2)

  fprintf('%s tst eval\n', datestr(now));
  [~,tstdump] = vectorparser(m0, tstconll, 'predict', 0, 'update', 0);
  p3 = model_predict_gpu(tstdump.x, m1, 1);
  c3=tstdump.cost(sub2ind(size(tstdump.cost),p3,1:numel(p3)));
  [min3,imin3] = min(tstdump.cost);
  tst_cost_eval = numel(find(c3~=min3))/numel(c3)
  tst_answer_eval = numel(find(p3~=tstdump.y))/numel(p3)

end

==> run/run_conll07_experiment1.m <==
path('dogma',path);
path('run',path);
savefile = 'run_conll07_experiment01.mat';
if 0
load('mat/conll07bansal.mat'); % trn, dev, tst
load('mat/bansal5k_archybrid_fv130_cache.mat'); % cache
[emin, imin] = min(besterror);
fprintf('%s Features(%d:%g) %s\n', datestr(now), imin, emin, bestfeats{imin});
feats = eval(bestfeats{imin});
parser = @archybrid;

run_conll07(parser, feats, trn, dev, tst, savefile);

else
run_conll07([], [], [], [], [], savefile);

end


2014-08-03  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_arctype_comparison.m: $$i00 running all arctypes with best
	features on conll07.  The first results (arceager) look really
	bad, trouble with vectorparser?  Different from using in dump
	mode?  Trouble with feature set?

	Debugging:
	load conll07bansal_arceager
	trndump.x [1022x855992]
	trnconll 18577	446573
	2*446573-2*18577 = 855992 ok
	? 17 feats =? 1022 dims
	m1=perceptron(trndump.x, trndump.y, m0);
	855992	65049	992	108.25	4.13e+08
	Finding unique SV in 65049...
	Saving 64828 unique SV.
	p1=model_predict_gpu(devdump.x, m1, 1);
	numel(find(p1 ~= devdump.y))/numel(devdump.y)
	0.2267 wtf? this should be 0.0542!

	ok perceptron instead of k_perceptron
	k_perceptron gives 0.2271.
	5k instead of full data
	size of 5k data: 3538x230120
	size of full data: 1022x855992

	both x and y are different even though the source data the same.
	in featselect trn comes from:
	vectorparser(m0, trn, 'update', 0, 'predict', 0);
	in run_arctype_comparison:
	[model,trndump] = vectorparser(m0, trnconll, 'predict', 0);
	trying to replicate with small data tstconll.
	no difference!

	compare devs in bansal5k_arceager_fv130 which comes from
	conllWSJToken_bansal100 with conll07bansal_arceager which comes
	from conll07bansal.

	a1: Aug  1 17:52 conllWSJToken_bansal100.mat (?)
	a2: Aug  2 13:33 bansal5k_arceager_fv130.mat (run_dump_bansal5k, run_featselect5k)
	b1: Aug  3 13:21 conll07bansal.mat (run_arctype_comparison)
	b2: Aug  3 19:46 conll07bansal_arceager.mat (run_arctype_comparison)

	conllWSJToken_bansal100 and conll07bansal dev.head and dev.wvec
	identical.  bansal5k_arceager_fv130 has 130 feats
	conll07bansal_arceager has 17.  Their y's do not match!  OK
	devdump.y will not match because conll07bansal_arceager is dumping
	it during testing thus following model moves?  We need that to get
	real parsing accuracy.  Like gtrans vs ptrans.  During testing we
	are measuring ptrans.  But trndumps should have identical y!  They
	do not.

	Heads are different in conll07!  OK, the training data is
	different.  run_featselect5k used conllWSJToken_bansal100 for trn
	(1:5000, 230120 instances) and dev.  run_arctype uses conll07 trn
	(855992 instances).

	Looking at training set accuracy:
	p=model_predict_gpu(b2.trndump.x, b2.model, 1);
	numel(find(p~=b2.trndump.y))/numel(p) => 0.1647

	b2.model has 50646 SV out of 855992 instances.  should be 5.92?
	Retry training with k_perceptron instead of vectorparser:
	b2: [model,trndump] = vectorparser(m0, trnconll, 'predict', 0);
	model2 = k_perceptron_multi_train_gpu(b2.trndump.x, b2.trndump.y, a2.m0);
	nd=1022 nx=855992 nc=4 ns=0
	#855992 g:4.12515e+08 nk:1000 SV: 7.57(64823)	AER: 0.00	t=112.736
	p2=model_predict_gpu(b2.trndump.x, model2, 1);
	numel(find(p2~=b2.trndump.y))/numel(p2)  % 0.0328

	ok, b2 model is totally fdup.  problem is with vectorparser training.
	how about dev and test?
	p3=model_predict_gpu(b2.devdump.x, model2, 1);
	numel(find(p3~=b2.devdump.y))/numel(p3)  % 0.2271
	p4=model_predict_gpu(b2.tstdump.x, model2, 1);
	numel(find(p4~=b2.tstdump.y))/numel(p4)  % 0.1723
	but these y's are not the original y's!

	get the plain dumps of tst and dev:
	[~,tstdump2] = vectorparser(b2.model, b1.tstconll, 'predict', 0, 'update', 0)
	p5=model_predict_gpu(tstdump2.x, model2, 1);
	numel(find(p5~=tstdump2.y))/numel(p5)   % 0.0537 OK
	[~,devdump2] = vectorparser(b2.model, b1.devconll, 'predict', 0, 'update', 0)
	p6=model_predict_gpu(devdump2.x, model2, 1);
	numel(find(p6~=devdump2.y))/numel(p6)   % 0.0901

	OK: definitely difference between dev and tst.  we need to find
	some other dev or figure out the difference.

	5.37 is not great, we used to get under 4.00 for gtrans in one
	epoch: but different corpus format and half the training data and
	using arceager with a new feature set.

	3.28 train-gtrans, 5.37 test-gtrans, 9.16 for test-ptrans.

	old model5k used to get 5.43 for test-gtrans and 5.78 for test-ptrans.

	ptrans number may have a problem because of vectorparser bug.

	This gives 16.47 train error:
	[b2.model,b2.trndump] = vectorparser(m0, trnconll, 'predict', 0);

	This gives 3.28 train error with same data:
	model2 = k_perceptron_multi_train_gpu(b2.trndump.x, b2.trndump.y, a2.m0);

	This gives 3.31 train error with same data:
	model3 = perceptron(b2.trndump.x, b2.trndump.y, a2.m0);

	Test if cost based eval is different:

	- answer train / answer eval / cost eval (k_perceptron):
	model2 = k_perceptron_multi_train_gpu(b2.trndump.x, b2.trndump.y, a2.m0);
	p2=model_predict_gpu(b2.trndump.x, model2, 1);
	c2=b2.trndump.cost(sub2ind(size(b2.trndump.cost),p2,1:numel(p2)));
	[min2,imin2] = min(b2.trndump.cost);
	numel(find(c2~=min2))/numel(c2) % 0.0290
	numel(find(p2~=b2.trndump.y))/numel(p2) % 0.0328

	- answer train / answer eval / cost eval (perceptron):
	model3 = perceptron(b2.trndump.x, b2.trndump.y, a2.m0);
	p3=model_predict_gpu(b2.trndump.x, model3, 1);
	c3=b2.trndump.cost(sub2ind(size(b2.trndump.cost),p3,1:numel(p3)));
	[min3,imin3] = min(b2.trndump.cost);
	numel(find(c3~=min3))/numel(c3) => 0.0292
	numel(find(p3~=b2.trndump.y))/numel(p3) => 0.0331

	% slight difference due to how equalities are handled:

	% perceptron never punishes an equal cost move:
	% update if cost(maxscoremove) > mincost
	[maxscore, maxscore_i] = max(score); % compare the cost of maxscore answers
	[mincost, mincost_i] = min(costij); % to the mincost answers
	mycost = costij(sub2ind(size(costij), maxscore_i, 1:nk)); % cost of maxscore answers
	updates = find(mycost > mincost);
	model.beta2 = model.beta2 + model.beta;

	% k_perceptron initially will, when all val_f = 0:
	% update if score(correctmove) <= score(maxscoreincorrectmove)
	tmp=val_f;                            % 922us
	tmp(Yi)=-inf;                         % 1200us
	[mx_val,idx_mx_val]=max(gather(tmp)); % 983us
	tr_val = val_f(Yi);                   % 996us
	updates = find(tr_val <= mx_val);     % 1219us

	- cost train / answer eval / cost eval (perceptron):
	% gives the best cost eval but worst answer eval (as expected):
	model4 = perceptron(b2.trndump.x, b2.trndump.cost, a2.m0);
	p4=model_predict_gpu(b2.trndump.x, model4, 1);
	c4=b2.trndump.cost(sub2ind(size(b2.trndump.cost),p4,1:numel(p4)));
	[min4,imin4] = min(b2.trndump.cost);
	numel(find(c4~=min4))/numel(c4) % 0.0276
	numel(find(p4~=b2.trndump.y))/numel(p4) % 0.0469


	Could be the same mistake as in the perceptron:
	score(cost==inf) = -inf;

	Try on small data:
	[m1,d1] = vectorparser(m0, b1.tstconll, 'predict', 0); % 0.1886
	m2 = perceptron(d1.x, d1.y, m0); % 0.0534
	% After commenting out the above line...
	% DONE: if predict, we need to not do impossible moves!
	[m3,d3] = vectorparser(m0, b1.tstconll, 'predict', 0); % 0.0678
	% At this point should be the same with perceptron?
	% But vectorparser uses cost based training:
	m4 = perceptron(d1.x, d1.cost, m0);  % 0.0696
	% still not identical?  There is a difference in beta2!
	% fixed beta2 bug in perceptron:
	m5 = perceptron(d1.x, d1.cost, m0);  % 0.0678

	So cost based training is worse than answer based.  Testing on
	dynamic data (ptrans) makes things bad.  And ignoring scores of
	impossible moves is by far the worst!  Why?

	* conll07:

	- goldberg-nivre-2013: gives results for conll07 english using
	greedy parsers.  training is 02-11 (half regular PTB size), look
	at where the test data comes from (it is part of wsj23).

	- results on conll07.  check the source of the test corpus: paper
	claims subset of wsj23.  It seems to be wsj_2300 to part of
	wsj_2308.  Best UAS in conll07 paper: 90.63.  Goldberg-Nivre-2013
	best deterministic parser: 89.41.  Both include punctuation.

		 nsent	nword
	trnconll 18577	446573
	tstconll 214	5003

	Short	UAS+p	Paper
	Car07	90.63	X. Carreras. 2007. Experiments with a high-order projective dependency parser. In Proc. of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
	GN13	89.41	Yoav Goldberg and Joakim Nivre. 2013. Training Deterministic Parsers with Non-Deterministic Oracles.. TACL, vol 1, pp 403--414.

	- results on PTB.  For beam parsers Zhang-Nivre-2011 test on PTB
	and report 92.9 on test (wsj23), 93.14 on dev (wsj22).  They also
	report two transition parsers: ZC08transition (91.4) HS10 (91.4),
	and four graph based parsers: MST (91.5), K08 (92.0), KC10 (93.0,
	92.9) These exclude punctuation: ZC08 says "Like McDonald et
	al. (2005), we evaluate the parsing accuracy by the precision of
	lexical heads (the percentage of input words, excluding
	punctuation, that have been assigned the correct parent) and by
	the percentage of complete matches, in which all words excluding
	punctuation have been assigned the correct parent."

		Secs	Sents 	Words
	Train 	2–21 	39832	950028
	Dev 	22 	1700	40117
	Test 	23 	2416	56684

	Short	UAS-p	Paper
	ZN11	92.9	Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Asso- ciation for Computational Linguistics: Human Language Technologies, pages 188–193.
	ZC08	92.1	Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 562–571.
	ZC08tr	91.4	just transition based features of ZC08.
	MST05	90.9	Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL, pages 91–98, Ann Arbor, Michigan, June.
	MST2	91.5	R McDonald and F Pereira. 2006. Online learning of approximate dependency parsing algorithms. In In Proc. of EACL, pages 81–88, Trento, Italy, April.
	MST1	90.7	First order result according to MP06.

	- conll07-ptb difference:

	from conll-xi/data/english/ptb/doc/README:
	The head and dependency relation fields were converted using the
	algorithms described in
	> Richard Johansson and Pierre Nugues (tentative title)
	> "Extended Constituent-to-Dependency Conversion for English"
	> (Submitted)
	> http://www.lucas.lth.se/lt/pennconverter
	This was run with the arguments: -conjAsHead -prepAsHead

	new website:
	http://nlp.cs.lth.se/software/treebank_converter
	http://fileadmin.cs.lth.se/nlp/software/pennconverter/pennconverter.jar

	The new arg -conll2007 is supposed to generate the conll07
	conventions.  But files are not identical.  It seems the old
	version of pennconverter (and thus the original conll07 data) had
	some conjunction bugs which were later fixed.  220/5003 (4.40%)
	head difference is the closest I can get to conll07.  On the
	training set about 5 sentences were deleted from ptb to conll07.
	Of the 446573 heads the closest I can get is 20298 difference
	(4.55%).  It seems if we are going to work with conll07 data we
	better use part of train as devel instead of trying to regenerate.

==> run/run_arctype_comparison.m <==
if 0
path('dogma',path);

trnfile = 'embedded/conll07EnglishToken_bansal100/00/english_0001.dp';
tstfile = 'embedded/conll07EnglishToken_bansal100/01/english_0101.dp';
devfile = 'embedded/conllWSJToken_bansal100/01/wsj_0101.dp';

fprintf('%s loading %s\n', datestr(now), trnfile);
trnconll = loadCoNLL(trnfile);
fprintf('%s loading %s\n', datestr(now), devfile);
devconll = loadCoNLL(devfile);
fprintf('%s loading %s\n', datestr(now), tstfile);
tstconll = loadCoNLL(tstfile);
fprintf('%s saving %s\n', datestr(now), 'conll07bansal');
save -v7.3 conll07bansal trnconll devconll tstconll
end

fprintf('%s initializing model\n', datestr(now));
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
% m0.parser = parser;
% m0.feats = feats;
m0.step = 1e5;
m0.batchsize = 1e3;

for arctype={'arceager', 'archybrid', 'arceager13', 'archybrid13'}
  arc = arctype{1};
  m0.parser = eval(['@' arc]);
  cachefile = ['bansal5k_' arc '_fv130_cache.mat'];
  fprintf('%s loading %s\n', datestr(now), cachefile);
  load(cachefile);
  [emin, imin] = min(besterror);
  fprintf('%s %g %s\n', datestr(now), emin, bestfeats{imin});
  m0.feats = eval(bestfeats{imin});
  fprintf('%s training\n', datestr(now));
  [model,trndump] = vectorparser(m0, trnconll, 'predict', 0);
  fprintf('%s parsing dev\n', datestr(now));
  [~,devdump] = vectorparser(model, devconll, 'update', 0);
  deveval = eval_conll(devconll, devdump)
  fprintf('%s parsing tst\n', datestr(now));
  [~,tstdump] = vectorparser(model, tstconll, 'update', 0);
  tsteval = eval_conll(tstconll, tstdump)
  savefile = ['conll07bansal_' arc];
  fprintf('%s saving to %s\n', datestr(now), savefile);
  save(savefile, 'model', 'trndump', 'devdump', 'tstdump', 'deveval', 'tsteval', '-v7.3');
end


==> run/run_convert_cache.m <==
function run_convert_cache(trn, cachefile, newfile)

load(cachefile);
oldcache = cache;
nfeats = numel(trn.fidx);
bestfeats = cell(1,nfeats);
besterror = inf(1,nfeats);
cachekeys = keys(oldcache);
cache = containers.Map();
for i=1:numel(cachekeys)
  fstr = cachekeys{i};
  ferr = oldcache(fstr)/100;
  fvec = eval(fstr);
  flen = numel(fvec);
  fk = fkey(fvec);
  cache(fk) = ferr;
  if ferr < besterror(flen)
    besterror(flen) = ferr;
    bestfeats{flen} = fk;
  end
end

save(newfile, 'cache', 'besterror', 'bestfeats');

function fk = fkey(f)
% f is an array of indices into trn.fidx and rows of trn.feats
fk = mat2str(sortrows(trn.feats(f,:)));
end % fkey

end


==> run/run_sort_bestfeats_dbg.m <==
function fsorted = run_sort_bestfeats_dbg(bestfeats, besterror, cache)

[emin, imin] = min(besterror);
imin = imin-1
emin = besterror(imin)
fmin = eval(bestfeats{imin});

for i=1:imin
  fsorted{i,1} = 0;
  fsorted{i,2} = fsymbol(fmin(i,:));
  fnew = fmin;
  fnew(i,:) = [];
  fk = mat2str(sortrows(fnew));
  if isKey(cache, fk)
    ferr = cache(fk);
    fsorted{i,1} = round(10000 * (emin - ferr));
  end
end

fsorted = sortrows(fsorted, 1);

function s = fsymbol(f)
a = {'s2','s1','s0','n0','n1','n2'};
b = {'l2','l1','','r1','r2'};
c = {'ac','d<','l<','l>','c','at','l=','-','t','+','r=','d=','w','r>','r<','d>','aw'};
s = [ a{f(1)+4} b{f(2)+3} c{f(3)+9} ];
end

end


==> run/run_to_cache.pl <==
#!/usr/bin/perl -w
use strict;

my @score;
my @feats;

while(<>) {
    next unless /^\d+\./;
    chop;
    my @a = split /\t/;
    push @score, $a[1];
    push @feats, $a[4];
}

print "cache = containers.Map({\n";
for (my $i = 0; $i <= $#feats; $i++) {
    print "'$feats[$i]'\n";
}
print "}, [\n";
for (my $i = 0; $i <= $#score; $i++) {
    print "$score[$i]\n";
}
print "]);\n";


==> run/run_watchfile.pl <==
#!/usr/bin/perl -w
use strict;
use Data::Dumper;
use File::Copy;
my $fh;
my $last_timestamp=0;

while (1) {
    sleep(1);
    open($fh, '814724.mat');
    my $epoch_timestamp = (stat($fh))[9];
    if ($epoch_timestamp != $last_timestamp) {
	$last_timestamp = $epoch_timestamp;
	copy '814724.mat', "814724-$last_timestamp.mat";
    }
    close($fh);
}


2014-08-02  Deniz Yuret  <dyuret@ku.edu.tr>

	* perceptron.m: The cost version.  Not really comparable to the
	old k_perceptron unfortunately: perceptron updates whenever the
	cost of maxscore is higher than mincost.  k_perceptron updates
	whenever the score of the correct (mincost) answer is <= any of
	the other scores.  In particular, during the first batch all
	scores are 0, so k_perceptron makes all support vectors.
	perceptron only updates when maxscore class (which in this case
	will be 1 by default) is not correct.  However confirmed that it
	has better accuracy and equal speed on at least one dataset.

	Training with Y vs training with cost: reduced nsv from 30553 to
	27042 on bansal5k, the dev error went from 7.42 to 13.34??  We
	should be careful using the cost for training until we understand
	what is going on.  For now perceptron also accepts correct answer
	vector.  Maybe we should not forgive picking inf cost moves.  Yes,
	that is correct, deleting the line:

	score(isinf(costij)) = -inf;        % do not punish for impossible answers

	recovered the normal score.

	* run_featselect_bansal5k.m: running on $$biyofiz.q.  Something
	seems wrong with arceager.
	+ check the featselect moves for arceager: backtracking seems ok.  except the first round multiple repeats.
	+ put actual feature vectors in cache
	+ write a cost based perceptron: do not punish when single move, or when multiple equiv moves.
	- transition error / parse error ratio may be different for arceager.
	- we may get different results with full training set.

	>> load('bansal5k_arceager13_fv130_cache.mat');
	>> run_sort_bestfeats(bestfeats, besterror, cache)
	% displays best feature combination (numfeats:besterror),
	% sorted by the loss of each feature:

	hybrid13(27:5.10)	hybrid(34:5.04) 	eager13(33:5.37)	eager(25:5.30)
	[-284]    'n0w'   	[-274]    'n0w'   	[-338]    'n0w'   	[-271]    'n0w'
	[-268]    's0w'   	[-234]    's0w'   	[-246]    's0w'   	[-237]    's0w'
	[-119]    's1w'   	[-106]    'n1w'   	[-155]    'n1w'   	[-104]    'n1w'
	[-110]    'n1w'   	[ -98]    's1w'   	[-150]    's1r1r<'	[ -40]    'n0c'
	[ -55]    'n0l1w' 	[ -52]    's0r1w' 	[ -45]    's0r1w' 	[ -35]    's0r1w'

	[ -38]    'n0l1c' 	[ -45]    'n0l1w' 	[ -44]    'n0c'   	[ -35]    'n0l1w'
	[ -36]    's0r1w' 	[ -35]    's0r1r<'	[ -39]    's1w'   	[ -31]    's1w'
	[ -32]    's0c'   	[ -32]    's0r1c' 	[ -39]    'n0l1w' 	[ -30]    's1r='
	[ -32]    's0r='  	[ -30]    's0r='  	[ -28]    's0c'   	[ -28]    's0r='
	[ -32]    'n0c'   	[ -30]    's0d>'  	[ -27]    's0l1w' 	[ -27]    's0l>'

	[ -27]    's2c'   	[ -29]    's1l1-' 	[ -27]    's0r1-' 	[ -26]    's0c'
	[ -27]    's1d='  	[ -28]    's2r='  	[ -27]    'n2-'   	[ -24]    'n1c'
	[ -27]    's0r1-' 	[ -28]    's0ac'  	[ -23]    's0r='  	[ -23]    's0r1r>'
	[ -25]    's0l1l='	[ -28]    's0c'   	[ -22]    'n1-'   	[ -22]    's2r1-'
	[ -25]    's0r1r<'	[ -28]    'n0c'   	[ -21]    's2l1r>'	[ -21]    's1-'

	[ -23]    's1r1r<'	[ -27]    'n0l1c' 	[ -21]    's2d>'  	[ -20]    's0-'
	[ -18]    's2r1l<'	[ -23]    's1r1r<'	[ -21]    's0r1+' 	[ -18]    'n0l>'
	[ -18]    's1l1l>'	[ -22]    's2c'   	[ -17]    's0l1r='	[ -16]    's1c'
	[ -18]    'n0l>'  	[ -20]    's2l1r='	[ -16]    's0-'   	[ -15]    's1l>'
	[ -16]    'n1-'   	[ -16]    's1r<'  	[ -15]    's2l='  	[ -15]    's1r1r='

	[ -15]    's1r1r>'	[ -14]    's1d>'  	[ -15]    's0l1l='	[ -13]    'n0l1l>'
	[ -12]    's1r1l>'	[ -14]    's0r1l<'	[ -15]    's0l1-' 	[ -11]    's1r1l>'
	[ -12]    's0r1l>'	[ -14]    'n1-'   	[ -14]    'n0l1l>'	[  -9]    's1r1-'
	[ -11]    's2l1l>'	[ -13]    's0l1w' 	[ -13]    's2r>'  	[  -8]    'n0-'
	[  -9]    's2r1r='	[ -11]    's2r1r='	[ -13]    's1r1r>'	[   0]    's2r1l='

	[  -8]    's0l1-' 	[ -11]    's0-'   	[ -12]    's2r1-'
	[  -5]    'n0l1-' 	[ -10]    's2r1l>'	[ -12]    's0r1r>'
				[ -10]    's2r1r>'	[ -11]    's2-'
				[  -6]    'n0l1l>'	[ -10]    'n0l1r<'
				[  -5]    's2d>'  	[  -8]    's2r1l>'
				[  -4]    'n0-'   	[  -7]    's2l1r='
				[  -3]    's0l1l='	[  -7]    's0l1l>'
				[  -3]    's0l1r>'	[  -6]    'n0-'
				[  -3]    'n0l1r>'



==> run/run_dump_bansal5k.m <==
function run_dump_bansal5k(parser, featname, dumpfile)

fprintf('%s Initializing features %s\n', datestr(now), featname);
newFeatureVectors;
feats = eval(featname);
size_feats = size(feats)

fprintf('%s Initializing model\n', datestr(now));
path('dogma',path);
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
m0.parser = parser;
m0.feats = feats;
m0.step = 1e7;
m0.batchsize = 1e3;

fprintf('%s Loading logs/conllWSJToken_bansal100.mat...\n', datestr(now));
load logs/conllWSJToken_bansal100.mat;  % trn,dev,tst; 7 mins

fprintf('%s trndump...\n', datestr(now)); % 20 mins
[~,trndump] = vectorparser(m0, trn(1:5000), 'update', 0, 'predict', 0);

fprintf('%s devdump...\n', datestr(now)); % 7 mins
[~,devdump] = vectorparser(m0, dev, 'update', 0, 'predict', 0);

fprintf('Saving...\n');
save(dumpfile, 'm0', 'trndump', 'devdump', '-v7.3');

end



==> run/run_featselect_bansal5k.m <==
function run_featselect_bansal5k(parser, featname, cachefile)

fprintf('%s Initializing features %s\n', datestr(now), featname);
newFeatureVectors;
feats = eval(featname);
size_feats = size(feats)

fprintf('%s Initializing model\n', datestr(now));
path('dogma',path);
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
m0.parser = parser;
m0.feats = feats;
m0.step = 1e7;
m0.batchsize = 1e3;

fprintf('%s Loading logs/conllWSJToken_bansal100.mat...\n', datestr(now));
load logs/conllWSJToken_bansal100.mat;  % trn,dev,tst; 7 mins

fprintf('%s trndump...\n', datestr(now)); % 20 mins
[~,trndump] = vectorparser(m0, trn(1:5000), 'update', 0, 'predict', 0);

fprintf('%s devdump...\n', datestr(now)); % 7 mins
[~,devdump] = vectorparser(m0, dev, 'update', 0, 'predict', 0);

fprintf('%s Starting featselect_gpu\n', datestr(now));
[a,b,c] = featselect_gpu(m0,trndump,devdump);

fprintf('%s this will never happen :)\n', datestr(now));
end


2014-08-01  Deniz Yuret  <dyuret@ku.edu.tr>

	* features.m:
	+ Separate word and context vectors.
	+ Put >= and <= as well as == numeric encodings.
	+ Average in-between feature.
	+ Output feature names and indices.
	+ The new fv102 defined in newFeatureVectors outputs 102 features in 2438 dims.
	+ Update featselect to use the new arctypes:
	+ use mat2str(sortrows(fv)) to get a string representation of feature vectors for caching.
	+ for that to work change fv from 3xn to nx3.
	+ actually that doesn't work, we need to use indices into fidx.
	+ run_loadCoNLL on top two token and type (do we need type? we can just filter using fv!) on $$b40,42,43.
	+ confirm that conll_bansal100 is identical to the first half of conllWSJToken_bansal100.
	= yes except for some 0 vectors (unknowns?) in conll_bansal100.
	+ what if fidx is out of order and featselect is doing logical indexing?
	= fine, fidx is never out of order, f (index into fidx) might be but that is no problem.
	x debug run on $$b41.
	x Saved dump data as trnhybrid.mat and devhybrid.mat. (fv808 arceager), but need fidx!
	x logs/trndump.mat includes the fv102 arceager dumps of trn1 and dev1 from conllToken_wikipedia2MUNK-50.
	+ use fv130 which separates all tokens into word/context and includes s2/n2.
	+ saving initial models:
	+ save logs/model130 archybrid_130 archybrid13_130 arceager_130 arceager13_130

	* archybrid,archybrid13,arceager,arceager13: The xx13 versions are
	replicating Goldberg and Nivre, allowing for multiple
	root-children.  The non-13 versions allow for a single root-child.
	All four make 2n-2 moves for each sentence, the first and the last
	moves are not counted: The first move which always shifts the
	first word is done during initialization, and the last move which
	assigns a root head to the last word in stack is not performed
	(the last word already has head=0).

	- all four oracle_costs verified on dev1.

	- extract all features for feature selection?  or use features
	with different vectors every time?  using vectorparser is out of
	the question, goes one move at a time, not mini-batch.  But then
	we need featureindices, symbolic names etc.  How fast is
	vectorparser in dump mode?  680 wps = 1400secs/ptb.  Too slow
	given that training takes 300secs.  So featselect has to rely on
	the full feature set and explicit indices with names.  (DONE)
	rewrite features to have a compact feature matrix representation.

==> run/run_featselect5k.m <==
function run_featselect5k(parser, feats)

fprintf('%s Loading logs/conllWSJToken_bansal100.mat...\n', datestr(now));
% load logs/conllWSJToken_bansal100.mat;  % trn,dev,tst

fprintf('%s Initializing model\n', datestr(now));
path('dogma',path);
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
m0.parser = parser;
m0.feats = feats;
m0.step = 1e7;
m0.batchsize = 1e3;

fprintf('%s trndump...\n', datestr(now));
[~,trndump] = vectorparser(m0, trn(1:5000), 'update', 0, 'predict', 0);

fprintf('%s devdump...\n', datestr(now));
[~,devdump] = vectorparser(m0, dev, 'update', 0, 'predict', 0);

fprintf('%s Starting featselect_gpu\n', datestr(now));
[a,b,c] = featselect_gpu(m0,trndump,devdump);

fprintf('%s this will never happen :)\n', datestr(now));
end


==> run/run_loadCoNLL.m <==
function run_loadCoNLL(emi)

root = 'embedded';
fprintf('Reading %s\n', emi);
tic;
  trn = {};
  dev = {};
  tst = {};
  if isdir([root '/' emi '/22'])
    for j=2:21                          % read train
      dir_j = sprintf('%s/%s/%02d', root, emi, j); % e.g. embedded/conllToken_rcv1UNK/02
      fprintf('Reading %s\n', dir_j);
      dp = dir(dir_j);  % e.g. dir('embedded/conllToken_rcv1UNK/02')
      for k=1:numel(dp);
        if dp(k).isdir continue; end
        dpk = dp(k).name;               % e.g. wsj_0201.dp
        if (~strcmp(dpk(end-2:end), '.dp')) continue; end
        trn_k = loadCoNLL([dir_j '/' dpk]); % e.g. embedded/conllToken_rcv1UNK/02/wsj_0201.dp
        trn = [trn trn_k];
      end % for k=1:numel(dp);
    end % for j=2:21

    for j=[0,1,23,24]                          % read test
      dir_j = sprintf('%s/%s/%02d', root, emi, j); % e.g. embedded/conllToken_rcv1UNK/02
      fprintf('Reading %s\n', dir_j);
      dp = dir(dir_j);  % e.g. dir('embedded/conllToken_rcv1UNK/02')
      for k=1:numel(dp);
        if dp(k).isdir continue; end
        dpk = dp(k).name;               % e.g. wsj_0201.dp
        if (~strcmp(dpk(end-2:end), '.dp')) continue; end
        tst_k = loadCoNLL([dir_j '/' dpk]); % e.g. embedded/conllToken_rcv1UNK/02/wsj_0201.dp
        tst = [tst tst_k];
      end % for k=1:numel(dp);
    end % for j=2:21

    dir_j = sprintf('%s/%s/%02d', root, emi, 22);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);    % e.g. dir('embedded/conllToken_rcv1UNK/22')
    for k=1:numel(dp);
      if dp(k).isdir continue; end
      dpk = dp(k).name;                 % e.g. wsj_2201.dp
      if (~strcmp(dpk(end-2:end), '.dp')) continue; end
      dev_k = loadCoNLL([dir_j '/' dpk]);
      dev = [dev dev_k];
    end % for k=1:numel(dp)

  elseif isdir([root '/' emi '/01'])
    dir_j = sprintf('%s/%s/%02d', root, emi, 0);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    trn = loadCoNLL([dir_j '/' dp3]);

    dir_j = sprintf('%s/%s/%02d', root, emi, 1);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    dev = loadCoNLL([dir_j '/' dp3]);

    dir_j = sprintf('%s/%s/%02d', root, emi, 2);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    tst = loadCoNLL([dir_j '/' dp3]);

  else
    fprintf('%s is empty\n', emi);
    return;
  end %if
toc;

outfile = ['logs/' emi '.mat'];
save(outfile, 'trn', 'dev', 'tst', '-v7.3');


2014-07-30  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:

	+ Try ArcEager.  -- should be easy 1-epoch, needs code
	= need feature optimization again!
	+ Nivre says it is important not to put the root on the left?
	+ multipe versions of archybrid, arceager (one root-child vs many, see readings in Nivre's mail)

	+ archybrid.m,dumpfeatures2.m: temporary new versions to replace
	the old when scripts finish.  need to keep these separate until
	featselect etc. is updated.  or replace featselect with trainparser.

	+ experiment with different number encodings for distance and
	children.
	+ experiment with in-between features?
	+ try separating the type and context parts of token vectors.


==> run/run_embeddings1.m <==
function run_embeddings1(emi)

gpu = gpuDeviceCount();
if gpu==0 error('No GPU'); end

path('dogma', path);
fv804 = [
%n0 s0 s1 n1 n0l1 s0r1 s0l1 s1r1 s0r
  0 -1 -2  1  0   -1   -1   -2   -1;
  0  0  0  0 -1    1   -1    1    0;
  0  0  0  0  0    0    0    0    2;
];
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;

root = 'embedded';
fprintf('Reading %s\n', emi);
tic;
  trn_i = {};
  dev_i = {};
  if isdir([root '/' emi '/22'])
    for j=2:21                          % read train
      dir_j = sprintf('%s/%s/%02d', root, emi, j); % e.g. embedded/conllToken_rcv1UNK/02
      fprintf('Reading %s\n', dir_j);
      dp = dir(dir_j);  % e.g. dir('embedded/conllToken_rcv1UNK/02')
      for k=1:numel(dp);
        if dp(k).isdir continue; end
        dpk = dp(k).name;               % e.g. wsj_0201.dp
        if (~strcmp(dpk(end-2:end), '.dp')) continue; end
        trn_k = loadCoNLL([dir_j '/' dpk]); % e.g. embedded/conllToken_rcv1UNK/02/wsj_0201.dp
        trn_i = [trn_i trn_k];
      end % for k=1:numel(dp);
    end % for j=2:21
    dir_j = sprintf('%s/%s/%02d', root, emi, 22);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);    % e.g. dir('embedded/conllToken_rcv1UNK/22')
    for k=1:numel(dp);
      if dp(k).isdir continue; end
      dpk = dp(k).name;                 % e.g. wsj_2201.dp
      if (~strcmp(dpk(end-2:end), '.dp')) continue; end
      dev_k = loadCoNLL([dir_j '/' dpk]);
      dev_i = [dev_i dev_k];
    end % for k=1:numel(dp)

  elseif isdir([root '/' emi '/01'])
    dir_j = sprintf('%s/%s/%02d', root, emi, 0);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    trn_i = loadCoNLL([dir_j '/' dp3]);

    dir_j = sprintf('%s/%s/%02d', root, emi, 1);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    dev_i = loadCoNLL([dir_j '/' dp3]);

  else
    fprintf('%s is empty\n', emi);
    return;
  end %if
toc;
fprintf('dumpfeatures...\n');
tic; [trn_x, trn_y] = dumpfeatures2(trn_i, fv804); toc;
tic; [dev_x, dev_y] = dumpfeatures2(dev_i, fv804); toc;
fprintf('k_perceptron_multi_train_gpu...\n');
gpuDevice(1);
tic;m1=k_perceptron_multi_train_gpu(trn_x,trn_y,m0); toc;
tic;[a,b]=model_predict_gpu(dev_x, m1, 1); toc;
score = numel(find(a ~= dev_y))/numel(dev_y);
fprintf('score:%g\t%s\n', score, emi);

==> run/run_embeddings1.sh <==
#!/bin/bash
#$ -N job_name
#$ -S /bin/bash
#$ -q ilac.q
#$ -cwd
#$ -o job_name.out
#$ -e job_name.err
#$ -M login_id@ku.edu.tr
#$ -m bea

# We have a file named bigtest.m
/share/apps/matlab/R2011a/bin/matlab -nojvm -nodisplay -r bigtest > output.txt

==> run/run_embeddings2.m <==
path('dogma', path);
fv804 = [
%n0 s0 s1 n1 n0l1 s0r1 s0l1 s1r1 s0r
  0 -1 -2  1  0   -1   -1   -2   -1;
  0  0  0  0 -1    1   -1    1    0;
  0  0  0  0  0    0    0    0    2;
];
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;

root = 'embedded';
em = dir(root);
for i=1:numel(em)
  emi = em(i).name;                     % e.g. conllToken_rcv1UNK
  if ~any(strcmp(emi, {
    % 'conllWSJToken_levy300',		% error
    % 'conllWSJToken_bansal100',	% .0370
    % 'conllWSJToken_dep-WSJ',		% .0490
    % 'conllWSJToken_ungar2-0.1',	% .0520
    % 'conllWSJToken_stratos50k5000scaled01', % .0604
    % 'conllWSJToken_gsng1M',		% .0610
    % 'conllWSJToken_gsngAE',		% .0631
    % 'conllWSJToken_gsngALL',		% .0662
    % 'conllWSJToken_murphy50scaled01',	% error
    % 'conllWSJToken_murphy50'		% error
                      }))
    continue;
  end
  fprintf('Reading %s\n', emi);
  tic;
  trn_i = {};
  dev_i = {};
  if isdir([root '/' emi '/24'])
    for j=2:21                          % read train
      dir_j = sprintf('%s/%s/%02d', root, emi, j); % e.g. embedded/conllToken_rcv1UNK/02
      fprintf('Reading %s\n', dir_j);
      dp = dir(dir_j);  % e.g. dir('embedded/conllToken_rcv1UNK/02')
      for k=1:numel(dp);
        if dp(k).isdir continue; end
        dpk = dp(k).name;               % e.g. wsj_0201.dp
        if (~strcmp(dpk(end-2:end), '.dp')) continue; end
        trn_k = loadCoNLL([dir_j '/' dpk]); % e.g. embedded/conllToken_rcv1UNK/02/wsj_0201.dp
        trn_i = [trn_i trn_k];
      end % for k=1:numel(dp);
    end % for j=2:21
    dir_j = sprintf('%s/%s/%02d', root, emi, 22);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);    % e.g. dir('embedded/conllToken_rcv1UNK/22')
    for k=1:numel(dp);
      if dp(k).isdir continue; end
      dpk = dp(k).name;                 % e.g. wsj_2201.dp
      if (~strcmp(dpk(end-2:end), '.dp')) continue; end
      dev_k = loadCoNLL([dir_j '/' dpk]);
      dev_i = [dev_i dev_k];
    end % for k=1:numel(dp)

  elseif isdir([root '/' emi '/02'])
    dir_j = sprintf('%s/%s/%02d', root, emi, 0);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    trn_i = loadCoNLL([dir_j '/' dp3]);

    dir_j = sprintf('%s/%s/%02d', root, emi, 1);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    dev_i = loadCoNLL([dir_j '/' dp3]);

  else
    fprintf('%s is empty\n', emi);
    continue;
  end %if
  toc;
  fprintf('dumpfeatures...\n');
  tic; [trn_x, trn_y] = dumpfeatures2(trn_i, fv804); toc;
  tic; [dev_x, dev_y] = dumpfeatures2(dev_i, fv804); toc;
  fprintf('k_perceptron_multi_train_gpu...\n');
  gpuDevice(1);
  tic;m1=k_perceptron_multi_train_gpu(trn_x,trn_y,m0); toc;
  tic;[a,b]=model_predict_gpu(dev_x, m1, 1); toc;
  score = numel(find(a ~= dev_y))/numel(dev_y);
  fprintf('score:%g\t%s\n', score, emi);
end % for i=1:numel(em)

==> run/run_embeddings.m <==
path('dogma', path);
fv804 = [
%n0 s0 s1 n1 n0l1 s0r1 s0l1 s1r1 s0r
  0 -1 -2  1  0   -1   -1   -2   -1;
  0  0  0  0 -1    1   -1    1    0;
  0  0  0  0  0    0    0    0    2;
];
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;

root = 'embedded';
em = dir(root);
for i=1:numel(em)
  emi = em(i).name;                     % e.g. conllToken_rcv1UNK
  if strcmp(emi(1),'.') continue; end
  if any(strcmp(emi, {
'conllWSJToken_bansal100',
'conllWSJToken_wikipedia2MUNK-200',
'conllWSJToken_wikipedia2MUNK-100',
'conllToken_wikipedia2MUNK-50',
'conllToken_wikipedia2MUNK-25',
'conll_bansal100',
'conllWSJToken_rcv1UNK50',
'conllToken_rcv1UNK',
'conllWSJToken_rcv1UNK',
'conllWSJToken_rcv1UNK25',
'conllWSJToken_dep-100',
'conll_cw100scaled',
'conllWSJToken_huang-0.1',
'conll_4scode+f50',
'conll_2scode+f100',
'conll_3scode+f100',
'conll_6scode+f50',
'conll_4scode+f100',
'conll_6scode+f100',
'conll_5scode+f100',
'conllWSJToken_dep-WSJ',
'conll_10scode+f50',
'conll_7scode+f50',
'conll_2scode+f50',
'conll_1scode+f100',
'conll_1scode+f50',
'conll_9scode+f50',
'conll_3scode+f50',
'conll_5scode+f50',
'conll_8scode+f50',
'conll_3scode+f25',
'conllWSJToken_cw50scaled',
'conll_cw50scaled',
'conllWSJToken_ungar2-0.1',
'conllWSJToken_hlbl50scaled',
'conll_cw',
'conllWSJToken_mikolovWiki-0.1',
'conll_dep-100',
'conll_dep-WSJ',
'conll_cw25+cw25',
'conll_cw25',
'conllWSJToken_cw25',
'conllWSJToken_stratos50k5000scaled01',
'conllWSJToken_gsng1M',
'conllWSJToken_mikolovRCV50scaled01',
'conllWSJToken_cw25scaled',
'conll_cw25scaled',
'conllWSJToken_gsngAE',
'conllWSJToken_gsngALL',
'conll_dep100+wiki25',
'conllWSJToken_yogamata52scaled01',
'conllWSJToken_yogamata52',
'conllWSJToken_stratos50k200scaled01',
'conll_german+german',
'conllWSJToken_manaal48',
'conllWSJToken_hpca50scaled01',
'conllWSJToken_manaal48scaled01',
'conll_german',
'conllWSJToken_rnn80',
'conll_dep100+mik',
'conllWSJToken_levy300',
'conllWSJToken_mikolovGoogleNews300',
'conllWSJToken_murphy50',
'conllWSJToken_murphy50scaled01',
'conllWSJToken_wikipedia2MUNK-25',
'conll_10scode+f50',
'conll_german2',

                     }))
    fprintf('Skipping %s\n', emi);
    continue;
  end

  fprintf('Reading %s\n', emi);
  tic;
  trn_i = {};
  dev_i = {};
  if isdir([root '/' emi '/22'])
    for j=2:21                          % read train
      dir_j = sprintf('%s/%s/%02d', root, emi, j); % e.g. embedded/conllToken_rcv1UNK/02
      fprintf('Reading %s\n', dir_j);
      dp = dir(dir_j);  % e.g. dir('embedded/conllToken_rcv1UNK/02')
      for k=1:numel(dp);
        if dp(k).isdir continue; end
        dpk = dp(k).name;               % e.g. wsj_0201.dp
        if (~strcmp(dpk(end-2:end), '.dp')) continue; end
        trn_k = loadCoNLL([dir_j '/' dpk]); % e.g. embedded/conllToken_rcv1UNK/02/wsj_0201.dp
        trn_i = [trn_i trn_k];
      end % for k=1:numel(dp);
    end % for j=2:21
    dir_j = sprintf('%s/%s/%02d', root, emi, 22);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);    % e.g. dir('embedded/conllToken_rcv1UNK/22')
    for k=1:numel(dp);
      if dp(k).isdir continue; end
      dpk = dp(k).name;                 % e.g. wsj_2201.dp
      if (~strcmp(dpk(end-2:end), '.dp')) continue; end
      dev_k = loadCoNLL([dir_j '/' dpk]);
      dev_i = [dev_i dev_k];
    end % for k=1:numel(dp)

  elseif isdir([root '/' emi '/01'])
    dir_j = sprintf('%s/%s/%02d', root, emi, 0);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    trn_i = loadCoNLL([dir_j '/' dp3]);

    dir_j = sprintf('%s/%s/%02d', root, emi, 1);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    dev_i = loadCoNLL([dir_j '/' dp3]);

  else
    fprintf('%s is empty\n', emi);
    continue;
  end %if
  toc;
  fprintf('dumpfeatures...\n');
  tic; [trn_x, trn_y] = dumpfeatures2(trn_i, fv804); toc;
  tic; [dev_x, dev_y] = dumpfeatures2(dev_i, fv804); toc;
  fprintf('k_perceptron_multi_train_gpu...\n');
  gpuDevice(1);
  tic;m1=k_perceptron_multi_train_gpu(trn_x,trn_y,m0); toc;
  tic;[a,b]=model_predict_gpu(dev_x, m1, 1); toc;
  score = numel(find(a ~= dev_y))/numel(dev_y);
  fprintf('score:%g\t%s\n', score, emi);
end % for i=1:numel(em)


2014-07-28  Deniz Yuret  <dyuret@ku.edu.tr>

	* eval_conll.m: this is not giving the same result as
	below.  testing vectorparser.m.
	- OK, all whead numbers were wrong! Fixed trainparser_gpu.
	- ptrans were different, fixed eval_conll.m
	- Confirm that total cost is equal to total head error.
	- It is not, more debugging of archybrid.oracle_cost is needed.
	- Done, total cost is equal to total head error.
	- DONE: do not use old ArcHybrid.m any more.

	* archybrid.m:
	- However now ptrans does not match old results exactly either!
	- Neither does the number of transitions?
  	  Why: only multi-choice transitions were used and their number changed with new archybrid.
	  Confirm: dev1 has 1700 sents, 40117 words, 2n-2=76834 transitions.

	  old dev_x: (1768,72551)
	  [x0,y0] = dumpfeatures(dev_w, dev_h); % (1768,72551): uses old ArcHybrid.m, only multi-choice transitions
	  [x1,y1] = dumpfeatures(dev_w, dev_h, 1); % (1768,76834): all transitions with ArcHybrid.m
	  [x2,y2] = dumpfeatures2(dev1, fv1768); % (1768,72555): uses new archybrid.m
	  [x3,y3] = dumpfeatures2(dev1, fv1768, 1); % (1768,76834): all transitions
	  m0.parser = @archybrid; m0.feats=fv1768;
	  [~,d4] = vectorparser(m0, dev1, 'update', 0, 'predict', 0); % (1768,76834): all transitions
	  check multi-choice transition count: 72555 confirmed.
	  numel(find(sum(isinf(d4.cost)) < 2)) => 72555

	* vectorparser.m: trainparser, testparser, dumpfeatures etc do
	very similar things.  We'll combine them all in this program.

	- Confirmed works correctly for dump, however all moves are
	dumped, not just the multi-choice ones.

	[m,d] = vectorparser(m0, dev1, 'update', 0, 'predict', 0);
	[x0,y0]=dumpfeatures2(dev1, fv808, 1);
	all(d.x(:) == x0(:)) => 1
	all(d.y(:) == y0(:)) => 1

	- Confirm for test.

	r=trainparser_gpu(mx5d1, dev1, fv808)
	[mz5,dz5]=vectorparser(mx5d1, dev1, 'update', 0);
	eval_conll(dev1, dz5)  % gives the same results

	- Confirm for train.

	mx5d1.parser = @archybrid; mx5d1.feats=fv808;
	[mz5d2,dz5d2] = vectorparser(mx5d1, trn1)
	[rt5d2, mx5d2] = trainparser_gpu(mx5d1, trn1, fv808)
	all(mx5d2.SV(:)==mz5d2.SV(:)) => 1
	all(mx5d2.beta(:)==mz5d2.beta(:)) => 1
	all(mx5d2.beta2(:)==mz5d2.beta2(:)) => 1

	- test if saving single-choice moves in dump makes a big
	difference in static oracle training, if it does find a way to
	filter the multi-choice instances.

	hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
	m0=model_init(@compute_kernel,hp);
	m0.parser=@archybrid; m0.feats=fv808;
	[~,trndump] = vectorparser(m0, trn1, 'update', 0, 'predict', 0);
	mz1 = k_perceptron_multi_train_gpu(trndump.x, trndump.y, m0);
	[~,devdump] = vectorparser(m0, dev1, 'update', 0, 'predict', 0);
	az1 = model_predict_gpu(devdump.x, mz1, 1);
	numel(find(az1 ~= devdump.y))/numel(devdump.y) => 0.0363=2786/76834
	Compare to mx1 which had 0.0375=2723/72551 or 0.0354=2723/76834

	So it does help a bit in first epoch (0.1%) to filter single-choice moves before training.
	Saved dump data as trnhybrid.mat and devhybrid.mat.

	DONE- Dump, train, test, and confirm for arceager.

	DONE- Remove trainparser, testparser, dumpfeatures variants.

	DONE- Start featselect with arceager.

	* arceager.m: should modify trainparser to also do dumpfeatures so
	we can test.  Checking oracle_cost.  Now checking archybrid oracle
	cost.  They both passed the test.  DONE: create two versions of
	arceager for testing, my version and the GoldbergNivre version.
	First try with a standard feature set, then do feature
	optimization.

	* run_dynamic_oracle2.m: with the new fv808 embeddings and mx5 model.  best at 7 epochs.
	$$i11			TEST				TRAIN
	model	nsv	time	gtrans	ptrans	phead	whead	ptrans	phead	whead
	mx5	169135	963	3.34	3.76	9.83	8.63	-	-
	mx5d1	204369	22472	4.07	3.78	10.02	8.83	3.02	8.37	7.42
	mx5d2	232151	?	4.02	3.63	9.59	8.47	2.73	7.60	6.77
	mx5d3	254535	?	3.99	3.49	9.23	8.16	2.45	6.88	6.13
	mx5d4	273226	?	4.00	3.47	9.20	8.12	2.23	6.28	5.61
	mx5d5	289077	?	3.99	3.39	9.02	7.98	2.02	5.73	5.11
	mx5d6	302335	?	3.99	3.39	8.96	7.92	1.83	5.24	4.67
	mx5d7	314292	?	4.00	3.37	8.90	7.88	1.71	4.92	4.40
	mx5d8	324433	?	4.02	3.42	9.05	8.00	1.56	4.54	4.06
	mx5d9	333380	?	4.04	3.41	9.02	7.98	1.46	4.25	3.80
	mx5d10	341257	?	4.04	3.41	9.00	7.94	1.36	4.00	3.59

	* run_dynamic_oracle.m: $$i10 best results in 9 dynamic oracle epochs.  However this is with fv804, fv808 is better.
				TEST				TRAIN
	model	nsv	time	gtrans	ptrans	phead	whead	ptrans	phead	whead
	m10	536294	21252	3.40	3.76	 9.89
	m10c	202888	-	3.40	3.76	 9.89	8.68
	m10d	230798	27415	4.07	3.84	10.15		2.84	7.90
	m10d2	251746	30510	4.06	3.67	 9.68		2.58	7.21
	m10d3	268378	?	4.07	3.59	 9.46		2.35	6.60	5.89
	m10d4	282674	31102	4.07	3.56	 9.39	8.22	2.21	6.23	5.57
	m10d5	303011	?	4.05	3.55	 9.33	8.19	2.06	5.84	5.22
	m10d6   305964	?	4.03	3.50	 9.18	8.06	1.93	5.51	4.93
	m10d7	315482	?	4.05	3.48	 9.15	8.05	1.82	5.20	4.69
	m10d8	324031	?	4.06	3.49	 9.16	8.05	1.73	4.98	4.49
	m10d9	331568	?	4.04	3.46	 9.12	8.03	1.62	4.69	4.23
	m10d10	338353	?	4.06	3.47	 9.16	8.08	1.54	4.45	4.03

	* run_embeddings.m: restarted again on after solving the issues
	below.  Compare the following to m1 with 0.0401 (aer:6.48) from
	conllWSJToken_wikipedia2MUNK-50.  sv is number of support vectors
	at the end of one epoch.  n is the training set size, 1720842 when
	(or 1720986 after Jul-29) not specified.

	score:0.0370498	conllWSJToken_bansal100 (sv:5.58)
	score:0.0395859	conllWSJToken_wikipedia2MUNK-200 (sv:6.28)
	score:0.0396549	conllWSJToken_wikipedia2MUNK-100 (sv:6.44)
	score:0.0401373	conllToken_wikipedia2MUNK-50 (sv:6.48)  -- this is m1
	score:0.0404659	conllWSJToken_wikipedia2MUNK-50
	score:0.0409643	conllToken_wikipedia2MUNK-25 (sv:6.56)
	score:0.0410309	conllWSJToken_wikipedia2MUNK-25
	score:0.0414881	conllWSJToken_rcv1UNK50 (sv:6.72)
	score:0.042067	conllToken_rcv1UNK (sv:6.82)
	score:0.042067	conllWSJToken_rcv1UNK (sv:6.82)
	score:0.042067	conllWSJToken_rcv1UNK25 (sv:6.82)
	score:0.0464087	conllWSJToken_dep-100 (sv:6.07)
	score:0.0477044	conllWSJToken_huang-0.1 (sv:7.30)
	score:0.049	conllWSJToken_dep-WSJ (sv:6.41)
	score:0.0516464	conllWSJToken_cw50scaled (sv:8.62)
	score:0.0519772	conllWSJToken_ungar2-0.1 (sv:8.56)
	score:0.0523769	conllWSJToken_hlbl50scaled (sv:8.78)
	score:0.0532729	conllWSJToken_mikolovWiki-0.1 (sv:8.05)
	score:0.0596959	conllWSJToken_cw25 (sv:9.74)
	score:0.0603713	conllWSJToken_stratos50k5000scaled01 (sv:9.84)
	score:0.0610054	conllWSJToken_gsng1M (sv:9.83)
	score:0.0611018	conllWSJToken_mikolovRCV50scaled01 (sv:10.18)
	score:0.0626732	conllWSJToken_cw25scaled (sv:10.27)
	score:0.063128	conllWSJToken_gsngAE (sv:10.33)
	score:0.0661604	conllWSJToken_gsngALL (sv:10.56)
	score:0.068793	conllWSJToken_yogamata52scaled01 (sv:10.57)
	score:0.0733691	conllWSJToken_yogamata52 (sv:11.03)
	score:0.0763187	conllWSJToken_stratos50k200scaled01 (sv:12.57)
	score:0.0796681	conllWSJToken_manaal48 (sv:13.16)
	score:0.0829072	conllWSJToken_hpca50scaled01 (sv:13.54)
	score:0.0929829	conllWSJToken_manaal48scaled01 (sv:15.55)
	score:0.104547	conllWSJToken_rnn80 (sv:16.37)

	score:0.0409781	conll_bansal100 (sv:5.99)
	score:0.04591	conll_wikipedia2MUNK-200
	score:0.0461581	conll_wikipedia2MUNK-50
	score:0.0463373	conll_wikipedia2MUNK-100
	score:0.0469161	conll_rcv1UNK200
	score:0.0474563	conll_cw100scaled (sv:7.44)
	score:0.0477431	conll_wikipedia2MUNK-25
	score:0.0477844	conll_rcv1UNK100
	score:0.0479085	conll_rcv1UNK50
	score:0.0483108	conll_4scode+f50 (sv:8.00)
	score:0.0485589	conll_2scode+f100 (sv:7.96)
	score:0.0487381	conll_3scode+f100 (sv:7.93)
	score:0.048876	conll_6scode+f50 (sv:8.01)
	score:0.0488897	conll_4scode+f100 (sv:7.93)
	score:0.0489173	conll_6scode+f100 (sv:7.98)
	score:0.0489311	conll_5scode+f100 (sv:7.96)
	score:0.0489973	conll_rcv1UNK
	score:0.049	conll_10scode+f50 (sv:7.96)
	score:0.0490276	conll_7scode+f50 (sv:8.02)
	score:0.0491792	conll_2scode+f50 (sv:8.00)
	score:0.049193	conll_1scode+f100 (sv:7.97)
	score:0.0492619	conll_1scode+f50 (sv:7.99)
	score:0.0494135	conll_9scode+f50 (sv:8.00)
	score:0.0494686	conll_3scode+f50 (sv:8.01)
	score:0.0494686	conll_5scode+f50 (sv:8.02)
	score:0.0495927	conll_8scode+f50 (sv:8.04)
	score:0.0505685	conll_hlbl100scaled
	score:0.0513266	conll_ungar2
	score:0.0519634	conll_cw50scaled (sv:8.52)
	score:0.0523603	conll_ungar2-0.1
	score:0.0529391	conll_stratos100k5000scaled01
	score:0.0531597	conll_wsj-dep
	score:0.0545685	conll_dep-100 (sv:6.72)
	score:0.0549544	conll_dep-WSJ (sv:7.34)
	score:0.0589346	conll_mikolov50
	score:0.0598994	conll_hlbl50scaled
	score:0.0621597	conll_mikolovReuters50
	score:0.0630591	conll_cw25scaled (sv:10.30)
	score:0.0645579	conll_hlbl
	score:0.0658811	conll_stratos100k200scaled01
	score:0.0667255	conll_dep100+wiki25 (sv:10.89)
	score:0.0679622	conll_mikolovWikipedia50
	score:0.0746882	conll_huang-0.1
	score:0.0803253	conll_hpca100scaled01
	score:0.0834953	conll_gsngAE
	score:0.0899869	conll_manaal48
	score:0.0907036	conll_mikolovWikipedia25
	score:0.0923024	conll_yogamata52
	score:0.0934188	conll_gsngALL
	score:0.0940666	conll_gsng1M
	score:0.112555	conll_dep100+mik (sv:17.72)
	score:0.118269	conll_rnn80
	score:0.405623	conll_murphy50
	score:0.401681	conll_murphy50scaled01

	score:0.0396251	conll_wiki50+wiki50 (n:1891402)
	score:0.0399145	conll_wiki25+wiki25 (n:1891402)
	score:0.0414444	conll_rcv1UNK25+rcv1UNK+25 (n:1891402/1891272)
	score:0.0502267	conll_3scode+f25 (sv:8.05 n:1891272)
	score:0.0524734	conll_cw (sv:8.21 n:1891272)
	score:0.0583452	conll_cw25+cw25 (sv:9.44 n:1891272)
	score:0.0586484	conll_cw25 (sv:9.35 n:1891272)
	score:0.0687203	conll_mikW25+mikW25 (n:1891402 after Jul-29)

	score:0.0523636	conll07Token_english (sv:8.39 n:808163)
	score:0.0613307	conll07English_wikipedia2MUNK-50 (sv:9.90 n:808163)

	score:0.0865817	conll07Token_czech (sv:13.04 n:757433)
	score:0.0978749	conll07Czech_czechSKETCH50 (sv:15.10 n:757433)

	score:0.0773805	conll_german+german (sv:12.77 n:1254045)
	score:0.0898668	conll_german2+german2 (n:1261947new)
	score:0.0944709	conll_german (sv:15.02 n:1254045)
	score:0.0998288	conll_german2 (sv:14.43 n:1254045)

	score:0.124902	conll_hungarian (n:301537)
	score:0.126377	conll_hungarian+f (n:301537)

	score:0.146755	conll_polish (n:107202)
	score:0.15181	conll_polish+f (n:107202)

	score:0.132375	conll_swedish (n:131239)
	score:0.13461	conll_swedish+f (n:131239)

	score:0.172941	conll_turkishSKETCH-25 (n:87228)

	$$i02 $$i03: still oom, do this on balina
	score:0.99 conllWSJToken_levy300 (out of memory at 1405179, sv:126177, nk:11, g:6.4e6)
	score:0.99 conll_levy300 (out of memory at 1596181, sv:165712, nk:263, g:9.8e7)
	score:0.99 conllWSJToken_mikolovGoogleNews300 (out of memory at 1111004, sv:123146, nk:19, g:4.1e6)
	score:0.99 conllWSJToken_murphy50 (out of memory at 1182126, sv:494871, nk:85, g:9.4e7)
	score:0.99 conllWSJToken_murphy50scaled01 (out of memory at 1289463, sv:494698, nk:85, g:9.4e7)

	score:0.99 conll_french has two test files
	score:0.99 conll_french+f has two test files

	score:0.99 conll08_wikipedia2MUNK-25 is empty
	score:0.99 conllToken_cw25scaled is empty
	score:0.99 conllToken_rnn80 is empty

2014-07-27  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_multi_epoch2.m:

	model	aer	time	nsv	gtrans	ptrans	phead	whead	notes
	m_trn		15841	101517	3.78	4.25	11.02		old fulltrain+static model
	m_trn01		52817	251480	3.89	3.94	10.15		old fulltrain+static+dynamic
	m1	6.47	321	111431	4.01	4.47	11.75		one epoch batchsize=1250 fv804
	mx1	6.09	314	104757	3.75	4.26	11.18		fv808 feature-set batch=1000
	m2	5.94	6105	183901	3.64	4.09	10.80		second epoch batchsize=1000
	mx2	4.89	727	133856	3.50	3.95	10.36		all mx are fv808 batch=1000
	m3	5.09	8781	245190	3.50	3.99	10.37		third epoch batchsize=1000
	mx3	4.11	860	150207	3.38	3.80	 9.96
	m4	4.58	11049	299268	3.44	3.91	10.17
	mx4	3.54	930	161208	3.36	3.80	 9.88
	m5	4.21	12990	347577	3.42	3.90	10.17
	mx5	3.21	963	169135	3.34	3.76	 9.83	8.63	best epoch for fv808: gtrans:3.34 phead:9.83 whead:7.60
	m6	3.92	15004	391424	3.42	3.89	10.13
	mx6	2.92	1000	175089	3.35	3.77	 9.86
	m7	3.69	16784	431962	3.43	3.83	10.02
	mx7	2.69	1019	179795	3.35	3.77	 9.90
	m8	3.49	18120	469130	3.42	3.82	10.00
	mx8	2.50	1030	183630	3.36	3.81	 9.95
	m9	3.33	19897	503651	3.41	3.79	 9.99
	mx9	2.35	1047	186983	3.35	3.83	10.01		first epoch where fv808 is worse than fv804
	m10x	3.18	21252	536294	3.40	3.76	 9.89
	m10	3.18	?	204438	3.40	3.76	 9.89
	mx10	2.22	1051	189857	3.34	3.83	10.01
	m11x	3.05	22599	566656	3.39	3.75	 9.85		out of memory, compactify
	m11	3.01	1182	207516	3.39	3.76	 9.89		new gpu version, diff due to reduced batchsize towards the end
	mx11	2.11	1062	192309	3.35	3.87	10.11
	m12x	2.90	1196	210024	3.41	3.76	 9.94		compactified, run on gpu
	m12	2.86	1197	210313	3.39	3.74	 9.86	8.64	best epoch for fv804: ptrans:3.74
	mx12	2.01	1070	194373	3.35	3.86	10.09
	m13	2.70	1215	212685	3.39	3.76	 9.94
	mx13	1.92	1081	196166	3.36	3.86	10.09
	m14	2.56	1233	214830	3.40	3.78	 9.98
	mx14	1.84	1087	197736	3.36	3.85	10.08
	m15	2.44	1239	216702	3.41	3.80	10.05
	mx15	1.76	1095	199189	3.37	3.85	10.06
	m16	2.34	1234	218294	3.41	3.79	10.02
	mx16	1.69	1097	200424	3.37	3.83	10.04
	m17	2.25	1234	219771	3.41	3.80	10.05
	mx17	1.63	1100	201609	3.37	3.85	10.11
	m18	2.17	1226	221062	3.41	3.82	10.07
	mx18	1.57	1105	202676	3.38	3.84	10.07		b43
	m19	2.10	1230	222344	3.42	3.84	10.11
	mx19	1.52	1107	203550	3.37	3.85	10.10
	m20	2.04	1241	223532	3.43	3.82	10.08		b40
	mx20	1.47	1112	204417	3.39	3.89	10.19
	model	aer	time	nsv	gtrans	ptrans	phead	whead	notes


	* featselect_gpu.m: Trying the best feature vector:
	fv808 = [
	%n0 s0 s1 n1 n0l1 s0r1 s1r1l s0l1 s0r1l s2
	  0 -1 -2  1  0   -1   -2    -1   -1    -3;
          0  0  0  0 -1    1    1    -1    1     0;
	  0  0  0  0  0    0   -2     0   -2     0;
	];

	First confirm it does give the same result as featselect:
	dyuret@yunus:~/vectorparser[0]$ grep 'n0,n0l1,n1,s0,s0l1,s0r1,s0r1l,s1,s1r1l,s2' logs/0726-biyofiz-4-3b.log
	5.09	3.71	102805	427.76	n0,n0l1,n1,s0,s0l1,s0r1,s0r1l,s1,s1r1l,s2
	>> newFeatureVectors
	trn1=loadCoNLL('embedded/conllWSJToken_wikipedia2MUNK-50/00/wsj_0001.dp');
	save logs/trn1 trn1
	hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3)
	model_bak = model_init(@compute_kernel,hp);
	model_bak.step = 200;
	model_bak.batchsize = 500;
	[trn1x, trn1y] = dumpfeatures2(trn1, fv808);
	[dev1x, dev1y] = dumpfeatures2(dev1, fv808);
	tic;gpuDevice(1);
	model_perceptron = k_perceptron_multi_train_gpu(trn1x,trn1y,model_bak);
	pred_perceptron_last = model_predict_gpu(dev1x,model_perceptron,0);
	pred_perceptron_av = model_predict_gpu(dev1x,model_perceptron,1);
	telapsed = toc();
	err_last = numel(find(pred_perceptron_last~= dev1y))/numel(dev1y)*100;
	err_av = numel(find(pred_perceptron_av~=dev1y))/numel(dev1y)*100;
	nsv = size(model_perceptron.beta, 2);
	fprintf('%.2f\t%.2f\t%d\t%.2f\t%s\n', ...
          err_last, err_av, nsv, telapsed, fkey);
	5.09	3.71	102805	404.16	Confirmed!

	After confirming this is the right feature set, rerun the
	multi-epoch (don't forget to compactify after every epoch, or
	modify k_perceptron to do multi-epoch and print stats, or output a
	sequence of models).

	* run_multi_epoch.m: When training the second epoch (starting
	with 100K sv) the same relation does not hold, bigger batches are
	faster.  The max speed is about 10x slower than the beginning.  It
	takes 20-25 secs to do 10K.  So one epoch starting at 100K sv
	should take about an hour or two.  Looking at multi-epoch static
	oracle performance.

	* featselect_gpu.m: we can do feature selection with the
	full data!  (using batchsize=500)

	3	8.56	6.01	165880	424.36	n0,s0,s1
	4	7.43	5.19	143744	389.52	n0,(n1),s0,s1
	5	6.14	4.52	123114	374.21	n0,(n0l1),n1,s0,s1
	6	5.63	4.15	113844	427.12	n0,n0l1,n1,s0,(s0r1),s1
	7	5.46	3.93	108832	416.48	n0,n0l1,n1,s0,s0r1,s1,(s1r1l)
	8	5.44	3.88	104300	411.61	n0,n0l1,n1,s0,s0l1,s0r1,s1,(s1r1)  -- previous (5k) best8
	8	5.27	3.82	104968	992.87	n0,n0l1,n1,s0,(s0l1),s0r1,s1,s1r1l
	9	5.29	3.86	105390	399.62	n0,n0l1,n1,s0,s0l1,(s0r),s0r1,s1,s1r1  -- previous (5k) best9
	9	5.13	3.76	104270	398.77	n0,n0l1,n1,s0,s0l1,s0r1,(s0r1l),s1,s1r1l
	10	5.09	3.71	102805	427.76	n0,n0l1,n1,s0,s0l1,s0r1,s0r1l,s1,s1r1l,(s2)
	11	5.27	3.70	103133	430.94	n0,n0l1,n1,s0,s0l1,s0r1,s0r1l,s1,(s1l1r),s1r1l,s2

	Got interrupted.  To restart we need to reconstruct the cache from
	textual output:

	$ ./featselect-cache.pl ~/yunus/parser/logs/0724-ilac-2-1.log >	~/yunus/parser/logs/0724-ilac-2-1.cache
	>> c0 = featselect_cache('logs/0724-ilac-2-1.cache');
	>> save logs/0724-ilac-2-1.mat c0
	>> [a,b,c] = featselect_gpu(trn_x, trn_y, dev_x, dev_y, 3, 'n0,s0,s1', c0);


	* dynamic-oracle:
	>> load m10
	>> m10c = compactify(m10)
	>> trn1=loadCoNLL('embedded/conllWSJToken_wikipedia2MUNK-50/00/wsj_0001.dp');
	>> newFeatureVectors;
	>> [rt10d, m10d] = trainparser_gpu(m10c, trn1, fv804);
	>> [at10d,~]=model_predict_gpu(trn_x(idx,:), m10d, 1);
	>> r10d = trainparser_gpu(m10d, dev1, fv804);  % check same as t10d
	>> [a10d,~]=model_predict_gpu(dev_x(idx,:), m10d, 1);
	>> [rt10d2, m10d2] = trainparser_gpu(m10d, trn1, fv804); % second epoch
	>> [rt10d3, m10d3] = trainparser_gpu(m10d2, trn1, fv804); % third epoch

	DONE: compactify does not work after trainparser_gpu.  It is
	hopeless to try to do this in a multi-epoch experiment with a
	dynamic oracle because the sequence of instances keep changing.
	We need to modify compactify and model_sparsify to not look at
	model.S.  Fixed compactify and added it to the end of
	trainparser_gpu.  Still need to fix model_sparsify (LATER) and
	k_perceptron_multi_train_gpu (compactify not part of dogma?).



==> run/run_dynamic_oracle2.m <==
load logs/mx5
newFeatureVectors
idx808 = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s1r1l','s0l1','s0r1l','s2'});

toc;tic;[rt10d1, mx5d1] = trainparser_gpu(mx5, trn1, fv808)
toc;tic;[a10d1,~]=model_predict_gpu(dev_x(idx808,:), mx5d1, 1);
numel(find(a10d1 ~= dev_y))/numel(dev_y)
toc;tic;r10d1 = trainparser_gpu(mx5d1, dev1, fv808)
save -v7.3 logs/mx5d1 mx5d1 rt10d1 r10d1 a10d1

toc;tic;[rt10d2, mx5d2] = trainparser_gpu(mx5d1, trn1, fv808)
toc;tic;[a10d2,~]=model_predict_gpu(dev_x(idx808,:), mx5d2, 1);
numel(find(a10d2 ~= dev_y))/numel(dev_y)
toc;tic;r10d2 = trainparser_gpu(mx5d2, dev1, fv808)
save -v7.3 logs/mx5d2 mx5d2 rt10d2 r10d2 a10d2

toc;tic;[rt10d3, mx5d3] = trainparser_gpu(mx5d2, trn1, fv808)
toc;tic;[a10d3,~]=model_predict_gpu(dev_x(idx808,:), mx5d3, 1);
numel(find(a10d3 ~= dev_y))/numel(dev_y)
toc;tic;r10d3 = trainparser_gpu(mx5d3, dev1, fv808)
save -v7.3 logs/mx5d3 mx5d3 rt10d3 r10d3 a10d3

toc;tic;[rt10d4, mx5d4] = trainparser_gpu(mx5d3, trn1, fv808)
toc;tic;[a10d4,~]=model_predict_gpu(dev_x(idx808,:), mx5d4, 1);
numel(find(a10d4 ~= dev_y))/numel(dev_y)
toc;tic;r10d4 = trainparser_gpu(mx5d4, dev1, fv808)
save -v7.3 logs/mx5d4 mx5d4 rt10d4 r10d4 a10d4

toc;tic;[rt10d5, mx5d5] = trainparser_gpu(mx5d4, trn1, fv808)
toc;tic;[a10d5,~]=model_predict_gpu(dev_x(idx808,:), mx5d5, 1);
numel(find(a10d5 ~= dev_y))/numel(dev_y)
toc;tic;r10d5 = trainparser_gpu(mx5d5, dev1, fv808)
save -v7.3 logs/mx5d5 mx5d5 rt10d5 r10d5 a10d5

toc;tic;[rt10d6, mx5d6] = trainparser_gpu(mx5d5, trn1, fv808)
toc;tic;[a10d6,~]=model_predict_gpu(dev_x(idx808,:), mx5d6, 1);
numel(find(a10d6 ~= dev_y))/numel(dev_y)
toc;tic;r10d6 = trainparser_gpu(mx5d6, dev1, fv808)
save -v7.3 logs/mx5d6 mx5d6 rt10d6 r10d6 a10d6

toc;tic;[rt10d7, mx5d7] = trainparser_gpu(mx5d6, trn1, fv808)
toc;tic;[a10d7,~]=model_predict_gpu(dev_x(idx808,:), mx5d7, 1);
numel(find(a10d7 ~= dev_y))/numel(dev_y)
toc;tic;r10d7 = trainparser_gpu(mx5d7, dev1, fv808)
save -v7.3 logs/mx5d7 mx5d7 rt10d7 r10d7 a10d7

toc;tic;[rt10d8, mx5d8] = trainparser_gpu(mx5d7, trn1, fv808)
toc;tic;[a10d8,~]=model_predict_gpu(dev_x(idx808,:), mx5d8, 1);
numel(find(a10d8 ~= dev_y))/numel(dev_y)
toc;tic;r10d8 = trainparser_gpu(mx5d8, dev1, fv808)
save -v7.3 logs/mx5d8 mx5d8 rt10d8 r10d8 a10d8

toc;tic;[rt10d9, mx5d9] = trainparser_gpu(mx5d8, trn1, fv808)
toc;tic;[a10d9,~]=model_predict_gpu(dev_x(idx808,:), mx5d9, 1);
numel(find(a10d9 ~= dev_y))/numel(dev_y)
toc;tic;r10d9 = trainparser_gpu(mx5d9, dev1, fv808)
save -v7.3 logs/mx5d9 mx5d9 rt10d9 r10d9 a10d9

toc;tic;[rt10d10, mx5d10] = trainparser_gpu(mx5d9, trn1, fv808)
toc;tic;[a10d10,~]=model_predict_gpu(dev_x(idx808,:), mx5d10, 1);
numel(find(a10d10 ~= dev_y))/numel(dev_y)
toc;tic;r10d10 = trainparser_gpu(mx5d10, dev1, fv808)
save -v7.3 logs/mx5d10 mx5d10 rt10d10 r10d10 a10d10


==> run/run_dynamic_oracle.m <==
toc;tic;[rt10d4, m10d4] = trainparser_gpu(m10d3, trn1, fv804)
toc;tic;[a10d4,~]=model_predict_gpu(dev_x(idx,:), m10d4, 1);
numel(find(a10d4 ~= dev_y))/numel(dev_y)
toc;tic;r10d4 = trainparser_gpu(m10d4, dev1, fv804)
save -v7.3 logs/m10d4 m10d4 rt10d4 r10d4 a10d4

toc;tic;[rt10d5, m10d5] = trainparser_gpu(m10d4, trn1, fv804)
toc;tic;[a10d5,~]=model_predict_gpu(dev_x(idx,:), m10d5, 1);
numel(find(a10d5 ~= dev_y))/numel(dev_y)
toc;tic;r10d5 = trainparser_gpu(m10d5, dev1, fv804)
save -v7.3 logs/m10d5 m10d5 rt10d5 r10d5 a10d5

toc;tic;[rt10d6, m10d6] = trainparser_gpu(m10d5, trn1, fv804)
toc;tic;[a10d6,~]=model_predict_gpu(dev_x(idx,:), m10d6, 1);
numel(find(a10d6 ~= dev_y))/numel(dev_y)
toc;tic;r10d6 = trainparser_gpu(m10d6, dev1, fv804)
save -v7.3 logs/m10d6 m10d6 rt10d6 r10d6 a10d6

toc;tic;[rt10d7, m10d7] = trainparser_gpu(m10d6, trn1, fv804)
toc;tic;[a10d7,~]=model_predict_gpu(dev_x(idx,:), m10d7, 1);
numel(find(a10d7 ~= dev_y))/numel(dev_y)
toc;tic;r10d7 = trainparser_gpu(m10d7, dev1, fv804)
save -v7.3 logs/m10d7 m10d7 rt10d7 r10d7 a10d7

toc;tic;[rt10d8, m10d8] = trainparser_gpu(m10d7, trn1, fv804)
toc;tic;[a10d8,~]=model_predict_gpu(dev_x(idx,:), m10d8, 1);
numel(find(a10d8 ~= dev_y))/numel(dev_y)
toc;tic;r10d8 = trainparser_gpu(m10d8, dev1, fv804)
save -v7.3 logs/m10d8 m10d8 rt10d8 r10d8 a10d8

toc;tic;[rt10d9, m10d9] = trainparser_gpu(m10d8, trn1, fv804)
toc;tic;[a10d9,~]=model_predict_gpu(dev_x(idx,:), m10d9, 1);
numel(find(a10d9 ~= dev_y))/numel(dev_y)
toc;tic;r10d9 = trainparser_gpu(m10d9, dev1, fv804)
save -v7.3 logs/m10d9 m10d9 rt10d9 r10d9 a10d9

toc;tic;[rt10d10, m10d10] = trainparser_gpu(m10d9, trn1, fv804)
toc;tic;[a10d10,~]=model_predict_gpu(dev_x(idx,:), m10d10, 1);
numel(find(a10d10 ~= dev_y))/numel(dev_y)
toc;tic;r10d10 = trainparser_gpu(m10d10, dev1, fv804)
save -v7.3 logs/m10d10 m10d10 rt10d10 r10d10 a10d10



==> run/run_multi_epoch2.m <==
fv808 = [
%n0 s0 s1 n1 n0l1 s0r1 s1r1l s0l1 s0r1l s2
  0 -1 -2  1  0   -1   -2    -1   -1    -3;
  0  0  0  0 -1    1    1    -1    1     0;
  0  0  0  0  0    0   -2     0   -2     0;
];

path('dogma',path);
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
mx0=model_init(@compute_kernel,hp);mx0.batchsize=1000; mx0.step=8;

tic;mx1=k_perceptron_multi_train_gpu(trn1x,trn1y,mx0)
toc;tic;mx1 = compactify(mx1)
toc;tic;[a1,~]=model_predict_gpu(dev1x, mx1, 1);
toc;tic;gtrans1 = numel(find(a1 ~= dev1y))/numel(dev1y)
toc;tic;r1=trainparser_gpu(mx1, dev1, fv808)
toc;tic;save -v7.3 logs/mx1 mx1 a1 r1
toc;

tic;mx2=k_perceptron_multi_train_gpu(trn1x,trn1y,mx1)
toc;tic;mx2 = compactify(mx2)
toc;tic;[a2,~]=model_predict_gpu(dev1x, mx2, 1);
toc;tic;gtrans2 = numel(find(a2 ~= dev1y))/numel(dev1y)
toc;tic;r2=trainparser_gpu(mx2, dev1, fv808)
toc;tic;save -v7.3 logs/mx2 mx2 a2 r2
toc;

tic;mx3=k_perceptron_multi_train_gpu(trn1x,trn1y,mx2)
toc;tic;mx3 = compactify(mx3)
toc;tic;[a3,~]=model_predict_gpu(dev1x, mx3, 1);
toc;tic;gtrans3 = numel(find(a3 ~= dev1y))/numel(dev1y)
toc;tic;r3=trainparser_gpu(mx3, dev1, fv808)
toc;tic;save -v7.3 logs/mx3 mx3 a3 r3
toc;

tic;mx4=k_perceptron_multi_train_gpu(trn1x,trn1y,mx3)
toc;tic;mx4 = compactify(mx4)
toc;tic;[a4,~]=model_predict_gpu(dev1x, mx4, 1);
toc;tic;gtrans4 = numel(find(a4 ~= dev1y))/numel(dev1y)
toc;tic;r4=trainparser_gpu(mx4, dev1, fv808)
toc;tic;save -v7.3 logs/mx4 mx4 a4 r4
toc;

tic;mx5=k_perceptron_multi_train_gpu(trn1x,trn1y,mx4)
toc;tic;mx5 = compactify(mx5)
toc;tic;[a5,~]=model_predict_gpu(dev1x, mx5, 1);
toc;tic;gtrans5 = numel(find(a5 ~= dev1y))/numel(dev1y)
toc;tic;r5=trainparser_gpu(mx5, dev1, fv808)
toc;tic;save -v7.3 logs/mx5 mx5 a5 r5
toc;

tic;mx6=k_perceptron_multi_train_gpu(trn1x,trn1y,mx5)
toc;tic;mx6 = compactify(mx6)
toc;tic;[a6,~]=model_predict_gpu(dev1x, mx6, 1);
toc;tic;gtrans6 = numel(find(a6 ~= dev1y))/numel(dev1y)
toc;tic;r6=trainparser_gpu(mx6, dev1, fv808)
toc;tic;save -v7.3 logs/mx6 mx6 a6 r6
toc;

tic;mx7=k_perceptron_multi_train_gpu(trn1x,trn1y,mx6)
toc;tic;mx7 = compactify(mx7)
toc;tic;[a7,~]=model_predict_gpu(dev1x, mx7, 1);
toc;tic;gtrans7 = numel(find(a7 ~= dev1y))/numel(dev1y)
toc;tic;r7=trainparser_gpu(mx7, dev1, fv808)
toc;tic;save -v7.3 logs/mx7 mx7 a7 r7
toc;

tic;mx8=k_perceptron_multi_train_gpu(trn1x,trn1y,mx7)
toc;tic;mx8 = compactify(mx8)
toc;tic;[a8,~]=model_predict_gpu(dev1x, mx8, 1);
toc;tic;gtrans8 = numel(find(a8 ~= dev1y))/numel(dev1y)
toc;tic;r8=trainparser_gpu(mx8, dev1, fv808)
toc;tic;save -v7.3 logs/mx8 mx8 a8 r8
toc;

tic;mx9=k_perceptron_multi_train_gpu(trn1x,trn1y,mx8)
toc;tic;mx9 = compactify(mx9)
toc;tic;[a9,~]=model_predict_gpu(dev1x, mx9, 1);
toc;tic;gtrans9 = numel(find(a9 ~= dev1y))/numel(dev1y)
toc;tic;r9=trainparser_gpu(mx9, dev1, fv808)
toc;tic;save -v7.3 logs/mx9 mx9 a9 r9
toc;

tic;mx10=k_perceptron_multi_train_gpu(trn1x,trn1y,mx9)
toc;tic;mx10 = compactify(mx10)
toc;tic;[a10,~]=model_predict_gpu(dev1x, mx10, 1);
toc;tic;gtrans10 = numel(find(a10 ~= dev1y))/numel(dev1y)
toc;tic;r10=trainparser_gpu(mx10, dev1, fv808)
toc;tic;save -v7.3 logs/mx10 mx10 a10 r10
toc;

tic;mx11=k_perceptron_multi_train_gpu(trn1x,trn1y,mx10)
toc;tic;mx11 = compactify(mx11)
toc;tic;[a11,~]=model_predict_gpu(dev1x, mx11, 1);
toc;tic;gtrans11 = numel(find(a11 ~= dev1y))/numel(dev1y)
toc;tic;r11=trainparser_gpu(mx11, dev1, fv808)
toc;tic;save -v7.3 logs/mx11 mx11 a11 r11
toc;

tic;mx12=k_perceptron_multi_train_gpu(trn1x,trn1y,mx11)
toc;tic;mx12 = compactify(mx12)
toc;tic;[a12,~]=model_predict_gpu(dev1x, mx12, 1);
toc;tic;gtrans12 = numel(find(a12 ~= dev1y))/numel(dev1y)
toc;tic;r12=trainparser_gpu(mx12, dev1, fv808)
toc;tic;save -v7.3 logs/mx12 mx12 a12 r12
toc;

tic;mx13=k_perceptron_multi_train_gpu(trn1x,trn1y,mx12)
toc;tic;mx13 = compactify(mx13)
toc;tic;[a13,~]=model_predict_gpu(dev1x, mx13, 1);
toc;tic;gtrans13 = numel(find(a13 ~= dev1y))/numel(dev1y)
toc;tic;r13=trainparser_gpu(mx13, dev1, fv808)
toc;tic;save -v7.3 logs/mx13 mx13 a13 r13
toc;

tic;mx14=k_perceptron_multi_train_gpu(trn1x,trn1y,mx13)
toc;tic;mx14 = compactify(mx14)
toc;tic;[a14,~]=model_predict_gpu(dev1x, mx14, 1);
toc;tic;gtrans14 = numel(find(a14 ~= dev1y))/numel(dev1y)
toc;tic;r14=trainparser_gpu(mx14, dev1, fv808)
toc;tic;save -v7.3 logs/mx14 mx14 a14 r14
toc;

tic;mx15=k_perceptron_multi_train_gpu(trn1x,trn1y,mx14)
toc;tic;mx15 = compactify(mx15)
toc;tic;[a15,~]=model_predict_gpu(dev1x, mx15, 1);
toc;tic;gtrans15 = numel(find(a15 ~= dev1y))/numel(dev1y)
toc;tic;r15=trainparser_gpu(mx15, dev1, fv808)
toc;tic;save -v7.3 logs/mx15 mx15 a15 r15
toc;

tic;mx16=k_perceptron_multi_train_gpu(trn1x,trn1y,mx15)
toc;tic;mx16 = compactify(mx16)
toc;tic;[a16,~]=model_predict_gpu(dev1x, mx16, 1);
toc;tic;gtrans16 = numel(find(a16 ~= dev1y))/numel(dev1y)
toc;tic;r16=trainparser_gpu(mx16, dev1, fv808)
toc;tic;save -v7.3 logs/mx16 mx16 a16 r16
toc;

tic;mx17=k_perceptron_multi_train_gpu(trn1x,trn1y,mx16)
toc;tic;mx17 = compactify(mx17)
toc;tic;[a17,~]=model_predict_gpu(dev1x, mx17, 1);
toc;tic;gtrans17 = numel(find(a17 ~= dev1y))/numel(dev1y)
toc;tic;r17=trainparser_gpu(mx17, dev1, fv808)
toc;tic;save -v7.3 logs/mx17 mx17 a17 r17
toc;

tic;mx18=k_perceptron_multi_train_gpu(trn1x,trn1y,mx17)
toc;tic;mx18 = compactify(mx18)
toc;tic;[a18,~]=model_predict_gpu(dev1x, mx18, 1);
toc;tic;gtrans18 = numel(find(a18 ~= dev1y))/numel(dev1y)
toc;tic;r18=trainparser_gpu(mx18, dev1, fv808)
toc;tic;save -v7.3 logs/mx18 mx18 a18 r18
toc;

tic;mx19=k_perceptron_multi_train_gpu(trn1x,trn1y,mx18)
toc;tic;mx19 = compactify(mx19)
toc;tic;[a19,~]=model_predict_gpu(dev1x, mx19, 1);
toc;tic;gtrans19 = numel(find(a19 ~= dev1y))/numel(dev1y)
toc;tic;r19=trainparser_gpu(mx19, dev1, fv808)
toc;tic;save -v7.3 logs/mx19 mx19 a19 r19
toc;

tic;mx20=k_perceptron_multi_train_gpu(trn1x,trn1y,mx19)
toc;tic;mx20 = compactify(mx20)
toc;tic;[a20,~]=model_predict_gpu(dev1x, mx20, 1);
toc;tic;gtrans20 = numel(find(a20 ~= dev1y))/numel(dev1y)
toc;tic;r20=trainparser_gpu(mx20, dev1, fv808)
toc;tic;save -v7.3 logs/mx20 mx20 a20 r20
toc;

==> run/run_multi_epoch.m <==
m10=compactify(m10)

tic;m11=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m10)
toc;tic;m11 = compactify(m11)
toc;tic;[a11,b11]=model_predict_gpu(dev_x(idx,:), m11, 1);
toc;tic;gtrans11 = numel(find(a11 ~= dev_y))/numel(dev_y)
toc;tic;r11=trainparser_gpu(m11, dev1, fv804)
toc;tic;save -v7.3 logs/m11 m11 a11 b11 r11
toc;

tic;m12=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m11)
toc;tic;m12 = compactify(m12)
toc;tic;[a12,b12]=model_predict_gpu(dev_x(idx,:), m12, 1);
toc;tic;gtrans12 = numel(find(a12 ~= dev_y))/numel(dev_y)
toc;tic;r12=trainparser_gpu(m12, dev1, fv804)
toc;tic;save -v7.3 logs/m12 m12 a12 b12 r12
toc;

tic;m13=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m12)
toc;tic;m13 = compactify(m13)
toc;tic;[a13,b13]=model_predict_gpu(dev_x(idx,:), m13, 1);
toc;tic;gtrans13 = numel(find(a13 ~= dev_y))/numel(dev_y)
toc;tic;r13=trainparser_gpu(m13, dev1, fv804)
toc;tic;save -v7.3 logs/m13 m13 a13 b13 r13
toc;

tic;m14=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m13)
toc;tic;m14 = compactify(m14)
toc;tic;[a14,b14]=model_predict_gpu(dev_x(idx,:), m14, 1);
toc;tic;gtrans14 = numel(find(a14 ~= dev_y))/numel(dev_y)
toc;tic;r14=trainparser_gpu(m14, dev1, fv804)
toc;tic;save -v7.3 logs/m14 m14 a14 b14 r14
toc;

tic;m15=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m14)
toc;tic;m15 = compactify(m15)
toc;tic;[a15,b15]=model_predict_gpu(dev_x(idx,:), m15, 1);
toc;tic;gtrans15 = numel(find(a15 ~= dev_y))/numel(dev_y)
toc;tic;r15=trainparser_gpu(m15, dev1, fv804)
toc;tic;save -v7.3 logs/m15 m15 a15 b15 r15
toc;

tic;m16=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m15)
toc;tic;m16 = compactify(m16)
toc;tic;[a16,b16]=model_predict_gpu(dev_x(idx,:), m16, 1);
toc;tic;gtrans16 = numel(find(a16 ~= dev_y))/numel(dev_y)
toc;tic;r16=trainparser_gpu(m16, dev1, fv804)
toc;tic;save -v7.3 logs/m16 m16 a16 b16 r16
toc;

tic;m17=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m16)
toc;tic;m17 = compactify(m17)
toc;tic;[a17,b17]=model_predict_gpu(dev_x(idx,:), m17, 1);
toc;tic;gtrans17 = numel(find(a17 ~= dev_y))/numel(dev_y)
toc;tic;r17=trainparser_gpu(m17, dev1, fv804)
toc;tic;save -v7.3 logs/m17 m17 a17 b17 r17
toc;

tic;m18=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m17)
toc;tic;m18 = compactify(m18)
toc;tic;[a18,b18]=model_predict_gpu(dev_x(idx,:), m18, 1);
toc;tic;gtrans18 = numel(find(a18 ~= dev_y))/numel(dev_y)
toc;tic;r18=trainparser_gpu(m18, dev1, fv804)
toc;tic;save -v7.3 logs/m18 m18 a18 b18 r18
toc;

tic;m19=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m18)
toc;tic;m19 = compactify(m19)
toc;tic;[a19,b19]=model_predict_gpu(dev_x(idx,:), m19, 1);
toc;tic;gtrans19 = numel(find(a19 ~= dev_y))/numel(dev_y)
toc;tic;r19=trainparser_gpu(m19, dev1, fv804)
toc;tic;save -v7.3 logs/m19 m19 a19 b19 r19
toc;

tic;m20=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m19)
toc;tic;m20 = compactify(m20)
toc;tic;[a20,b20]=model_predict_gpu(dev_x(idx,:), m20, 1);
toc;tic;gtrans20 = numel(find(a20 ~= dev_y))/numel(dev_y)
toc;tic;r20=trainparser_gpu(m20, dev1, fv804)
toc;tic;save -v7.3 logs/m20 m20 a20 b20 r20
toc;


2014-07-26  Deniz Yuret  <dyuret@ku.edu.tr>

	* k_perceptron_multi_train_gpu.m: check to see if during
	multi-epoch we are using cpu!  compactify during multi epoch!
	run_multi_epoch.m figure out the memory problem with k_train in
	run_embeddings!  We probably were not using gpu in multi_epoch
	before and now it blows up around 186K sv.  So all speeds in
	run_multi_epoch are wrong, all nsv's are wrong because of
	redundancy but the accuracy results should be correct.

	After fixing the bug, multi-epoch became ten times faster and I
	confirmed the same results for m1->m2 (at least up to the point
	where batchsize was reduced to fit memory).  This, combined with
	compactify should allow us to run one epoch in < 20 mins.

==> run/gpu_train_script.m <==
gpuDevice(1);
m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;
tic();m1=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m0);toc();
tic();m2=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m1);toc();
tic();m3=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m2);toc();
tic();m4=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m3);toc();
tic();m5=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m4);toc();


2014-07-25  Deniz Yuret  <dyuret@ku.edu.tr>

	* model_sparsify.m: Added unique to sort(model.S): in multi-epoch
	models, same index gets added multiple times.  Since we are not
	shuffling they should still be correct.  For example m9 has 201060
	unique SVs out of a 503651 total.  (DONE: take advantage of this
	for multi-epoch training).  Running with m9.  In fact with dynamic
	oracle things are even more confusing, the data we see depends on
	the moves.  So either go to the S+X convention and do not have SV,
	or (better) forget about keeping the original SV and just pick
	randomly (DONE).

	time	iter	nsv	err_tr	err_te	maxdiff	note
	0	0	503651	24373	2475	0	m9 (201060 uniq sv)
	72721	754468	79425	28008	2535	3.37138e+06 m9 sparsified

	model	nsv	aer	time	gtrans	ptrans	phead	notes
	m9	503651	3.33	19897	3.41	3.79	 9.99   201060 uniqSV
	m9s	79425	--	72721	3.49	3.94	10.35	i00

	Conclusion: it is possible to reduce nsv from 200K to 73K with
	some performance hit.  However this does not seem to be worth
	doing right now, given that the number of unique SV stays around
	200K we can fit everything on gpu.  Also the time cost seems a
	lot, should look at that if we ever decide to use this again.


	* trainparser_gpu.m: Adding the weight updates.  Appending a
	single SV is 25ms regardless of which way we turn the matrix.
	Unexpectedly overwriting to existing space takes about the same.
	We'll leave it be for now.  However training would be much faster
	if we could eliminate the redundancy in SVs caused by multi-epoch.
	So I wrote compactify.m to eliminate redundant SV's and add their
	weights.  Testing on m9 before applying dynamic oracle.

	>> m9c = compactify(m9)
	>> trainparser_gpu(m9c, dev1, fv804)
	Elapsed time is 861.776661 seconds.
	Same answer as below.


	* trainparser_gpu.m: rewriting for both test/train.
	>> load m1
	>> dev1=loadCoNLL('embedded/conllWSJToken_wikipedia2MUNK-50/01/wsj_0101.dp');
	>> newFeatureVectors  % defines fv804
	>> r1 = trainparser_gpu(m1, dev1, fv804, 0)
	Elapsed time is 530.643937 seconds.
	npct: 0.1175
	xpct: 0.0447
	wtot: 34681
	werr: 3473
	wpct: 0.0866

	Nonpunct number is wrong, should be 35324.  OK, looked at the
	difference, it is catching 351x$ (which is wrong), and the 292x`
	character (which is right).  I guess eval.pl has a bug with the
	backtick!  Looking at the P deprels like parser.py gets it mostly
	right, except misses one comma, does not take % as punct (which
	sounds right like $)  Anyway I fixed the code so it gives the same
	nonpunct count as eval.pl for now.

	>> r9 = trainparser_gpu(m9, dev1, fv804, 0)
	Elapsed time is 2215.061447 seconds.
	ntot: 40117
	nerr: 4007
	npct: 0.0999
	xtot: 76834
	xerr: 2917
	xpct: 0.0380
	wtot: 35324
	werr: 3089
	wpct: 0.0770 (92.3)

	* DONE:
	- Try smaller word vectors, type based word vectors, other embeddings.
	oo Try /ai/home/vcirik/eParse/run/embedded/conllWSJToken_rcv1UNK50  -- should be easy 1-epoch
	most parsers do better with it.  can do this without waiting for featselect...
	oo or we can make featureindices part of ArcHybrid, that way code should work!
	-- need to finish feature optimization and fix features before
	experimenting with embeddings!
	-- run_embeddings.m: Try all embeddings under
	/mnt/ai/home/vcirik/eParse/run/embedded
	problem is different sized embeddings have different indices so
	idx804 does not work.  better wait until featselection is over and
	use ArcHybrid that just produces the necessary features.

	* features.m: new feature script.  verified the same output as old
	features.  found a bug in old features (variable d overwritten),
	which may have caused distances not being selected.  should try
	again.  (actually no need, this happened at the end of the
	features fn and had no negative effect).  also should try
	different encoding of numbers.  however new features.m twice as
	slow.  no obvious improvement.  presumably shorter feature sets
	will be faster.  hardcoding features will speed up future code.

	>> newFeatureVectors % defines the usual features in new format
	>> [dev1_x, dev1_y] = dumpfeatures2(dev1(1:100), fv804);
	Elapsed time is 5.990231 seconds.
	>> [dev_x, dev_y] = dumpfeatures(dev_w(1:100), dev_h(1:100));
	Elapsed time is 7.681979 seconds.
	>> dev_x2=dev_x(idx804,:);
	>> all(dev_x2(:)==dev1_x(:)) => 1

	Addressed the following, archybrid no longer has the sentence or
	the features: -- make parser and features, embedding dim etc part
	of model or vice versa.  think about software engineering.  conll
	fields.  embeddings.  features.  arceager. forget dogma.

	* loadCoNLL.m: modified to load the whole information.

==> run/run_batchsize.m <==
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
m0 = model_init(@compute_kernel, hp);
m0.step = 10;
for bs=500:500:3000
  batchsize = bs
  m0.batchsize = bs;
  gpuDevice(1);
  tic();m1 = k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m0); ...
             toc();
end


2014-07-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	- make sure testparser is efficient.
	-- convert into trainer (like primal case)
	- try dynamic oracle / sparsification on top of multi-epoch.
	-- we can do dynamic oracle by running the parser and collecting data
	-- is running the parser faster?  try on small sets.  we need to speed up dyn oracle.
	-- 38wps seems to be max

	* DONE:
	- Need performance numbers excluding punctuation on section 23.
	oo change loadCoNLL.m to load all fields and testparser to report punc and non-punc scores.
	-- or testparser can output conll format to be fed into eval.pl.
	-- wsj22 has 40117 tokens 35324 non-punc according to all versions of eval.pl.
	-- Honnibal says: I then converted the gold-standard trees from WSJ 22, for the evaluation. Accuracy scores refer to unlabelled attachment score (i.e. the head index) of all non-punctuation tokens.
	-- Nivre says: The numbers are excluding punctuation on the standard split (2-21, 23).
	-- Husnu says: Asagidaki wordler icin parent prediction basarimi hesaba katilmiyor.
	if (strcmp(w->postag, ",") != 0 && strcmp(w->postag, ":") != 0 && strcmp(w->postag, ".") != 0 && strcmp(w->postag, "``") != 0 && strcmp(w->postag, "''") != 0)
	-- eval.pl uses: scalar(Encode::decode_utf8($word)=~ /^\p{Punctuation}+$/).  wsj22 has 35324 non-punc.
	-- parser.py uses: deplabel P or punct.  wsj22 has 4728 P, leaving 35389 non-punc.

	* DONE:
	- bigger models suffer more from copying. pre-allocate in perceptron_train.
	-- daume trick for beta2 in perceptron_train.
	-- are we using singletons to train or not?
	-- transpose cost anything in perceptron_train?

	* DONE:
	x gpu multiply how can it be so slow?  how can it improve our
	other code?  (e.g. model_predict, sparsify etc)?
	x New version of primal (with turan) should get same accuracy. it doesn't!
	r0=trainparser_primal_12(dev_w(1:50),dev_h(1:50),idx708,w0,0);
	% s=50 w=1175 we=0.4647 m=2250 me=0.1720 wps=10.20 Elapsed time is 115.170739 seconds.
	r1=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w1,0);
	% s=50 w=1175 we=0.4247 m=2250 me=0.1631 wps=14.33 Elapsed time is 82.015995 seconds.
	x Also compare and confirm match of primal with dual model.
	x Maybe some more feat optimization with dynamic oracle.
	x Do a kernel x feature exhaustive experiment.
	x Try second degree primal?
	x Adding dep labels help?  Only .0007 according to ZN11.

	* DONE:
	x try other algorithms that do better in one epoch (pegasos, pa) after converting to minibatch. needs code.
	-- Effect of learning algorithm: pegasos, pa etc. on perf, feats.
	-- Could actually try on 5k before converting to minibatch.  running...
	-- Figure out the right way to run pegasos, make it multiclass...
	-- Figure out opt params for pa, make it gpu-minibatch...
	-- Just try pa for one epoch: not impressed...

	* multi-epoch:
	>> m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;
	>> tic();m1=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m0);toc();
	Elapsed time is 321.474055 seconds.
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m1, 1); toc();
	Elapsed time is 26.001716 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0401  % avg model
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m1, 0); toc();
	Elapsed time is 26.334112 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0482  % last model
	>> save m1.mat m1
	>> tic();m2=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m1);toc();
	Elapsed time is 6105.521107 seconds.
	>> save m2.mat m2
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m2, 1); toc();
	Elapsed time is 44.278723 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0364
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m2, 0); toc();
	Elapsed time is 42.569236 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0524
	>> tic(); r1=testparser(m1, dev_w, dev_h, idx); toc();
	Elapsed time is 2214.591667 seconds.
	>> r1
	ntot: 40117
	nerr: 4715  (0.1175)
	xtot: 76834
	xerr: 3433  (0.0447)
	>> tic(); r2=testparser(m2, dev_w, dev_h, idx); toc();
	Elapsed time is 3468.987827 seconds.
	>> r2
	ntot: 40117
	nerr: 4333  (0.1080)
	xtot: 76834
	xerr: 3142  (0.0409)
	>> tic(); r3=testparser(m3, dev_w, dev_h, idx); toc();r3
	Elapsed time is 4621.268330 seconds.
	>> r3
	ntot: 40117
	nerr: 4161  (0.1037)
	xtot: 76834
	xerr: 3046  (0.0399)


	* testparser_gpu.m: Here is the wps for various things for a model
	with 250K SV (model_trn01):

	dumpfeatures: 313wps -- does not use a model, just gold parser moves
	testparser: 12wps
	testparser_par: 24wps
	testparser_gpu: 8wps -- improved to 38wps by the end of this note
	model_predict_gpu: 700wps -- advantage of minibatch processing

	Dynamic oracle requires at least testparser cost.  Parallelization
	is limited because the next state is determined by the last move.
	If we keep the model fixed we could use kernel caching.  It would
	be fast to cache gold contexts and moves.  Add new elements to
	cache when we stray from gold path for future epochs.  But model
	updated and cache useless before the next epoch.  Need a hash
	table with several million context/score entries.

	Is gpu useless even when SV matrix is large?  Maybe try dot
	instead of mtimes?  Let's profile testparser_gpu.  Testing with
	model_trn with SV:804x101517.

	Is bsxfun better than mtimes for vector x matrix?  Not so, in fact
	5x slower.

	>> s=gpuArray(rand(804,100000));
	>> x=gpuArray(rand(804,1));
	>> str=s';
	>> tic;for i=1:1000 k2=sum(bsxfun(@times,s,x));wait(gpuDevice);end; toc;
	Elapsed time is 20.198047 seconds.
	>> tic;for i=1:1000 k1=str*x;wait(gpuDevice);end; toc;
	Elapsed time is 4.740397 seconds.
	>> max(abs(k1(:)-k2(:)))
	3.9790e-13

	But both at least 5x faster than the cpu:
	>> s=rand(804,100000);
	>> x=rand(804,1);
	>> str=s';
	>> tic;for i=1:1000 k1=str*x;end; toc;
	Elapsed time is 23.783135 seconds.
	>> tic;for i=1:10 k2=sum(bsxfun(@times,s,x));end; toc;
	Elapsed time is 3.486507 seconds.

	How about the skinny beta * K_x operation?  It seems like the best
	option is to use bsxfun+sum on the gpu.  Faster than mtimes or dot.

	>> b=rand(3,100000);
	>> c=b';
	>> k=rand(100000,1);
	>> bgpu=gpuArray(b);
	>> cgpu=gpuArray(b);
	>> kgpu=gpuArray(k);
	>> tic;for i=1:10000 f1=b*k;end;toc
	Elapsed time is 3.429669 seconds.
	>> tic;for i=1:10000 f2=sum(bsxfun(@times,c,k));end;toc
	Elapsed time is 24.121618 seconds.
	>> max(abs(f1(:)-f2(:)))
	1.9281e-10
	>> tic;for i=1:10000 f1gpu=bgpu*kgpu;end;wait(gpuDevice);toc
	Elapsed time is 84.003939 seconds.
	>> max(abs(f1(:)-f1gpu(:)))
	1.8190e-10
	>> tic;for i=1:10000 f2gpu=sum(bsxfun(@times,cgpu,kgpu));end;wait(gpuDevice);toc
	Elapsed time is 4.584367 seconds.
	>> max(abs(f1(:)-f2gpu(:)))
	1.8190e-10
	>> tic;for i=1:10000 f1x=gather(bgpu)*gather(kgpu);end;toc
	Elapsed time is 27.313679 seconds.
	>> tic;for i=1:10000 for j=1:3 f1gpu(j)=dot(cgpu(:,j),kgpu);end;end;wait(gpuDevice);toc
	Elapsed time is 10.073763 seconds.
	>> max(abs(f1gpu(:)-f1(:)))
	1.8554e-10

	Applying the bsxfun trick to testparser_gpu makes a difference.
	On dev(1:100), 2449 words, the speed goes from 8wps to 38wps.
	Regular testparser is 12wps, parfor version with 12 workers is
	24wps.  Can't use parfor and gpu together.  So 38wps is as fast as
	it gets for dynamic-oracle or any problem where we have to
	evaluate contexts one by one.  This is 25000 secs (7 hrs) for the
	ptb (for a 250K SV model).

	Before:
	>> tic(); r=testparser_gpu(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 295.032944 seconds.

	After:
	>> tic(); r=testparser_gpu(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 64.945317 seconds.
	>> tic(); r=testparser_par(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 129.508759 seconds.
	>> tic(); r=testparser(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 290.181428 seconds.


	* k_pa_multi_train: run for one epoch unoptimized.

	>> m0=model_init(@compute_kernel,hp);
	>> tic();m1pa=k_pa_multi_train(trn_x(idx,:),trn_y,m0);toc();
	#1721 SV:16.33(281074)	AER: 5.01
	Elapsed time is 87270.739688 seconds.

	This is way too slow but comparable to m3 in terms of aer and nsv.

	PA models have sparse betas for some reason?
	>> issparse(m1pa.beta2) => 1
	>> m1pa.beta = full(m1pa.beta);
	>> m1pa.beta2 = full(m1pa.beta2);
	>> save m1pa.mat m1pa

	It does not generalize as well as m2 (2 epochs of avg perceptron):

	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m1pa, 1); toc();
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0370  % for avg model, better than m1 but worse than m2
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m1pa, 1); toc();
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0470  % for last model

	Conclusion: maybe worth trying to optimize parameters etc on a
	smaller dataset but results are not promising.



2014-07-23  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_idx708.m: try optimized feats for one epoch and compare to
	idx804.  Does worse.  I guess we need to wait until full corpus
	optimization is done.  $$i00.

	idx804 = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	idx708=featureindices({'n0','n0l1','n0l2','n1','s0','s0r','s0r1','s0s1','s1'});
	>> m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;
	>> tic();m1=k_perceptron_multi_train_gpu(trn_x(idx708,:),trn_y,m0);toc();
	Elapsed time is 314.146673 seconds. SV:708x118690
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx708,:), m1, 1); toc();
	Elapsed time is 26.159195 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0424  % avg model - compare to 0.0401 with idx804
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx708,:), m1, 0); toc();
	Elapsed time is 26.030733 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0509  % last model - compare to 0.0482 with idx804
	>> tic(); r1=testparser(m1, dev_w, dev_h, idx708); toc();
	Elapsed time is 2074.158120 seconds.
	>> r1
	ntot: 40117
	nerr: 4949  (0.1234) - compare to 0.1175 for idx804
	xtot: 76834
	xerr: 3585  (0.0467) - compare to 0.0447 for idx804


	* parser.py: compare parser.py at one epoch.  OK, first parser.py
	counts as punctuation any token with DEPREL=P and excludes them
	from scoring, reporting 32025/35389 correct for wsj22 after 15
	epochs of dynamic-oracle training.  It is difficult to compare
	this with my results for multiple reasons: excluding punctuation
	-- getting the parser_with_punct.py results with all 40117 tokens
	now.  The way punct is excluded is nonstandard, eval.pl uses
	characters in the word and gets 35324 non-punc tokens.  Dynamic
	oracle means to ramp up is pretty slow during the first few epochs
	(compare model00 with model5k).  The initial results are on the
	training set and do not include weight averaging, only the final
	result is comparable.  The initial results do not exclude
	punctuation.  When punctuation is included the final result of
	parser.py falls 1.5% to 88.9!  So our static parser is already way
	ahead.

	altay:parser[09:25:36(0)]$ make parser_py_model
	awk '{if(NF==0){print $0}else{print $2, $4, $7-1, $8}}' wsj_0001.dp > parser_py_train
	cut -f2,4 wsj_0101.dp | perl -pe 'if(/\S/){s/\t/\//;s/\n/ /;}' > parser_py_test
	awk '{if(NF==0){print $0}else{print $2, $4, $7-1, $8}}' wsj_0101.dp > parser_py_gold
	python parser.py parser_py_model parser_py_train parser_py_test parser_py_gold
	0 0.758
	1 0.816
	2 0.836
	3 0.849
	4 0.860
	5 0.874
	6 0.880
	7 0.886
	8 0.891
	9 0.894
	10 0.896
	11 0.900
	12 0.902
	13 0.904
	14 0.906
	Averaging weights
	Saving model to parser.pickle
	Parsing took 21271.548 ms
	32025 35389 0.904942213682
	altay:parser[12:34:46(11347)]$

	balina:parser[18:52:08(0)]$ python parser_with_punct.py parser_py_model parser_py_train parser_py_test parser_py_gold
	0 0.757
	1 0.815
	2 0.836
	3 0.850
	4 0.859
	5 0.874
	6 0.881
	7 0.886
	8 0.890
	9 0.894
	10 0.897
	11 0.900
	12 0.903
	13 0.905
	14 0.906
	Averaging weights
	Saving model to parser.pickle
	Parsing took 21752.239 ms
	35675 40117 0.889273873919
	balina:parser[21:41:53(10163)]$


==> run/run_idx708.m <==
m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;
tic();m1=k_perceptron_multi_train_gpu(trn_x(idx708,:),trn_y,m0);toc();
show_m1=m1
tic(); [a,b]=model_predict_gpu(dev_x(idx708,:), m1, 1); toc();
numel(find(a ~= dev_y))/numel(dev_y)
tic(); [a,b]=model_predict_gpu(dev_x(idx708,:), m1, 0); toc();
numel(find(a ~= dev_y))/numel(dev_y)
tic(); r1=testparser(m1, dev_w, dev_h, idx708); toc();
show_r1=r1
r1.nerr/r1.ntot
r1.xerr/r1.xtot


2014-07-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* k_perceptron_multi_train_gpu.m: To optimize featselect we need
	to optimize train: look into mini-batch perceptron training so we
	can use gpu: http://acl.cs.qc.edu/~lhuang/papers/minibatch.pdf

	Implementing minibatch training.  Kernel calculation takes 8.5ms
	per 500 batch, 17us per instance.  This is 10x faster than
	yesterdays results, because we are starting with an empty model,
	yesterday we were experimenting with 100K sv.  Updates are done in
	a 500 for loop which is expensive (2921ms).  Vectorized updates
	reduce the whole operation (kernel plus update) to 27ms per 500
	minibatch.  This means 20K instances/sec, 86 seconds for the whole
	ptb?  Well it slows down as we collect more SV and one epoch takes
	432 secs at a slight accuracy cost:

	old:
	>> model_trn = k_perceptron_multi_train(trn_x(idx,:), trn_y, model_trn);
	#1720 SV: 5.90(101473)	AER: 5.90
	Elapsed time is 15841.799743 seconds.
        SV: [804x101517 double]
        pred: [3x1720842 double]

	new:
	m=model_init(@compute_kernel,hp);m.batchsize=500; m.step=10;
	tic();m=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m);toc();
	#1720000 SV: 6.13(105351)	AER: 6.13	t=431.945

	* done: Implemented beta2 ignoring age differences within one batch.

	* done: Fastest batch size seems to be 1250.  Training with 100K:
	batch	time
	250	4.65
	500	3.56
	750	3.22
	1000	3.18
	1250	3.14
	1500	3.18
	1750	3.26
	2000	3.36

	* run_learners_5k.m: compare learners on 5k data.  I think I did
	5k instances instead of 5k sentences.  Rerunning on 5k sentences.
	SVM has an advantage that gets smaller as data gets larger and it
	is too slow.  PA has an advantage on the first epoch but later
	results show that second epoch of avg perceptron beats PA.

	5k instance results
	last	avg	nsv	time	algo
	16.86	14.17	941	190.49	k_perceptron_multi_train
	13.26	12.58	3201	260.74	k_pa_multi_train
	16.86	14.17	941	186.68	k_projectron2_multi_train
	0.00	11.26	2930	21.44	svmtrain

	5k sentence results
	last	avg	nsv	time	algo
	7.01	5.35	18164	1127.07	k_perceptron_multi_train
	5.97	5.17	55428	2657.21	k_pa_multi_train
	7.01	5.35	18145	30339.92 k_projectron2_multi_train
	0.00	5.09	42035	108601.55	svmtrain

	>> save m4svm.mat m4  % ilac-2-2

	old 5k results for comparison
	model	nsv	gtrans	ptrans	phead	time	notes
	mo5k	17318	5.43	5.78	15.14	1939	one epoch static, from scratch
	mo01	40502	5.56	5.36	14.13	6489	static+dynamic, from model5k
	mo02	58979	5.74	5.31	13.79	10679	static+2*dynamic, from model01

	* run_batchsize.m: look at differences in final performance as a
	fn of batchsize.

	batch	nsv	aer	time
	1	101517	5.90	15481.80
	100	102869	5.98	945.91
	500	105390	6.12	401.66
	1000	109144	6.34	335.52
	1500	113888	6.61	325.42
	2000	118131	6.86	336.41
	2500	out-of-memory


==> run/run_learners_5k.m <==
x_tr = trn_x(idx, 1:217603);
y_tr = trn_y(1:217603);
x_te = dev_x(idx, :);
y_te = dev_y;
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3)
m0 = model_init(@compute_kernel,hp);
fprintf('last\tavg\tnsv\ttime\talgo\n');

tic();
m1 = k_perceptron_multi_train(x_tr, y_tr, m0)
m1ttm = toc();
m1nsv = size(m1.beta, 2);
m1lst = model_predict(x_te, m1, 0);
m1avg = model_predict(x_te, m1, 1);
m1lst_err = numel(find(m1lst ~= y_te))/numel(y_te)*100;
m1avg_err = numel(find(m1avg ~= y_te))/numel(y_te)*100;
fprintf('%.2f\t%.2f\t%d\t%.2f\tk_perceptron_multi_train\n', ...
        m1lst_err, m1avg_err, m1nsv, m1ttm);

tic();
m2 = k_pa_multi_train(x_tr, y_tr, m0)
m2ttm = toc();
m2nsv = size(m2.beta, 2);
m2lst = model_predict(x_te, m2, 0);
m2avg = model_predict(x_te, m2, 1);
m2lst_err = numel(find(m2lst ~= y_te))/numel(y_te)*100;
m2avg_err = numel(find(m2avg ~= y_te))/numel(y_te)*100;
fprintf('%.2f\t%.2f\t%d\t%.2f\tk_pa_multi_train\n', ...
        m2lst_err, m2avg_err, m2nsv, m2ttm);

tic();
m3 = k_projectron2_multi_train(x_tr, y_tr, m0)
m3ttm = toc();
m3nsv = size(m3.beta, 2);
m3lst = model_predict(x_te, m3, 0);
m3avg = model_predict(x_te, m3, 1);
m3lst_err = numel(find(m3lst ~= y_te))/numel(y_te)*100;
m3avg_err = numel(find(m3avg ~= y_te))/numel(y_te)*100;
fprintf('%.2f\t%.2f\t%d\t%.2f\tk_projectron2_multi_train\n', ...
        m3lst_err, m3avg_err, m3nsv, m3ttm);

path('libsvm-3.18/matlab', path);
tic();
m4 = svmtrain(y_tr', x_tr', '-t 1 -d 3 -g 1 -r 1 -c 1')
m4ttm = toc();
m4nsv = m4.totalSV;
[m4lst, m4acc, m4val] = svmpredict(y_te', x_te', m4);
m4lst_err = numel(find(m4lst ~= y_te'))/numel(y_te)*100;
fprintf('%.2f\t%.2f\t%d\t%.2f\tsvmtrain\n', ...
        m4lst_err, 0, m4nsv, m4ttm);


2014-07-21  Deniz Yuret  <dyuret@ku.edu.tr>

	* trainparser_gpu.m: Profiling trainparser to find the ideal
	minibatch size.  When computing kernels for single x's cpu and gpu
	have the same speed (18ms for SV=804x101517).  The gpu advantage
	comes out as we process multiple x's:

	xcnt	cpuμs/x	gpuμs/x
	1	16479	16606
	10	4152	3549
	100	2037	427
	250		301
	500		272
	1000	1752	262
	2500	1683	271

	Seems like we hit max performance around 500 x's and memory
	overflows at 2500 x's.  This should speed up static oracle
	training 60x!  Make 500 decisions at once and compare with static
	oracle, apply all updates together.  However dynamic oracle
	requires making decisions sequentially so this won't help us...
	Still, one ptb epoch in 5 mins makes multi-epoch experiments
	easier.

	* model_sparsify_profile.m: Profiling version.  margins is taking
	most of the time, more than twice that of pred_tr update.
	specifically the max(pred_tr) in margins.  It turns out gpu max
	takes 134ms where cpu max on the same data takes 2.5ms.  MAX IS
	EXTREMELY SLOW ON THE GPU.  So changing max(f) with max(gather(f))
	makes the program roughly 2.5x as fast.  This should reduce full
	ptb time from 9 hours to 3.6.

	* model_sparsify_popt.m: Parameter optimization version.  The
	errors are reported at 1000 SV.  Seems same point is (epsilon=0.1,
	margin=1.0, eta=0.3) still efficient.

	epsilon	margin	eta	time	iter	err_tr	err_te
	0.1	1.0	0.3	1590	8392	138754	6827
	0.2	1.0	0.3	1386	7310	138983	6970
	0.05	1.0	0.3	1625	8580	144475	7328
	0.1	2.0	0.3	1590	8392	138752	6827  ?? why so close
	0.1	0.5	0.3	1590	8392	138752	6827  ?? why so close
	0.1	1.0	0.6	898	4704	152362	7786
	0.1	1.0	0.15	2816	14943	145843	7473

2014-07-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* model_sparsify.m: New version uses singles and subsamples the
	data to fit the gpu.  Running with trn_x(idx804,:) Subsampling
	x_tr from 1720842 to 1411972 instances to fit in gpu.  It
	eventually reaches a better point but with more time and more SV.
	Should optimize the parameters on the big set.  Here are the
	results in comparison with the earlier 433k training set results.
	(model_sparsify_0718.log vs model_sparsify_0721.log) The 433K is
	better up to 18000 SVs and about 3x faster (which is linear with
	the size increase in the training set).  Starting parameter
	optimization at 1000 SV experiment (model_sparsify_popt).

	433K results			1.4M results
	nsv	time	iter	err_tr	err_te	time	iter	err_tr	err_te
	101517	0	0	13044	2740	0	0	47820	2740
	1000	610	8368	36997	6464	1590	8392	138754	6827
	2000	1178	16151	27445	4909	3068	16180	104949	5206
	3000	1738	23831	23736	4250	4490	23660	92833	4616
	4000	2260	30972	21570	3948	5876	30951	83229	4098
	5000	2730	37405	20138	3733	7240	38118	78250	3913
	6000	3202	43857	18654	3533	8490	44685	73853	3763
	7000	3621	49586	17835	3452	9733	51209	71192	3656
	8000	4017	54994	17249	3405	10940	57538	68933	3490
	9000	4409	60333	16495	3303	12120	63723	66933	3454
	10000	4766	65200	15888	3275	13256	69666	64980	3355
	11000	5098	69724	15400	3228	14301	75133	63546	3324
	12000	5406	73920	14930	3179	15303	80368	62171	3284
	13000	5703	77945	14552	3138	16357	85883	61033	3264
	14000	5969	81558	14169	3104	17321	90918	59697	3183
	15000	6212	84843	13877	3100	18290	95978	58643	3136
	16000	6432	87807	13601	3058	19215	100810	57913	3105
	17000	6623	90380	13348	3042	20104	105449	57129	3066
	18000	6794	92682	13192	3037	20946	109840	56270	3036
	19000	6944	94680	13002	3044	21781	114189	55358	3008
	20000	7075	96424	12885	3028	22579	118348	54607	3004
	21000	7197	98028	12764	3018	23350	122359	54211	2957
	22000	7311	99540	12686	3007	24076	126138	53686	2936
	22340	7349	100038	12641	2998
	23000			(2.9%)	(4.1%)	24777	129780	53187	2944
	40614					32470	169251	48144	2808
								(3.4%)	(3.9%)

2014-07-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* model_sparsify.m: Look into sparsification with full data.
	>> m0s=model_trn;
	>> m0s.step=10;
	>> idx804 = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	>> p=struct('average',1,'x_te',dev_x(idx,:),'y_te',dev_y,'eta',0.3,'epsilon',0.1,'margin',1.0);
	% edit model_sparsify to turn off gpu completely...
	>> m0sc = model_sparsify(m0s,trn_x(idx,:),trn_y,p);
	% takes too long
	% increase max_num_el to 1e10 (80GB)
	% still takes too long
	% try turning gpu back on but not pushing x_tr
	% It becomes 2x slower than the cpu version!
	% We have to keep x in gpu :(
	% Or just use the cpu at one sv every 5 seconds!
	% Use singles and automatically subsample x_tr.

	* featselect.m: Now reading cache.
	[bestf, besterr, cache] = featselect(x5k(:,10001:end),y5k(10001:end),x5k(:,1:10000),y5k(1:10000),3,'n0,s0,s1',featselect_03_5k)

	* featselect-cache.m,featselect-cache.pl,featselect_03_5k.mat:
	Convert text output to cache file.

2014-07-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* timing: To get model_trn with static oracle from 950028 words we
	spent 15841 seconds (60wps=4.4h/ptb) going from 0 to 100K SVs.
	Dynamic training the next epoch, model_trn01, took 52817 secs
	(18wps=14.7h/ptb) going from 100K to 250K SVs.  trainparser_primal
	goes at a constant speed (13wps=20.3h/ptb).  In dual models, the
	time cost of static and dynamic are roughly the same, the number
	of SVs determine the runtime (see m5s-vs-m5d below).
	Sparsification seems very fast (129wps=2h/ptb) but note that we
	were not able to use the full trn (1720842 move instances) with
	the gpu, we had to subsample.  parser.py finishes 15 epochs at 100
	words/sec (i.e. goes at 1500 words/sec?)!

	full trn experiments:
	model		nsv	gtrans	ptrans	phead	time	notes
	model_trn	101517	3.78	4.25	11.02	15841	fulltrain+static, from scratch
	model_trn_sp	22340	4.13	4.59	11.92	7349	fulltrain+static+sparsify, from trn
	model_trn01	251480	3.89	3.94	10.15	52817	fulltrain+static+dynamic, from trn
	model_trn_sp01	186554	4.14	4.22	10.87	46844	fulltrain+static+sparsify+dynamic, from trn_sp

	5k experiments:
	model		nsv	gtrans	ptrans	phead	time	notes
	model00		29187	9.21	6.78	17.56	?	one epoch dynamic, from scratch
	model5k		17318	5.43	5.78	15.14	1939	one epoch static, from scratch
	model01		40502	5.56	5.36	14.13	6489	static+dynamic, from model5k
	model02		58979	5.74	5.31	13.79	10679	static+2*dynamic, from model01
	m6		12555	5.34	5.72	15.04	9219	static+sparsify, from model5k
	model03b	36073	5.47	5.46	14.18	5467	static+sparsify+dynamic, from m6
	model04		13043	5.35	5.59	14.47	2157	static+sparsify+dynamic+sparsify, from model03b w/gpu?

	* m5s-vs-m5d: m5s takes 593 secs on ilac-0-0 but gets dumpfeatures
	for free which is 358 secs, m5d takes 1251 secs doing its own
	feature extraction.  The small difference is due to the larger
	number of SVs m5d accumulates (17318 vs 27954).

	m0=model_init(@compute_kernel,hp);
	tic(); m5s=k_perceptron_multi_train(x_tr,y_tr,m0); toc();
	% Elapsed time is 592.851920 seconds.
	tic();size(dumpfeatures(trn_w(210:5000),trn_h(210:5000)));toc();
	% Elapsed time is 357.939658 seconds.
	tic(); m5d = trainparser(m0, trn_w(210:5000), trn_h(210:5000), idx); toc();
	% Elapsed time is 1251.707183 seconds.

	* naming: Better naming convention for models:
	1. 'm' is the first char
	2. [0-9]+ indicating the training set size in 1000 sentences
	   use 0 to indicate the full ptb training set.
	3. Followed by a sequence of letters indicating training steps:
	   's' for static oracle (perceptron)
	   'd' for dynamic oracle (trainparser)
	   'c' for compression (sparsify)

	* x5k: we got x5k from the first 5000 sentences of trn.  in the 5k
	experiments we have been using the first 10k features as test and
	the remaining 207603 as training set.  How many sentences does
	this correspond to, so we can run corresponding experiments with
	trainparser?  It turns out the answer is the 209 sentences give
	the first 10013 instances.  So we can use trn_w(1:209) as test and
	trn_w(210:5000) as train.

	* trainparser_primal.m: Now we can also do averaging at a cost of
	25% speed.  At this speed 1 epoch of ptb (950028 words) will take
	18 hours.  Testing on dev (40117 words) is 45 mins.

	% This time with w2 calculation.
	>> [w,w2]=trainparser_primal(trn_w(1:10),trn_h(1:10),idx708);
	% s=10 w=189 wps=14.6609 Elapsed time is 12.891465 seconds.

	% Run old/new versions on (1:50) to confirm:
	[w1,w1b]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	% s=50 w=1259 wps=13.2452 Elapsed time is 95.053105 seconds.
	[w0,w0b]=trainparser_primal_12(trn_w(1:50),trn_h(1:50),idx708);
	% 50 Elapsed time is 136.959744 seconds.  w0b empty.

	% Averaged version should get better accuracy.
	r1b=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w1b,0);
	% s=50 w=1175 we=0.4085 m=2250 me=0.1551 wps=14.32 Elapsed time is 82.079185 seconds.

	% Check the averaging calculation by comparing with slow version.
	[nv,v,v2,v2b]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	% s=50 w=1259 we=0.4845 m=2418 me=0.1729 wps=8.04 Elapsed time is 156.651738 seconds.
	all(w1(:)==v(:)) => 1
	all(w1b(:)==v2(:)) => 1
	all(v2(:)==v2b(:)) => 0
	max(abs(v2(:)-v2b(:))) => 0.0029  % we have rounding errors
	% Daume's trick could be numerically less stable :(

	% Try adding 1 as x0.  Definitely improves the averaged results.
	[n2,w2,w2b]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	% s=50 w=1259 we=0.4599 m=2418 me=0.1625 wps=12.12 Elapsed time is 103.896044 seconds.
	% n2.f=709 -- used to be 708
	r2=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w2,0);
	% s=50 w=1175 we=0.4409 m=2250 me=0.1733 wps=14.26 Elapsed time is 82.371071 seconds.
	% compare to r1 we=0.4247 me=0.1631
	% compare to r0 we=0.4647 me=0.1720
	r2b=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w2b,0);
	% s=50 w=1175 we=0.3651 m=2250 me=0.1378 wps=14.28 Elapsed time is 82.311780 seconds.
	% compare to r1b we=0.4085 me=0.1551

	% How slow is the non-gpu version?
	[n3,w3,w3b]=trainparser_primal_nogpu(trn_w(1:50),trn_h(1:50),idx708);
	% s=11 w=222 we=0.5811 m=422 me=0.1848 wps=0.56 Elapsed time is 396.590939 seconds.
	% compare to n2 with wps=12.12 about 20x faster

	% BAD IDEA: is this good for learning?  try moving after max and
	% learn from guessing impossible moves.
	% scores(cost == inf) = -inf;
	% [maxscore, move] = max(scores);
	[n2,w2,w2b]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	% s=50 w=1259 we=0.4599 m=2418 me=0.1625 wps=12.12 Elapsed time is 103.896044 seconds.
	r2b=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w2b,0);
	% s=50 w=1175 we=0.3651 m=2250 me=0.1378 wps=14.28 Elapsed time is 82.311780 seconds.
	% Now after the mod:
	[n4,w4,w4b]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	% s=50 w=1259 we=0.4821 m=2418 me=0.2047 wps=13.34 t=94.41
	r4b=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w4b,0);
	% s=50 w=1175 we=0.3736 m=2250 me=0.1516 wps=16.79 t=69.98

	* turan.m: Sule suggested Turan's theorem to construct the filter
	matrix.  This reduces the redundancy in the matrix representation
	of triples, halves memory use and doubles the speed:

	>> [w,w2]=trainparser_primal(trn_w(1:10),trn_h(1:10),idx708);
	Training in 8.89744e+07 dims...
	% Used to be Training in 1.77698e+08 dims...
	Sentences=10 Words=189 Words/Sec=17.4957
	Elapsed time is 10.802716 seconds.
	% Used to be 20.96 seconds with wps=9.

	* model_trn_sparse01.mat: fulltrn-static+sparsify+dynamic.
	#3541 SV: 5.27(186537)	AER: 7.50
	Elapsed time is 46844.421232 seconds.

	* trainparser_primal.m: Test training with primal vectors for
	efficiency.  It does 7.5 wps, which would finish PTB in 35 hours.
	Compared to dual models it is not very good.  Also gpu memory is
	limited and can only handle 788 dims (at least until I can figure
	out how to reduce dims to n(n+1)(n+2)/6.)

	>> [w,w2]=trainparser_primal(trn_w(1:10),trn_h(1:10),idx708);
	Training in 1.77698e+08 dims...
	Sentences=10 Words=189 Words/Sec=9.01343
	Elapsed time is 20.968780 seconds.
	>> size(model_trn01.SV)
         804      251480
	>> tic();m=trainparser_gpu(model_trn01,trn_w(1:10),trn_h(1:10),idx);toc();
	Elapsed time is 28.091093 seconds.
	>> tic();m=trainparser(model_trn01,trn_w(1:10),trn_h(1:10),idx);toc();
	Elapsed time is 16.848159 seconds.

	- ok not working again.  i give up.  either singles causing cuda
	failure.  or multiply not working.  this is all messed up.  fixed
	when I sprinkled some wait(gpuDevice) in the code.

	>> [w,w2]=trainparser_primal(trn_w,trn_h,idx708);
	Sentences=39832 Words=950028 Words/Sec=9.4902
	Elapsed time is 100106.226213 seconds.
	M-x write-region trainparser_primal_0719.log
	save trainparser_primal_0719.mat w w2
	This is the first version of trainparser_primal,
	i.e. unnecessarily large w, no w2, no 1 added...

	* profile: Use matlab profiler (e.g. on dumpfeatures).
	>> profile on -history
	>> [n2p,w2p,w2bp]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	>> p=profile('info');
	>> profsave(p,'primal-profile');
	Saves nice html file.
	But was useless with GPU code.

2014-07-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* q: gpu instability caused by singles?  no we just need a bunch of wait(gpuDevice)

	* gpu: weird shit is happening.  gpu takes 15 secs to multiply.
	what am i doing wrong?  I think applying matrix multiply for dot
	product is the cause.  MATRIX MULTIPLY IS NOT GOOD FOR SKINNY
	MATRICES ON GPU! Should use 'dot'.
	http://blog.theincredibleholk.org/blog/2012/12/10/optimizing-dot-product/

	* gpu: This is out of control:

	>> a=gpuArray(rand(1,1e8));
	>> b=gpuArray(rand(1,1e8));
	>> tic();c=sum(b.*a);disp(c);toc();
	   2.5000e+07
	Elapsed time is 0.025064 seconds.
	>> tic();c=b*a';disp(c);toc();
	   2.5000e+07
	Elapsed time is 7.178565 seconds.


2014-07-16  Deniz Yuret  <dyuret@ku.edu.tr>

	* poly3basis.c (mexFunction): 0.29 secs to generate one poly 3
	basis vector from a 804 dimensional original vector.  The
	resulting vector size is n(n+1)(n+2)/6 = 86943220 (695MB).  Dot
	product is negligible.  model_trn takes about the same amount of
	time to do kernels with 100K SV for 100 x vectors.

	GPU does it 7 times faster:
	>> x1 = gpuArray(rand(804,1));
	>> xi = gpuArray(triu(true(804,804)));
	>> tic();for i=1:1000 x2=x1*x1';x3=x2(xi)*x1';end; toc();
	Elapsed time is 40.811311 seconds.

	If I could find a better xi filter it could be even faster and
	more memory efficient.  The array indexing operation does not seem
	to slow it down:

	n=500;
	x1=gpuArray(rand(n,1));
	xi=gpuArray(triu(true(n,n)));
	x2=x1*x1';
	size(x2(:))  250000           1
	size(x2(xi)) 125250           1
	tic();for i=1:1000 x2=x1*x1';x3=x2(:)*x1';end; toc();
	% Elapsed time is 14.054163 seconds.
	tic();for i=1:1000 x2=x1*x1';x3=x2(xi)*x1';end; toc();
	% Elapsed time is 8.054333 seconds.

	Does the speed have to do with the shape of xi?  No...

	>> xi=false(n,n);
	>> xi(round(rand(175000,1)*n*n))=true;
	>> sum(xi(:))
	125848
	>> xi=gpuArray(xi);
	>> x1=gpuArray(rand(n,1));
	>> tic();for i=1:1000 x2=x1*x1';x3=x2(xi)*x1';end; toc();
	Elapsed time is 7.933409 seconds.

	Working with single's to save memory the gpuDevice crashes.
	Unless I use wait(gpuDevice).  Does that slow us down?  No.

	>> tic();for i=1:1000 wait(gpuDevice);x2=x1*x1';x3=x2(xi)*x1';end; toc();
	Elapsed time is 8.042499 seconds.
	>> x1=gpuArray(rand(n,1,'single'));
	>> tic();for i=1:1000 wait(gpuDevice);x2=x1*x1';x3=x2(xi)*x1';end; toc();
	Elapsed time is 6.866563 seconds.

	We can now do the full triple product with 804 dimensions, but no need:

	>> tic();for i=1:1000 wait(gpuDevice);x2=x1*x1';x3=x2(:)*x1';end; toc();
	Elapsed time is 60.887271 seconds.
	>> tic();for i=1:1000 wait(gpuDevice);x2=x1*x1';x3=x2(xi)*x1';end; toc();
	Elapsed time is 27.982342 seconds.

	The operations with singles are still a bit unstable.  Getting GPU
	errors half the time.  If I can find a smaller xi, it would be
	more memory efficient and possibly faster.

	* model_trn_sparse: Optimize model_sparsify and try on model03,
	model_trn, model_trn01.  GPU memory is not sufficient to handle
	the full problem.  We have to subsample the training data.

	load dumpfeatures
	load model_trn
	pick = false(1,size(trn_x,2));
	pick(model_trn.S)=true;
	pick(round(rand(1,500000)*size(trn_x,2)))=true;
	sum(pick)
	% 433500
	x_tr500k = trn_x(idx,pick);
	y_tr500k = trn_y(pick);
	model_trn_sparse = model_sparsify(model_trn, x_tr500k, y_tr500k, struct('x_te',dev_x(idx,:),'y_te',dev_y,'eta',0.3,'epsilon',0.1,'margin',1.0));

	time	iter	nsv	err_tr	err_te	maxdiff
	0	0	101517	13044	2740	0
	7349	100038	22340	12641	2998	2.329387e+08

	save model_trn_sparse.mat model_trn_sparse
	>> model_trn_sparse
	SV: [804x22340 double]    %% compare with 101517 of model_trn
        pred: [3x1720842 double]
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model_trn_sparse, 1); toc();
	Elapsed time is 8.466255 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0413   %% compare to 0.0378 of model_trn
	>> tic(); r=testparser(model_trn_sparse, dev_w, dev_h, idx); toc();
	Elapsed time is 560.445457 seconds.
	>> r
	ntot: 40117
	nerr: 4782  (0.1192)
	xtot: 76834
	xerr: 3526  (0.0459)

	* featselect.m: Implemented the SFFS algorithm with backtrack.
	From Guyon's feature selection book pp.149.  Exit when no more
	improvement?  Well it always has a chance to come back after a few
	iterations, so for now run continuously and save in a cache.

	* featselect: do 5k featselect for order6 kernel.  Below is the
	top two for each size, and comparison with order3.   It looks like
	order3 has the advantage after the third feature.

	nfeat	last	avg6	avg3	best3	nsv	time	feats
	2	15.06	11.68	12.27	12.27	30848	289.06	n0,s0
	2	19.19	14.82	14.57	12.27	39803	398.05	n0,s1
	3	10.59	8.36	8.75	8.75	23186	317.80	n0,s0,s1
	3	14.56	11.14	?	8.75	28960	273.85	n0,s0,s0r1r
	4	9.69	7.83	7.74	7.64	21493	300.65	n0,s0,s0r1r,s1
	4	10.86	7.88	8.49	7.64	22723	298.05	n0,n0l2-,s0,s1
	4	9.34	7.88	7.64	7.64	20588	363.11	n0,n0l1,s0,s1
	5	9.66	7.53	?	7.08	21326	392.55	n0,s0,s0r1r,s1,s1r2
	5	9.91	7.53	?	7.08	21063	286.53	n0,n0l2-,s0,s0r1r,s1
	5	9.60	7.58	?	7.08	21106	296.12	n0,s0,s0r1r,s1,s1+
	6	9.67	7.22	?	6.63	21248	287.42	n0,n0l2-,s0,s0r1r,s0r2r,s1
	6	8.95	7.27	?	6.63	19466	347.79	n0,n0l2-,n1,s0,s0r1r,s1
	6	9.76	7.27	?	6.63	21292	291.94	n0,n0+,n0l2-,s0,s0r1r,s1
	6	9.76	7.27	?	6.63	21292	297.71	n0,n0l2-,s0,s0+,s0r1r,s1
	6	9.76	7.27	?	6.63	21292	301.43	n0,n0l2-,n0r,s0,s0r1r,s1

	* dogma/model_sparsify.m: Added gpu support.  Unfortunately gives
	only 50% speedup.  GPU is detected and used automatically if
	present.  Here is re-optimization with new beta scaling.  The
	ideal settings in the new version are:

	eta=0.3
	epsilon=0.1
	margin=1.0

	>> model_sparsify(model5k, x_tr, y_tr, struct('x_te',x_te,'y_te',y_te,'eta',0.5,'epsilon',0.1,'margin',1));
	Scaled epsilon = 1.92221e+07, margin = 1.92221e+08
	Scaled eta = 64858.8 (used to be 37984.1)

	time	iter	nsv	err_tr	err_te	notes(eta/epsilon/margin)
	0	0	17318	275	618	(original model)
	1199	5761	1000	17968	919	(old best 0.5/0.1/1.0)
	197	5728	1000	17639	912	(new best 0.3/0.1/1.0)
	186	5390	1000	17729	912	(0.3/0.15/1.0)
	227	6609	1000	17705	925	(0.25/0.1/1.0)
	173	5021	1000	17578	940	(0.35/0.1/1.0)
	207	6025	1000	17640	966	(0.3/0.05/1.0)
	177	5138	1000	17997	978	(0.3/0.1/0.75)
	276	8043	1000	18161	989	(0.2/0.1/1.0)
	176	5122	1000	17739	994	(0.3/0.2/1.0)
	212	6168	1000	18874	994	(0.3/0.1/1.5)
	132	3833	1000	18622	1029	(0.5/0.1/1.0)

2014-07-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* dogma/model_sparsify.m: The output beta needs to be scaled.
	If we approximate beta2, then use it as beta, new training data
	cannot make a dent.  Norm^2 of beta is nsv if binary problem, 2*nsv
	if multiclass problem.  Other learning algorithms may have
	different scaling.  So we will just copy the norm from the old
	beta at the same number of sv.

==> run/run_altay.m <==
tic(); r3=testparser(model03, dev_w, dev_h, idx); toc();
r3
r3.nerr/r3.ntot
r3.xerr/r3.xtot
x_te = x5k(idx,1:10000);
y_te = y5k(1:10000);
x_tr = x5k(idx,10001:end);
y_tr = y5k(10001:end);
p4 = struct('x_te', x_te, 'y_te', y_te, epsilon, 0.1, margin, 1, eta, 0.5);
tic(); model04 = model_sparsify(model03, x_tr, y_tr, p4); toc();
model04
tic(); [a4,b4]=model_predict_gpu(dev_x(idx,:), model04, 0); toc();
numel(find(a4 ~= dev_y))/numel(dev_y)
tic(); r4=testparser_par(model04, dev_w, dev_h, idx); toc();
r4
r4.nerr/r4.ntot
r4.xerr/r4.xtot

2014-07-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* testparser_par.m: parfor version.
	- schd = findResource('scheduler', 'configuration', 'local')
	gives ClusterSize=12.  Should be higher.
	- new version parpool, old version matlabpool to start pool before
	using parfor.
	- says findResource is deprecated, use parcluster.
	- parfor copies everything.  To speed up need shared memory.
	Found at http://www.mathworks.com/matlabcentral/fileexchange/28572-sharedmatrix
	- labindex and numlabs show the id and number of threads inside an
	spmd block but not a parfor block.  for parfor getCurrentTask()
	works.
	- matlab prior to 2014 allows up to 12 workers which is what these
	results are based on:
	- does not work well on altay/balina as opposed to biyofiz.
	matlab version possible difference.

	>> tic(); r=testparser(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 202.581656 seconds.
	>> tic(); r=testparser_gpu(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 295.032944 seconds.
	>> tic(); r=testparser_par(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 102.460237 seconds.

	* testparser_gpu.m: GPU does not give much improvement in
	testparser, which needs to go sequential.  Maybe parfor is better?

	>> tic(); r=testparser_gpu(model_trn01, dev_w(1:30), dev_h(1:30), idx); toc();
	Loading model on GPU.
	Elapsed time is 3.647802 seconds.
	Processing sentences...
	Elapsed time is 52.543305 seconds.
	Elapsed time is 52.546265 seconds.
	>> tic(); r=testparser(model_trn01, dev_w(1:30), dev_h(1:30), idx); toc();
	Elapsed time is 61.559950 seconds.

	* dogma/model_predict_gpu.m: GPU optimized version of
	model_predict.m.  About 6x faster:

	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model_trn, 1); toc();
	tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model_trn, 1); toc();
	Gpu mem: 4.90497e+09, will use max 6.12508e+08 doubles.
	svtr:101517x804 beta:3x101517 margins:3x72551 numel*8:6.57135e+08 free:4.24764e+09
	Processing 28 chunks of ksize:101517x2601 (numel*8:2.11237e+09).
	Elapsed time is 22.509422 seconds.
	>> tic(); [a2,b2]=model_predict(dev_x(idx,:), model_trn, 1); toc();
	tic(); [a2,b2]=model_predict(dev_x(idx,:), model_trn, 1); toc();
	Elapsed time is 149.088427 seconds.

	* startup: To startup a gpu machine for the experiment above:
	dyuret@yunus:~[0]$ ssh biyofiz-4-2
	-bash-3.2$ /mnt/kufs/progs/matlab/R2013a/bin/matlab
	cd parser
	path('dogma',path);
	load conllWSJToken_wikipedia2MUNK-50.mat
	load model_trn.mat
	load model_trn01.mat
	idx = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	load dumpfeatures.mat

2014-07-13  Deniz Yuret  <dyuret@ku.edu.tr>

	* GPU: matlab and gpu: gpuArray, arrayfun.  nvidia-smi for info gives
	Tesla K20m with 4799MB memory.  Web search says 2496 cores, >1 TFlop,
	706MHz clock.  Try some operations with gpuArray.  Maybe model.beta2
	can be a gpuArray.  Do some profiling and speed tests.  In matlab
	gpuDevice and gpuDeviceCount give information about the GPU
	hardware.  Use class and classUnderlying to find out about
	variable types.

	The advantage is not too big for loop operations:
	>> a=rand(1000,100000);
	>> b=rand(1000,1);
	>> tic();for i=1:1000 c=a'*b;end;toc();
	Elapsed time is 19.321784 seconds.
	>> aa=gpuArray(a);
	>> bb=gpuArray(b);
	>> tic();for i=1:1000 cc=aa'*bb;end;toc();
	Elapsed time is 12.098012 seconds.

	But it is huge for single operations:
	>> a=rand(2e4,1e3);
	>> b=rand(1e3,2e4);
	>> tic(); c=a*b; toc();
	Elapsed time is 4.911055 seconds.

	>> aa=gpuArray(a);
	>> bb=gpuArray(b);
	>> tic();cc=aa*bb;toc();
	Elapsed time is 0.015769 seconds.

	GPU memory is 4.8e9 bytes, i.e. 6e8 doubles.  When exceeded we get:
	Out of memory on device. To view more detail about available memory on the GPU,
	use 'gpuDevice()'. If the problem persists, reset the GPU by calling
	'gpuDevice(1)'.

	More realistic example of our kernel calculation:
	>> a=rand(1e5,1e3);
	>> b=rand(1e3,2.5e3);  % the biggest size gpu can handle
	>> tic();c=(a*b+1).^3;toc();
	Elapsed time is 5.796152 seconds.
	>> aa=gpuArray(a);bb=gpuArray(b);
	>> tic();cc=(aa*bb+1).^3;toc();
	Elapsed time is 0.671553 seconds.

	arrayfun and bsxfun serve the function of map with one and two
	elements, respectively.  But memory needs to be sufficient to keep
	intermediate values.

	We should be able to improve model_predict and model_sparsify
	ten-fold.  Maybe some other day... (done) Training will be harder to
	improve unless we can organize it in mini-batches.

	* featselect-5k.log: do feature selection for the 5k dataset with
	poly3 kernel.  avg2 gives the second best score.  Results similar to
	1k.

	nfeat	last	avg		avg2		nsv	time	feats
	2	16.74	12.27(n0,s0)	14.57(n0,s1)	32924	173.68	n0,s0
	3	11.84	8.75(s1)	11.23(n0l1)	24319	183.60	n0,s0,s1
	4	10.64	7.64(n0l1)	7.68(s0r1)	21430	176.57	n0,n0l1,s0,s1
	5	10.49	7.15(s0r)	7.18(s1r1r)	21127	187.71	n0,n0l1,s0,s0r,s1
xx	5	9.08	7.08(s0r1+n1)			19979	251.18	n0,n1,s0,s0r1,s1
	6	9.07	6.70(n1)	7.04(n0l1r)	19017	204.83	n0,n0l1,n1,s0,s0r,s1
xx	6	8.72	6.63(-s0r+s0r1)			18609	270.89	n0,n0l1,n1,s0,s0r1,s1
	7	8.38	6.35(s0r1)	6.45(s0l2+,s0s1)18800	258.10	n0,n0l1,n1,s0,s0r,s0r1,s1
xx	7	8.94	6.34(-s0r+s1r2r)6.38(-s0r+s1r1+)18349	265.81	n0,n0l1,n1,s0,s0r1,s1,s1r2r
	8	8.60	6.20(s0l1)	6.22(s1r1l,s0s1)17879	300.95	n0,n0l1,n1,s0,s0l1,s0r,s0r1,s1
xx	8	8.53	6.22(s0s1)			17775	252.31	n0,n0l1,n1,s0,s0r,s0r1,s0s1,s1
	9	8.05	5.99(n0l2)	6.16(s1r2r)	17736	298.63	n0,n0l1,n0l2,n1,s0,s0r,s0r1,s0s1,s1

	* idx708.mat: idx708=featureindices({'n0','n0l1','n0l2','n1','s0','s0r','s0r1','s0s1','s1'});

	* model_trn.mat:
	>> path('dogma',path);
	>> load dumpfeatures.mat
	>> idx = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	>> hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3)
	>> model_trn = model_init(@compute_kernel, hp);
	>> model_trn = k_perceptron_multi_train(trn_x(idx,:), trn_y, model_trn);
	#1720 SV: 5.90(101473)	AER: 5.90
	Elapsed time is 15841.799743 seconds.
        SV: [804x101517 double]
        pred: [3x1720842 double]
	>> tic(); [a,b]=model_predict(dev_x(idx,:), model_trn, 1); toc();
	Elapsed time is 151.497801 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0378
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model_trn, 0); toc();
	Elapsed time is 22.676895 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0527
	>> tic(); r=testparser(model_trn, dev_w, dev_h, idx); toc();
	Elapsed time is 1408.589679 seconds.
	ntot: 40117
	nerr: 4422  (0.1102)
	xtot: 76834
	xerr: 3263  (0.0425)
	>> tic(); model_trn01 = trainparser(model_trn, trn_w, trn_h, idx); toc();
	Elapsed time is 52817.115166 seconds.
	>> model_trn01
        SV: [804x251480 double]
        pred: [3x3541234 double]
	>> tic(); [a,b]=model_predict(dev_x(idx,:), model_trn01, 1); toc();
	Elapsed time is 393.421166 seconds.
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model_trn01, 1); toc();
	Elapsed time is 56.210434 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0389
	>> tic(); r=testparser_par(model_trn01, dev_w, dev_h, idx); toc();
	Elapsed time is 1610.735364 seconds.
	>> r
	ntot: 40117
	nerr: 4072  (0.1015)
	xtot: 76834
	xerr: 3029  (0.0394)

	* trainparser.m: Next train using dynamic oracle to fine tune.  Use
	trn_w and trn_h (1:5000) to train, dev_w and dev_h to test.  gtrans
	is the error rate on gold transitions (static oracle), ptrans is the
	error rate during parsing (dynamic oracle), phead is the UAS.

	- model00: starting from scratch and training for one epoch with
	dynamic oracle we get parser transition error of 6.78, gold-path
	transition error of 9.21, head error of 17.56,

	>> path('dogma', path);
	>> load conllWSJToken_wikipedia2MUNK-50.mat
	>> hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3)
	>> idx = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	>> model00 = model_init(@compute_kernel,hp);
	>> model00.n_cla = 3;
	>> model00 = trainparser(model00, trn_w(1:5000), trn_h(1:5000), idx)
	#230 SV:12.69(29177)	AER:12.69
	SV: [804x29187 double]
	pred: [3x230120 double]
	>> [dev_x,dev_y]=dumpfeatures(dev_w,dev_h);
	Elapsed time is 114.633561 seconds.
	dev_x           1768x72551             1026161344  double
	dev_y              1x72551                 580408  double
	>> [a,b]=model_predict(dev_x(idx,:), model00, 1);
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0921
	>> tic(); r=testparser(model00, dev_w, dev_h, idx); toc();
	Elapsed time is 2098.151274 seconds.
	ntot: 40117 (0.1756)
	nerr: 7044
	xtot: 76834
	xerr: 5210  (0.0678)

	- model01: starting from static x5k model and train one epoch with
	dynamic oracle.  The static model has gold transition error of 5.43,
	parser transition error of 5.78, and head error of 15.14.  After one
	epoch of dynamic oracle training we have gold: 5.56, ptrans:5.36,
	head:14.13.

	>> load dump5000-model.mat
	model         1x1                 120667128  struct
	SV: [804x17318 double]
        pred: [3x207603 double]
	>> model.step = 1000;
	>> tic(); [a,b]=model_predict(dev_x(idx,:), model, 1); toc();
	Elapsed time is 58.129140 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0543
	>> tic(); r = testparser(model, dev_w, dev_h, idx); toc();
	Elapsed time is 336.569856 seconds.
	>> tic(); r2 = testparser_par(model, dev_w, dev_h, idx); toc();
	Elapsed time is 103.098387 seconds.
	ntot=40117 % number of words
	nerr=6072 (0.1514) % number of wrong heads
	xtot=76834 % number of parser transitions
	xerr=4443 (0.0578) % number of suboptimal transitions
	% Note the original model was not trained on 1:5000, it used the
	% first 10k transitions as test
	>> tic(); model01 = trainparser(model, trn_w(1:5000), trn_h(1:5000), idx); toc();
	Elapsed time is 6489.773220 seconds.
        SV: [804x40502 double]
        pred: [3x437723 double]
	>> tic(); [a,b]=model_predict(dev_x(idx,:), model01, 1); toc();
	Elapsed time is 140.067637 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0556
	>> tic(); r=testparser(model01, dev_w, dev_h, idx); toc();
	Elapsed time is 2865.993597 seconds.
	ntot: 40117
	nerr: 5667  (0.1413)
	xtot: 76834
	xerr: 4121  (0.0536)

	- model02: try multiple epochs.  doing the second round of dynamic
	oracle over model01.

	>> model02 = trainparser(model01, trn_w(1:5000), trn_h(1:5000), idx);
	#667 SV: 8.83(58916)	AER: 8.83
	Elapsed time is 10679.100909 seconds.
	>> model02
        SV: [804x58979 double]
	pred: [3x667843 double]
	>> [a,b]=model_predict(dev_x(idx,:), model02, 1);
	Elapsed time is 211.158363 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0574
	>> tic(); r=testparser(model02, dev_w, dev_h, idx); toc();
	Elapsed time is 4107.217578 seconds.
	>> r
	ntot: 40117
	nerr: 5533  (0.1379)
	xtot: 76834
	xerr: 4081  (0.0531)

	- model03: starting from sparsified x5k model m6.

	>> load m6
	SV: [804x12555 double]
        pred: [3x207603 double]
	>> tic(); [a,b]=model_predict(dev_x(idx,:), m6, 0); toc();
	Elapsed time is 80.493971 seconds.
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m6, 1); toc();
	Elapsed time is 10.364309 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0534
	>> tic(); r = testparser_par(m6, dev_w, dev_h, idx); toc();
	Elapsed time is 75.726568 seconds.
	>> r
	ntot: 40117
	nerr: 6035  (0.1504)
	xtot: 76834
	xerr: 4393  (0.0572)
	>> tic(); model03 = trainparser(m6, trn_w(1:5000), trn_h(1:5000), idx); toc();
	Elapsed time is 5483.439487 seconds.
	>> model03
        SV: [804x36073 double]
        pred: [3x274392 double]
	>> tic(); [a,b]=model_predict(dev_x(idx,:), model03, 1); toc();
	Elapsed time is 123.986750 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0716
	>> tic(); r3=testparser_par(model03, dev_w, dev_h, idx); toc();
	Elapsed time is 2573.779485 seconds.
	>> r3
	ntot: 40117
	nerr: 6319  (0.1575)
	xtot: 76834
	xerr: 4638  (0.0604)

	% maybe getting rid of beta2 was not a good idea.  let us try with
	% appropriately weighted beta2 this time.

	>> n1=norm(model.beta2(:,1:nsv),'fro')
	>> n2=norm(m6.beta2, 'fro')
	>> m6.beta2 = m6.beta2 * (n1/n2);
	>> tic(); model03b = trainparser(m6, trn_w(1:5000), trn_h(1:5000), idx); toc();
	Elapsed time is 5467.717893 seconds.
	>> model03b
        SV: [804x36073 double]
        pred: [3x274392 double]
	% same errors, same sv, but different beta2
	>> a0=model_predict_gpu(dev_x(idx,:), model03, 0); numel(find(a0 ~= dev_y))/numel(dev_y)
	0.1192
	>> b0=model_predict_gpu(dev_x(idx,:), model03b, 0); numel(find(b0 ~= dev_y))/numel(dev_y)
	0.1192
	>> a1=model_predict_gpu(dev_x(idx,:), model03, 1); numel(find(a1 ~= dev_y))/numel(dev_y)
	0.0716
	>> b1=model_predict_gpu(dev_x(idx,:), model03b, 1); numel(find(b1 ~= dev_y))/numel(dev_y)
	0.0547
	>> tic(); r3b=testparser(model03b, dev_w, dev_h, idx); toc();
	Elapsed time is 2574.903771 seconds.
	>> r3b
	ntot: 40117
	nerr: 5689  (0.1418)
	xtot: 76834
	xerr: 4192  (0.0546)

	>> model04 = model_sparsify(model03b, x_tr, y_tr, struct('x_te',x_te,'y_te',y_te,'eta',0.3,'epsilon',0.1,'margin',1.0));
	Elapsed 2157 secs.
	>> model04
        SV: [804x13043 double]
        pred: [3x274392 double]
	pred_te: [3x10000 double]
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model04, 1); toc();
	Elapsed time is 16.827549 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0535
	>> tic(); r=testparser_par(model04, dev_w, dev_h, idx); toc();
	Elapsed time is 379.683786 seconds.
	>> r
	ntot: 40117
	nerr: 5803  (0.1447)
	xtot: 76834
	xerr: 4296  (0.0559)

	* model_dbg: test to see if trainparser gives the same result when oracle moves
	are picked.  However be careful because it is using the whole x5k as training,
	older model used the first 10k instances for testing.  Another difference is
	whether or not to call the learner when there is only one legal move.  I started
	not to in dumpfeatures, but now I think maybe that was a bad idea.  So we need to
	add options to dumpfeatures for this test.

	Result: trainparse is faster to train than dumpfeatures+k_perceptron
	and results in an identical model.

	>> path('dogma',path);
	>> hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3)
	>> idx = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	>> load conllWSJToken_wikipedia2MUNK-50.mat
	>> [x5k1,y5k1] = dumpfeatures(trn_w(1:5000), trn_h(1:5000), 1);
	Elapsed time is 348.282005 seconds.
	x5k1           1768x230120            3254817280  double
	y5k1              1x230120               1840960  double
	>> model_ref = model_init(@compute_kernel,hp);
	>> tic(); model_ref = k_perceptron_multi_train(x5k1(idx,:), y5k1, model_ref); toc();
	#230 SV: 7.98(18348)	AER: 7.98
	Elapsed time is 2262.626120 seconds.
	>> model_dbg = model_init(@compute_kernel,hp);
	>> model_dbg.n_cla = 3;
	>> tic(); model_dbg = trainparser_dbg(model_dbg, trn_w(1:5000), trn_h(1:5000), idx); toc();
	Elapsed time is 2509.524566 seconds.
	>> all(model_ref.S == model_dbg.S)
	1

	* dumpfeatures.m: added option to dump singleton moves (moves without
	alternative) as well.

	* testparser.m (testparser): modified to return single struct
	array.

	* dumpfeatures.mat: convert all data to features once and for all. (on yunus)
	[trn_x,trn_y]=dumpfeatures(trn_w,trn_h);
	Elapsed time is 2922.888628 seconds.
	[dev_x,dev_y]=dumpfeatures(dev_w,dev_h);
	Elapsed time is 123.879583 seconds.
	[tst_x,tst_y]=dumpfeatures(tst_w,tst_h);
	Elapsed time is 571.966855 seconds.
	save dumpfeatures.mat -v7.3

	Name          Size                       Bytes  Class     Attributes
	dev_h         1x1700                    511336  cell
	dev_w         1x1700                  32284000  cell
	dev_x      1768x72551               1026161344  double
	dev_y         1x72551                   580408  double
	trn_h         1x39832                 12061408  cell
	trn_w         1x39832                764483584  cell
	trn_x      1768x1720842            24339589248  double
	trn_y         1x1720842               13766736  double
	tst_h         1x7676                   2328680  cell
	tst_w         1x7676                 147756512  cell
	tst_x      1768x332821              4707420224  double
	tst_y         1x332821                 2662568  double

	* featselect.m: take an optional starting cell array of features
	argument.

	* featureindices.m: modified to output actual index vector in
	addition to an index hash and the total number of features.

	* polydegree.m: try 8 words (n0,s0,s1,n1,n0l1,s0r1,s0l1,s1r1) and
	all their aux features (e.g. n0l,n0r,n0+,n0-) on x5k with all
	kernels.  Compare to earlier results we got with x5k.  Order 6 gives
	best last-error, 8/9 give best avg-error but none are as good as
	order 3 with its optimized smaller 9 feature set.  6.18 is still the
	best we get with x5k.

	degree	last	avg	nsv	time
	1	18.34	13.34	36732	3890.22
	2	14.56	8.21	24299	2698.12
	3	12.73	7.10	21440	2454.09
	4	11.90	6.90	20083	2310.41
	5	9.00	6.75	19167	2223.89
	6	7.97	6.83	18649	2169.10
	7	8.67	6.89	18372	2152.53
	8	8.44	6.73	18225	2127.00
	9	8.13	6.73	18100	2115.84

	>> dump5000
	Using poly3 kernel on n0,s0,s1,n1,n0l1,s0r1,s0l1,s1r1,s0r
	train	last	avg	nsv	time
	207603	7.81	6.18	17318	1939.12


==> run/run_polydegree.m <==
% load conllWSJToken_wikipedia2MUNK-50.mat;
% Elapsed time is 9.625437 seconds.
% [x5k,y5k]=dumpfeatures(trn_w(1:5000),trn_h(1:5000));
% Elapsed time is 341.514392 seconds.

[fi,i] = featureindices();
assert(i == size(x5k,1));
feats = {'n0','n0l','n0r','n0+','n0-', ...
         's0','s0l','s0r','s0+','s0-', ...
         's1','s1l','s1r','s1+','s1-', ...
         'n1','n1l','n1r','n1+','n1-', ...
         'n0l1','n0l1l','n0l1r','n0l1+','n0l1-', ...
         's0r1','s0r1l','s0r1r','s0r1+','s0r1-', ...
         's0l1','s0l1l','s0l1r','s0l1+','s0l1-', ...
         's1r1','s1r1l','s1r1r','s1r1+','s1r1-', ...
         'n0s0','s0s1'};
idx = [];
for i=1:length(feats) idx = [idx, fi(feats{i})]; end

x_te = x5k(idx,1:10000);
y_te = y5k(1:10000);
x_tr = x5k(idx,10001:end);
y_tr = y5k(10001:end);

hp.type = 'poly';
hp.gamma = 1;
hp.coef0 = 1;

fprintf('degree\tlast\tavg\tnsv\ttime\n');
for d=1:9
  hp.degree = d;
  model = model_init(@compute_kernel, hp);
  model.step = 1000000;
  tic();
  model = k_perceptron_multi_train(x_tr, y_tr, model);
  telapsed = toc();
  nsv = size(model.beta, 2);
  last = numel(find(y_te ~= model_predict(x_te, model, 0)))/numel(y_te)*100;
  avg = numel(find(y_te ~= model_predict(x_te, model, 1)))/numel(y_te)*100;
  fprintf('%d\t%.2f\t%.2f\t%d\t%.2f\n', hp.degree, last, avg, nsv, telapsed);
end


2014-07-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* testparser.m: shows that 5.4% static error rate corresponds to 15%
	parse error rate.
	- This model is from 5K words, more should decrease the static rate.
	- This model did not train with dynamic oracle, that should decrease
	the difference.
	- This model was only trained for one epoch.

	>> path('dogma',path);
	>> load conllWSJToken_wikipedia2MUNK-50.mat
	dev_h         1x1700                 511336  cell
	dev_w         1x1700               32284000  cell
	>> load dump5000-model.mat
	model         1x1                 120667128  struct
	>> feats = {'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'};
	>> idx = [];
	>> for i=1:length(feats) idx = [idx, fi(feats{i})]; end
	idx           1x804                    6432  double
	>> [dev_x,dev_y]=dumpfeatures(dev_w, dev_h);
	Elapsed time is 143.275703 seconds.
	dev_x      1768x72551            1026161344  double
	dev_y         1x72551                580408  double
	>> [a,b]=model_predict(dev_x(idx,:), model, 1);
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0543
	>> [nerr,ntot,xerr,xtot] = testparser(model, dev_w, dev_h, idx);
	Elapsed time is 336.569856 seconds.
	ntot=40117 % number of words
	nerr=6072 (0.1514) % number of wrong heads
	xtot=76834 % number of parser transitions
	xerr=4443 (0.0578) % number of suboptimal transitions

	* cluster: figure out how to use the cluster.
	man sge_intro for a summary.
	qhost to see status of hosts.
	qstat to see status of jobs.
	qsub to submit a job.
	/mnt/kufs/progs/matlab/R2013a/bin/matlab is the binary.
	/mnt/kufs/scratch/dyuret is the scratch space.
	ssh biyofiz-4-3 if available.

	* multithreading: figure out how to set number of processors.

	MATLAB supports three kinds of parallelism: multithreaded, distributed
	computing, and explicit parallelism.

	Note:   maxNumCompThreads will be removed in a future version. You
	can set the -singleCompThread option when starting MATLAB to limit
	MATLAB to a single computational thread. By default, MATLAB makes
	use of the multithreading capabilities of the computer on which it
	is running.

	https://newton.utk.edu/bin/view/Main/MatlabParallelization

	taskset -pc pid: retrieve a processes’s CPU affinity

	* why: are ural and balina so much slower than altay?  Because we
	haven't setup the step parameter, so they don't print out anything!

	* sparsify5000d.log: running the optimal params to conclusion in x5k.

	>> m6 = model_sparsify(model, x_tr, y_tr, p);
	Using averaged solution beta2.
	epsilon=0.1 margin=1 eta=0.5
	train: 207603, test: 10000, classes: 3, nsv: 17318
	Computing initial scores...
	Elapsed time is 150.042002 seconds.
	Scaled epsilon = 1.92221e+07, margin = 1.92221e+08
	Scaled eta = 37984.1
	time	iter	nsv	err_tr	err_te	maxdiff
	0	0	17318	7268	618	(original model)
	1200	5761	1000	17968	919	2.160987e+08
	2317	11131	2000	13775	787	1.797998e+08
	3399	16322	3000	11897	712	1.529027e+08
	4419	21225	4000	10571	675	1.201366e+08
	5323	25561	5000	9688	657	1.051552e+08
	5743	27577	5500	9361	649	8.962474e+07
	6378	30627	6300	8808	636	7.853851e+07
	6942	33341	7100	8433	628	6.410252e+07
	7748	37203	8500	7831	614	5.001900e+07
	8254	39639	9600	7513	607	3.902603e+07
	9119	43789	12200	7012	601	2.695608e+07
	9219	44272	12555	6963	610	1.911820e+07

	* model-sparsify-parameters: So what are the ideal parameters for
	sparsify.  Some samples so far (all results for x5k, y5k, at
	nsv=1000): epsilon/margin scale: 1.92221e+08 (mean positive margin)
	eta scale: 75968.2 (mean absolute beta)

	However note that the eta scaling changed in the updated version
	of 2014-07-16.

	eta	epsilon	margin	err_tr	err_te	time	iter	maxdiff
	(original nsv=17318)	7268	618	-	-	-
	0.5	0.1	1.0	17968	919	1199	5761	2.160987e+08
	0.5	0.05	1.0	17756	923	1287	6180	2.217087e+08
	0.25	0.1	1.0	17654	941	2241	10783	2.158356e+08
	0.75	0.1	1.0	18008	960	869	4176	2.350121e+08
	0.5	0.25	1.0	17634	968	1019	4887	2.084023e+08
	0.5	0.5	1.0	17815	977	729	3498	2.06433e+08
	0.5	0.1	1.25	18402	983	1269	6094	2.333505e+08
	0.5	0.1	0.75	18815	986	1090	5239	2.052092e+08
	0.5	0.5	0.75	18342	1000	532	2550	1.650722e+08
	0.5	0.5	1.5	19641	1030	908	4360	2.389662e+08
	0.5	0.5	2.0	20894	1081	959	4605	2.456402e+08
	0.5	0.25	2.0	21075	1102	1171	5626	2.489516e+08
	0.5	0.25	0.5	23241	-	-	3120	1.35809e+08
	0.5	0.1	0.2	38032	-	-	2605	7.10636e+07
	0.5	0.05	0.1	54547	2638	491	2413	5.24318e+07

	* dogma/model_sparsify.m: done: epsilon and eta should be input
	parameters not model fields (should not touch the original model).
	Is it a good idea to cap the margins at 2*epsilon?  Those two
	should be independent (done).  The new signature is:
	function m = model_sparsify(model,x_tr,y_tr,p)
	with optional parameters in p: margin, epsilon, eta, average,
	x_te, y_te.  done: compare results with older log files.

	* sparsify5000c.log:
	x_tr             804x207603            1335302496  double
	y_tr               1x207603               1660824  double
	p =
	x_te: [804x10000 double]
	y_te: [1x10000 double]
	epsilon: 0.2500
	margin: 2
	>> m5 = model_sparsify(model, x_tr, y_tr, p);
	Using averaged solution beta2.
	epsilon=0.25 margin=2 eta=0.5
	train: 207603, test: 10000, classes: 3, nsv: 17318
	Computing initial scores...
	Elapsed time is 150.562805 seconds.
	Scaled epsilon = 4.80553e+07, margin = 3.84442e+08
	Scaled eta = 37984.1
	time	iter	nsv	err_tr	err_te	maxdiff
	0	0	17318	7268	618	0
	128	614	100	49243	2334	4.266032e+08
	3740	17950	4000	10988	696	1.170816e+08
	3995	19173	4400	10607	679	1.097213e+08
	4176	20039	4700	10372	666	1.019421e+08
	4294	20605	4900	10122	656	9.877697e+07
	4348	20865	5000	10126	649	9.834636e+07
	5018	24079	6500	9310	639	6.960811e+07
	5616	26946	8589	8685	632	4.795281e+07

	* dogma/model_sparsify.m: should also take a test set so we can
	see how out of sample performance evolves.
	Done (sparsify5000b.log), but epsilon and the margin cap are tied
	which should be fixed.

	time	iter	nsv	err_tr	err_te	max(h-c)
	0	0	17318	7268	618	0
	3717	17878	7300	7325	631	3.15407e+07
	3760	18077	7400	7193	624	2.9221e+07
	4203	20135	8300	6696	621	2.62548e+07
	4239	20299	8400	6630	615	2.69433e+07
	6213	29002	13377	5899	599	9.47589e+06

==> run/run_perceptron.m <==
% path('dogma',path);
function [last,avg,nsv,time,model] = run_perceptron(x_tr, y_tr, x_te, y_te, kernel)
model = model_init(@compute_kernel, kernel);
model.step = 1000000;
tic();
model = k_perceptron_multi_train(x_tr,y_tr,model);
pred_perceptron_last = model_predict(x_te,model,0);
pred_perceptron_av = model_predict(x_te,model,1);
time = toc();
last = numel(find(pred_perceptron_last~= y_te))/numel(y_te)*100;
avg = numel(find(pred_perceptron_av~=y_te))/numel(y_te)*100;
nsv = size(model.beta, 2);
end


==> run/run_sparsify5000.m <==
% load conllWSJToken_wikipedia2MUNK-50.mat;
% Elapsed time is 9.625437 seconds.
% [x5k,y5k]=dumpfeatures(trn_w(1:5000),trn_h(1:5000));
% Elapsed time is 341.514392 seconds.
% load dump5000-model.mat; % loads model
% Elapsed time is 0.704188 seconds.

[fi,i] = featureindices();
assert(i == size(x5k,1));
feats = {'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'};
idx = [];
for i=1:length(feats)
  idx = [idx, fi(feats{i})];
end % for

x_te = x5k(idx,1:10000);
y_te = y5k(1:10000);
x_tr = x5k(idx,10001:end);
y_tr = y5k(10001:end);
return
model.step = 100;
m = model_sparsify(x_tr, y_tr, model, 1);

p0 = model_predict(x_te, model, 1);
numel(find(p0 ~= y_te))
p = model_predict(x_te, m, 0);
numel(find(p ~= y_te))


2014-07-11  Deniz Yuret  <dyuret@ku.edu.tr>

	* sparsify5000.m: Saved as sparsify5000.log and
	sparsify5000-models.mat.

	epsilon	iter	nsv	train	test	secs	notes
	0.5	12666	5821	10889	683	2708	m
	0.25	23329	10413	8286	604	4848	m2
	0.10	30826	13534	6387	590	6360	m3
	--	--	17318	7268	618	1939	model (original)

	* dogma/model_sparsify.m: First results.  We scale eta with
	mean(abs(beta)) and epsilon with mean(margin(margin>0)).  To see
	the effect of eta on performance and convergence, we take a
	snapshot at 300 sv on the dump1000 model.  eta=0.5 does seem like
	a good time performance trade-off.

	Initial nsv=3999 error=1453/34217
	eta	iter	nsv	err	max(h-c)
	2.0	393	300	6007	2.81e+07
	1.0	535	300	5058	2.58e+07
	0.5	858	300	4835	2.51e+07
	0.25	1461	300	4773	2.42e+07
	0.10	3158	300	4680	2.32e+07

	Running to conclusion with eta=0.5 we get
	iter	nsv	err	max(h-c)
	4754	2446	2089	1.07428e+07

	Evaluating this model on the test set (10K) we get 882 errors vs
	838 with the original model:

	[x5k,y5k]=dumpfeatures(trn_w(1:5000),trn_h(1:5000));
	x_te = x5k(idx,1:10000);
	y_te = y5k(1:10000);
	x_tr = x5k(idx,10001:end);
	y_tr = y5k(10001:end);
	>> m = model_sparsify(x_tr, y_tr, model1bak, 1);
	   m.SV: [804x2446 double]
	   model1bak.SV: [804x3999 double]
	>> p = model_predict(x_te, m, 0);
	>> numel(find(p ~= y_te))
	ans = 882
	>> p0 = model_predict(x_te, model1bak, 1);
	>> numel(find(p0 ~= y_te))
	ans = 838

	* sparsify5000.m: Time to try sparsifying big model.  The original
	model took 1740 secs to train on 207603 instances with err=7268,
	nsv=17318, test_error=618/10K.

	Instances: 207603, classes: 3, nsv: 17318
	Initial nsv=17318 error=7268/207603 test_error=618/10K


2014-07-10  Deniz Yuret  <dyuret@ku.edu.tr>

	* dogma/model_sparsify.m: Implementing cotter13.
	- Need to figure out scaling.  Both beta and x magnitudes effect
	scale.  eta determines the size of beta updates and epsilon
	determines when we quit by looking at margin differences.  Problem
	is averaged perceptron does not give us a well defined margin.
	Maybe we can get an idea of where the margin is by looking at the
	SV activations.  Or simply assume take max margin as 1.  epsilon
	would then be 1/2 max margin.

	f = w.φ(x) = Σ bi φ(xi).φ(x)  where bi = ai.yi

	The paper suggests stopping when f > 1/2, but assumes K(x,x)<=1
	and svm margin = 1.  We will scale eta using mean_beta and epsilon
	using mean_margin.

	* test_models.mat: Saved a few test models to test model_sparsify:
	model is multiclass, model2 is binary, both trained on x_tr and
	y_tr (multi), y_tr2 (binary).

	* perceptron.m: Things to try:
	+ Dump gold sequences and first get a good model for classification.
	+ Kernel: poly degree, gauss, etc.
	+ Features: which words, distances, children etc.
	+ Effect of kernel on features.
	+ Effect of data size performance.
	+ Effect of data size on features.
	+ Next reduce SV's using cotter13.

	* featselect-poly3-n2000.out: Effect of data size on feature
	selection.  Going from 1000 to 2000 training sentences.  Resulting
	features are identical to previous experiment but featselect stops
	at 5 features, previously it went to 10.  We need more experiments
	to understand what is going on.

	n	feat	last	avg	nsv	time
	2	s0	17.81	13.16	13202	60.08	n0,s0
	3	s1	12.40	9.42	10348	67.12	n0,s0,s1
	4	n0l1	11.56	8.52	9304	97.72	n0,n0l1,s0,s1
	5	s0r1r	10.73	7.82	8994	93.17	n0,n0l1,s0,s0r1r,s1

2014-07-09  Deniz Yuret  <dyuret@ku.edu.tr>

	* dump5000.m: Effect of data size on performance.

	>> load conllWSJToken_wikipedia2MUNK-50.mat
	>> [x,y]=dumpfeatures(trn_w(1:5000),trn_h(1:5000));
	Elapsed time is 343.961048 seconds.
	>> dump5000
	Using poly3 kernel on n0,s0,s1,n1,n0l1,s0r1,s0l1,s1r1,s0r
	train	last	avg	nsv	time
	10000	12.75	11.24	1469	11.33
	20000	11.10	9.54	2599	21.30
	30000	9.84	8.65	3564	45.20
	40000	9.80	8.18	4511	83.13
	50000	9.98	7.75	5376	145.05
	60000	9.31	7.58	6237	207.82
	70000	10.17	7.48	7080	275.49
	80000	9.40	7.22	8032	392.28
	90000	9.01	7.00	8859	389.71
	100000	9.43	6.92	9618	479.01
	110000	8.72	6.89	10304	579.18
	120000	8.72	6.77	10995	688.76
	130000	8.80	6.78	11696	795.67
	140000	8.66	6.75	12435	923.55
	150000	8.48	6.66	13120	1046.87
	160000	7.85	6.54	13914	1190.26
	170000	8.29	6.45	14611	1333.57
	180000	8.14	6.30	15322	1481.91
	190000	7.81	6.25	16100	1643.82
	200000	8.40	6.23	16832	1800.59
	207603	7.81	6.18	17318	1939.12

==> run/dump1000b.m <==
% path('dogma', path);
%

tic();
fprintf('Loading dump1000b dataset\n');
load dump1000b.mat;                     % load x,y
toc();

% reduce features to 8 basic words
tic();
fprintf('Splitting features\n');
[fi,i] = featureindices();
assert(i == size(x,1));

feats = {'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'};
fprintf('Using poly kernel on %s\n', strjoin(feats,','));
idx = [];
for i=1:length(feats)
  idx = [idx, fi(feats{i})];
end % for
x = x(idx, :);
x_te = x(:,1:10000);
y_te = y(1:10000);
x_tr = x(:,10001:end);
y_tr = y(10001:end);
%x_tr = x(:,10001:20000);
%y_tr = y(10001:20000);
toc();

fprintf('degree\tlast\tavg\tnsv\ttime\n');
for degree=1:9
hp.type = 'poly';
hp.gamma = 1;
hp.coef0 = 1;
hp.degree = degree;

% fprintf('Using %dth degree poly kernel\n', hp.degree);

% for i=[.07,.09]
% hp.type = 'rbf';
% hp.gamma = i;
% fprintf('Using rbf kernel with gamma=%f\n', hp.gamma);

model_bak = model_init(@compute_kernel,hp);
model_bak.step = 1000000;

tic();
%% PERC
% train Perceptron
% fprintf('Training Perceptron model...\n');
model_perceptron = k_perceptron_multi_train(x_tr,y_tr,model_bak);
% fprintf('Done!\n');
% fprintf('Number of support vectors last solution:%d\n',size(model_perceptron.beta, 2));
% fprintf('Number of support vectors averaged solution:%d\n',size(model_perceptron.beta2, 2));
% fprintf('Testing last solution...');
pred_perceptron_last = model_predict(x_te,model_perceptron,0);
% fprintf('Done!\n');
% fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_perceptron_last~=y_te))/numel(y_te)*100);
% fprintf('Testing averaged solution...');
pred_perceptron_av = model_predict(x_te,model_perceptron,1);
% fprintf('Done!\n');
% fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_perceptron_av~=y_te))/numel(y_te)*100);

telapsed = toc();
err_last = numel(find(pred_perceptron_last~= y_te))/numel(y_te)*100;
err_av = numel(find(pred_perceptron_av~=y_te))/numel(y_te)*100;
nsv = size(model_perceptron.beta, 2);
fprintf('%d\t%.2f\t%.2f\t%d\t%.2f\n', ...
        degree, err_last, err_av, nsv, telapsed);

end
return

%% PA_I
tic();
fprintf('Training PA-I model...\n');
model_pa1 = k_pa_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_pa1.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_pa1.beta2, 2));
fprintf('Testing last solution...');
pred_pa1_last = model_predict(x_te,model_pa1,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_pa1_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_pa1_av = model_predict(x_te,model_pa1,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_pa1_av~=y_te))/numel(y_te)*100);
toc();

%% PA_II
tic();
fprintf('Training PA-II model...\n');
model_pa2 = model_bak;
model_pa2.update = 2;
model_pa2 = k_pa_multi_train(x_tr,y_tr,model_pa2);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_pa2.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_pa2.beta2, 2));
fprintf('Testing last solution...');
pred_pa2_last = model_predict(x_te,model_pa2,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_pa2_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_pa2_av = model_predict(x_te,model_pa2,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_pa2_av~=y_te))/numel(y_te)*100);
toc();


%% PROJ++ eta=0.1
tic();
% train Projectron++
fprintf('Training Projectron++ model...\n');
model_projectron2 = k_projectron2_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_projectron2.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_projectron2.beta2, 2));
fprintf('Testing last solution...');
pred_projectron2_last = model_predict(x_te,model_projectron2,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_projectron2_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_projectron2_av = model_predict(x_te,model_projectron2,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_projectron2_av~=y_te))/numel(y_te)*100);
toc();

%% RBP
tic();
% set maximum number of support vectors for RBP
model_bak.maxSV = size(model_projectron2.beta,2);
% train RBP
fprintf('Training RBP...\n');
model_rbp = k_perceptron_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_rbp.beta, 2));
fprintf('Testing last solution...');
pred_rbp_last = model_predict(x_te,model_rbp,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_rbp_last~=y_te))/numel(y_te)*100);
toc();

% tic();
% fprintf('Plotting error and SV curves...\n');
% % plot error curves
% figure(1)
% plot(model_pa1.aer(100:end),'k')
% hold on
% plot(model_perceptron.aer(100:end),'c')
% plot(model_projectron.aer(100:end),'b')
% plot(model_projectron2.aer(100:end),'m')
% plot(model_rbp.aer(100:end),'r')
% plot(model_forgetron.aer(100:end),'y')
% plot(model_oisvm.aer(100:end),'g')
% grid
% legend('PA-I','Perceptron','Projectron','Projectron++','RBP','Forgetron','OISVM')
% xlabel('Number of Samples')
% ylabel('Average Online Error')

% % plot support vector curves
% figure(2)
% plot(model_pa1.numSV,'k')
% hold on
% plot(model_perceptron.numSV,'c')
% plot(model_projectron.numSV,'b')
% plot(model_projectron2.numSV,'m')
% plot(model_rbp.numSV,'r')
% plot(model_forgetron.numSV,'y')
% plot(model_oisvm.numSV,'g')
% legend('PA-I','Perceptron','Projectron','Projectron++','RBP','Forgetron','OISVM','Location','NorthWest')
% grid
% xlabel('Number of Samples')
% ylabel('Number of Support Vectors')
% toc();

	* dump1000b.m: Rerun the poly kernel experiment with the basic
	poly3 feature set:  Best result 7.65->8.38 probably due to removal
	of easy (single choice) instances.  Poly6 no longer has an
	advantage.  Seems each poly degree requires own feature
	optimization.  But the optimal feature set could also change based
	on data size.  It will change yet again with dynamic oracle.

	Loading dump1000b dataset
	Elapsed time is 3.208295 seconds.
	Splitting features
	Using poly kernel on n0,s0,s1,n1,n0l1,s0r1,s0l1,s1r1,s0r
	Elapsed time is 0.411737 seconds.
	degree	last	avg	nsv	time
	1	21.33	14.63	6905	83.23
	2	11.65	8.93	4470	26.64
	3	9.63	8.38	3999	29.65
	4	10.37	8.58	3948	24.89
	5	10.01	9.02	3949	26.10
	6	10.48	9.30	4084	28.22
	7	10.62	9.72	4182	30.28
	8	10.73	9.94	4286	33.43
	9	11.29	10.37	4436	35.86

	* note: rbf kernel does not do as good with poly3 optimized
	features, although we have not optimized gamma=0.1.
	last	avg	nsv	time	feats
	11.32	9.75	4836	457.16	n0,n0l1,n1,s0,s0l1,s0r,s0r1,s1,s1r1

	* featselect-poly3b.out: Is this set still optimal for
	poly3 (done)?  A small improvement is possible by adding
	s1r2+ (8.16) but locally that's it.  Starting featselect from
	scratch to make sure: we have four new features after #4, the
	final result improves from 8.38 to 8.14.

	n	feat	last	avg	nsv	time
	1	n0	48.90	48.37	15094	42.20
	2	s0	18.66	14.47	6231	20.88
	3	s1	12.97	10.75	5205	20.02
	4	n0l1	12.75	10.01	4755	20.32
	5	s0r1r	12.25	9.17	4595	18.69
	6	s1-	11.79	9.15	4391	17.30
	7	n1	11.21	9.02	4023	19.06	# +s0s1-s1- gives 8.60 here
	8	s0s1	10.52	8.67	4064	18.23	# +s0r-s1- gives 8.55 here
	9	s0r	10.91	8.43	4064	18.44
	10	s1r1r	10.07	8.14	3995	19.23

	* dumpfeatures.m (1.2): Checked in the version for experiments
	below as version 1.1.  Version 1.2 does not output transitions for
	which there is a single valid move.

	>> load conllWSJToken_wikipedia2MUNK-50.mat
	>> [x,y]=dumpfeatures(trn_w(1:1000),trn_h(1:1000));
	Elapsed time is 68.892985 seconds.
	x          1768x44217            625405248  double
	y             1x44217               353736  double
	>> save dump1000b.mat x y;

	* ArcHybrid.m (1.2): Checked in the version for the experiments
	below as 1.1.  Version 1.2 adds an exists bit in addition to a
	not-exists bit.

	* featselect-poly3.out: Experiment to see if kernel makes a
	difference in feature selection.  Here is the order of features
	with a poly3 kernel.  There is definitely effect in both
	directions.  Poly 3 improves much faster with fewer features
	compared to poly 6 tops at 9 features (7.65).  Poly3 uses almost
	exclusively word features.

	length	feat1	score1	feat2	score2
	1	n0	47.87
	2	s0	13.89
	3	s1	10.62	s1r	11.39
	4	n0l1	9.54	s0r	9.66
	5	s0r	9.01	s0r1	9.12
	6	s1r1	8.59	s0r2	8.60
	7	n1	8.35	s1r2r	8.62	+n1+s0r1-s1r1	8.49	+n1+s0r1-s0r	8.51	+n1+s0r1-n0l1	8.56
	8	s0r1	7.86	s1r2l	8.08
	9	s0l1	7.65	s0s1	7.67

	* featselect-poly6.out: Here is the order of features selected and the
	closest alternatives: (compare to 9.57 for all 66 features).
	Feature selection seems to be worth 2% points.
	$ perl -lane 'print if scalar(split(/,/,$F[4]))==8' featselect4.out | sort -g -k2,2 | head

	length	feat1	score1	feat2	score2
	1	n0	47.90	n2x	38.54
	2	s0	13.68	?
	3	s1	11.22	s1r	11.67
	4	s0r1	9.95	s0r1r	10.02
	5	n0l1x	9.30	s1r2x	9.39
	6	n0r	9.24	s1r	9.33
	7	s0r1x	9.08	s1r1x	9.08
	8	s1x	8.88	n0l1	8.89	+s1x+n0l1-n0l1x	8.82
	9	n0l1	8.73	s1r1x	8.75	+n0l1+n1-n0l1x	8.28	+n0l1+n1-s1x	8.56
	10	n1	8.26	s1r2x	8.50	+n1+s1r1r-n0l1x	8.19	+n1+s1r1r-n0l1	8.36	+n1+s1r1r-n0r	8.43	+n1+s1r1r-s1x	8.44
	11	s1r1r	7.86	s1r	8.02
	12	s0l1x	7.73	n0l2r	7.77	+s0l1x+s0r-s0r1x	7.77
	13	s0r	7.68	s1r1	7.72	+s0r+s0x-s0l1x	7.64
	14	s0x	7.58	s0s1	7.59	s1r	7.62	s0r2r	7.63

2014-07-08  Deniz Yuret  <dyuret@ku.edu.tr>

	* featselect3.out: Confirmed that s1 is the third feature picked
	when started with n0,s0.

	* featselect2.out: Started from n0,s0,s1 (11.22), search stopped at
	n0,s0,s1,s0r1,n0l1x,n0r (9.24) due to a bug, restarting.  BTW n0r
	should always have the zero bit set (except when there is no n0, in
	which case n0x is set).  So n0r and n0x have the same information.
	But their results look different.  Maybe we should have an exists
	bit instead of (or in addition to) a not exists bit (done).  Why
	should it matter?  6-degree poly?  Other curiosities: n0,s0 pair
	seems to do well, s1 not so necessary.  Start a new search from
	n0,s0 (done, picks s1).  Also curious about the interaction of
	kernel vs feature, rerun feature selection with other kernels to
	see (done).  Do not output instances where parser has a single
	option (done).

	So far no counts, no distances, no children other than s0r1 and
	n0l1 picked.

	* featselect1.out: Just did a single feature select for test.  n2,
	n1 related features did best probably because they can see the end
	of sentence.  Now need a search algorithm.

	* featselect.m: experimenting with feature selection using
	dump1000 dataset and poly 6 kernel.

	last	avg	nsv	time	feats
	10.77	9.57	4848	189.93	all
	12.57	11.22	5530	19.89	n0,s0,s1

	* feature-vector: current feature vector consists of the following
	dimensions.  ni refer to buffer words, si stack words, 0 is
	closest to stack|buffer boundary.  l1 refers to leftmost child, l2
	second leftmost child, similarly for r1, r2.  Each word has 100
	dims of vector + 9 dims of binary features indicating how many
	left/right children word has and whether it is missing:

	[l0 l1 l2 l3+ r0 r1 r2 r3+ missing]

	Last two sets of features are binary indicators for the n0-s0
	distance and s0-s1 distance, the two arc candidates.

	DONE: should scan Zhang08, Nivre11, parser.py, redshift for more
	possible features.  They ZN11 is a superset.  Only uses labels in
	addition to stuff we have.

	0001:0109: n0
	0110:0218: n1
	0219:0327: n2
	0328:0436: s0
	0437:0545: s1
	0546:0654: s2
	0655:0763: s0l1
	0764:0872: s0l2
	0873:0981: s0r1
	0982:1090: s0r2
	1091:1199: s1l1
	1200:1308: s1l2
	1309:1417: s1r1
	1418:1526: s1r2
	1527:1635: n0l1
	1635:1744: n0l2
	1745:1748: n0-s0 dist bits: 1,2,3,4+
	1749:1752: s0-s1 dist bits: 1,2,3,4+

	* kernel-summary: poly kernel degree [4..7] and rbf kernel
	gamma=[0.07..0.11] give best results.  Poly kernel about 5x faster
	than rbf with no significant performance difference (well 9.22 vs
	9.57 may be a bit significant,

	* rbf-kernel: 5x slower with 37K trset, compared to poly-kernel,
	confirming same minimum as 10K:
	gamma	last	avg	nsv	time
	0.06	12.86	9.84	5095	1078.2
	0.07	13.26	9.72	4963	1050.6
	0.08	11.53	9.22	4970	1062.3
	0.09	11.52	9.52	4908	1041.2
	0.10	10.34	9.62	4883	1035.9
	0.12	11.05	10.12	4875	1039.9

	* rbf-kernel: with reduced 10K training set
	gamma	last	avg	nsv	time
	0.01	29.63	20.96	2400	130.91
	0.02	17.79	17.28	2138	120.58
	0.05	15.50	14.40	1786	98.79
	0.06	14.54	14.08	1776	98.05
	0.07	14.48	13.55	1713	93.73
	0.08	15.70	13.52	1727	95.51
	0.09	14.25	13.61	1723	95.25
	0.10	14.74	13.62	1705	94.89
	0.11	15.51	13.63	1744	94.63
	0.12	14.41	13.92	1723	97.52
	0.15	14.85	14.43	1780	99.66
	0.20	15.95	14.94	1881	103.71
	0.50	19.02	19.26	2244	122.13
	1.00	21.79	21.67	2421	132.29
	10.0	23.93	23.75	2565	139.51

	* poly-kernel:
	degree	last	avg	nsv	time
	1	19.92	14.89	7117	263.07
	2	13.33	11.09	5691	208.75
	3	11.56	9.93	5212	207.43
	4	10.93	9.76	4933	194.89
	5	11.67	9.82	4895	190.71
	6	10.77	9.57	4848	189.93
	7	10.83	9.78	4933	196.31
	8	11.68	10.18	5035	201.17
	9	11.44	10.28	5038	203.50

==> run/dump1000.m <==
% path('dogma', path);
%

tic();
fprintf('Loading dump1000 data set\n');
load dump1000.mat;

x_te = x(:,1:10000);
y_te = y(1:10000);
x_tr = x(:,10001:end);
y_tr = y(10001:end);
%x_tr = x(:,10001:20000);
%y_tr = y(10001:20000);
toc();

% for i=1:9
% hp.type = 'poly';
% hp.gamma = 1;
% hp.coef0 = 1;
% hp.degree = i;
% fprintf('Using %dth degree poly kernel\n', hp.degree);

for i=[.07,.09]
hp.type = 'rbf';
hp.gamma = i;
fprintf('Using rbf kernel with gamma=%f\n', hp.gamma);

model_bak = model_init(@compute_kernel,hp);

tic();
%% PERC
% train Perceptron
fprintf('Training Perceptron model...\n');
model_perceptron = k_perceptron_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_perceptron.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_perceptron.beta2, 2));
fprintf('Testing last solution...');
pred_perceptron_last = model_predict(x_te,model_perceptron,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_perceptron_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_perceptron_av = model_predict(x_te,model_perceptron,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_perceptron_av~=y_te))/numel(y_te)*100);
toc();
end
return

%% PA_I
tic();
fprintf('Training PA-I model...\n');
model_pa1 = k_pa_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_pa1.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_pa1.beta2, 2));
fprintf('Testing last solution...');
pred_pa1_last = model_predict(x_te,model_pa1,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_pa1_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_pa1_av = model_predict(x_te,model_pa1,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_pa1_av~=y_te))/numel(y_te)*100);
toc();

%% PA_II
tic();
fprintf('Training PA-II model...\n');
model_pa2 = model_bak;
model_pa2.update = 2;
model_pa2 = k_pa_multi_train(x_tr,y_tr,model_pa2);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_pa2.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_pa2.beta2, 2));
fprintf('Testing last solution...');
pred_pa2_last = model_predict(x_te,model_pa2,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_pa2_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_pa2_av = model_predict(x_te,model_pa2,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_pa2_av~=y_te))/numel(y_te)*100);
toc();


%% PROJ++ eta=0.1
tic();
% train Projectron++
fprintf('Training Projectron++ model...\n');
model_projectron2 = k_projectron2_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_projectron2.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_projectron2.beta2, 2));
fprintf('Testing last solution...');
pred_projectron2_last = model_predict(x_te,model_projectron2,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_projectron2_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_projectron2_av = model_predict(x_te,model_projectron2,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_projectron2_av~=y_te))/numel(y_te)*100);
toc();

%% RBP
tic();
% set maximum number of support vectors for RBP
model_bak.maxSV = size(model_projectron2.beta,2);
% train RBP
fprintf('Training RBP...\n');
model_rbp = k_perceptron_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_rbp.beta, 2));
fprintf('Testing last solution...');
pred_rbp_last = model_predict(x_te,model_rbp,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_rbp_last~=y_te))/numel(y_te)*100);
toc();

% tic();
% fprintf('Plotting error and SV curves...\n');
% % plot error curves
% figure(1)
% plot(model_pa1.aer(100:end),'k')
% hold on
% plot(model_perceptron.aer(100:end),'c')
% plot(model_projectron.aer(100:end),'b')
% plot(model_projectron2.aer(100:end),'m')
% plot(model_rbp.aer(100:end),'r')
% plot(model_forgetron.aer(100:end),'y')
% plot(model_oisvm.aer(100:end),'g')
% grid
% legend('PA-I','Perceptron','Projectron','Projectron++','RBP','Forgetron','OISVM')
% xlabel('Number of Samples')
% ylabel('Average Online Error')

% % plot support vector curves
% figure(2)
% plot(model_pa1.numSV,'k')
% hold on
% plot(model_perceptron.numSV,'c')
% plot(model_projectron.numSV,'b')
% plot(model_projectron2.numSV,'m')
% plot(model_rbp.numSV,'r')
% plot(model_forgetron.numSV,'y')
% plot(model_oisvm.numSV,'g')
% legend('PA-I','Perceptron','Projectron','Projectron++','RBP','Forgetron','OISVM','Location','NorthWest')
% grid
% xlabel('Number of Samples')
% ylabel('Number of Support Vectors')
% toc();


	* dump1000.m, k_perceptron_multi_train: Using first 10000 as test
	and 37716 as train (1 epoch) using all 1752 features.  At 3
	minutes and 90+ accuracy we can use this to do feature and kernel
	optimization.

	#36 SV:13.51(4862)	AER:13.51
	Number of support vectors last solution:4933
	Number of support vectors averaged solution:4933
	Testing last solution...Done!
	10.93% of errors on the test set.
	Testing averaged solution...Done!
	9.76% of errors on the test set.
	Elapsed time is 194.888029 seconds.

	* dump1000.mat: Saved features for first 1000 sentences:
	>> [x,y]=dumpfeatures(trn_w(1:1000),trn_h(1:1000));
	x          1752x46716            654771456  double
	y             1x46716               373728  double
	>> save dump1000 x y
	238705982 Jul  8  2014 dump1000.mat

	* dumpfeatures.m: The transition sequence is NOT unique.  SLR and
	RSL can always be interchanged!  So there are cases where
	oracle_cost has multiple zeros.  Prefer shift first?

	ns=22 nw=9 nx=1011 sptr=2 wptr=4
	stack=1  3
	cost=0  0  1
	heads=
	1  2  3  4  5  6  7  8  9
	6  1  1  5  1  0  8  6  6
	0  1  0  0  0  0  0  0  0

	We do not always have a zero cost move.  When the gold tree is
	projective all moves have positive cost:

	ns=106 nw=15 nx=5490 sptr=3 wptr=8
	stack=2  4  7
	cost=1  1  2
	heads=
	1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
	2   0   2   2   7   7   4   4   8   7  10  14  14  11   2
	2  0  2  0  7  7  0  0  0  0  0  0  0  0  0

	The number of transitions for an n word sentence is 2n-2.

2014-07-07  Deniz Yuret  <dyuret@ku.edu.tr>

	* ArcHybrid.m (valid_moves): Some code optimization.
	Constructor: 3886 sentences/sec
	Shift: 6141 w/s
	Shift+Right: 2680 w/s
	Shift+Left: 3014 w/s
	2xmove,2xhonnibal/w: 592 w/s
	2xmove,2xoracle/w: 1480 w/s
	2xmove,2xoracle,2xhonnibal/w: 468 w/s

	* ArcHybrid.m (update_features): Moved feature calculation in
	class, so feature vector does not need allocating.  This slowed
	down the code from 25 secs to 35 secs.  Creating and filling a new
	feature vector seems faster.  Keep feature calc out!?  Moving
	oracle_cost out did not make much difference.

	* honnibalFeatures.m:
	% Notes:
	% - We replace word-form and part-of-speech with word vector.
	% - We use polynomial kernel to represent feature combinations.
	% - In both left and right moves, s0 will be the child and popped
	% - There is no distance info for the two link candidates n0-s0, s0-s1
	% -- We should add them.
	% - We need indicator variables of when words do not exist.
	% -- Yes.
	% - There is no indication of sentence boundary (first/last word)
	% -- Missing flags should take care of this.
	% - There is no indication of root?
	% -- In general words that get heads get popped.
	% - Does n0 not have right children?
	% -- No.
	% - How about children of s1?  It is also a candidate (for RIGHT).
	% -- We should add them.
	% - Are children in the ldeps, rdeps lists ordered?
	% -- Yes, furthest children at the end.
	% - We should do some feature selection, are these all necessary?
	% - There is no in-between word features.
	% - Compare with graph based features of Zhang&Clark08
	% - Compare with feature engineering in Zhang&Nivre11

2014-07-06  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	+ We should debug honnibalFeatures before writing more code...
	+ Write oracle -- done but check in case.
	+ Write averaged perceptron.
	+ Test with raw features.
	+ Write outer product or kernel. (using dogma).

	* honnibalFeatures.m: Implemented features based on honnibal's
	blog entry which probably comes from:
	Goldberg, Yoav; Nivre, Joakim
	Training Deterministic Parsers with Non-Deterministic Oracles
	TACL 2013

	I added children of s1 as well and the distances n0-s0, s0-s1
	because those are the two current link candidates.  Each word also
	gets a nonexistent flag and left/right child count flags.  Total
	number of features is 109 per word, i.e. 16*109+8=1752.

	Timing for all shift/left transition followed by honnibalFeatures:
	35.5 secs for 1000 sentences 23358 words = 658 w/s.

	* ArcHybrid.m: Need to implement parse state using oop, otherwise
	parse transitions copy object data.  Takes 15 secs to create
	initial parse states 39832 training sentences.  146 secs if we
	initialize 39832 sentences and then shift through each of the
	950028 words.  283 seconds if we shift/left all words.  318
	seconds if we shift/right all words.  So parsing is more than 3000
	w/s.  Next we should look at extracting features, training etc.

2014-07-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* parser.py (Parser.train_one): gold_moves comes back empty when
	sentence non projective.  Modified script to ignore those cases by
	setting best = guess.  Training is roughly 100 words / sec for the
	15 iterations specified in code.  Training on WSJ-02-21 and
	evaluation on WSJ-22 gives 0.9034.  Commands in Makefile.

	* saveData.m: Convert conll data to matlab format for faster
	load.  Reduces load speed from several minutes to 11 seconds.
	Converted conllWSJToken_wikipedia2MUNK-50.mat.

	* loadCONLL.m: Load conll data to cell arrays.

==> run/savedata.m <==
clear;
dir = '/ai/home/vcirik/eParse/run/embedded/conllWSJToken_wikipedia2MUNK-50/';
tic();
[trn_w, trn_h] = loadCoNLL([dir '00/wsj_0001.dp']); % 989860 lines, wsj 02-21
toc(); tic();
[dev_w, dev_h] = loadCoNLL([dir '01/wsj_0101.dp']); % 41817 lines, wsj 22
toc(); tic();
[tst_w, tst_h] = loadCoNLL([dir '02/wsj_0201.dp']); % 191297 lines, wsj 00, 01, 23, 24
toc(); tic();
save('conllWSJToken_wikipedia2MUNK-50');
toc();
