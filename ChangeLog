2014-08-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* gparser.m: object-oriented version of vectorparser.m with the
	various improvements discussed below.
	- Training debug running on @@i30.
	- Test debug running on @@i12.
	

2014-08-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* gparser.m: We need to rethink the interface between various
	scripts.  Create a dbg set and work out the new interface:

	vectorparser_dbg.m
	perceptron_dbg.m
	beamparser_dbg.m
	eval_conll_dbg.m: should be integrated into vectorparser?
	featselect_dbg.m
	eval_model_dbg.m: should be integrated into eval_conll?

	- Forget about dump and input options.  Take whatever information
	from the model, put whatever result into the model.
	
	- We have to figure out how to output multiple paths so both
	vectorparser and beamparser can be evaluated.  There is a
	mincostpath and a maxscorepath, beamparser follows both,
	vectorparser follows one or the other.  Predicted heads are always
	determined by the maxscorepath.  In fact vectorparser only follows
	mincostpath (predict=0) in dump or static-train modes.  But I
	guess we need possible preds as well.  To keep sane we should
	prefix all related to mincost as y, and all related to maxscore as
	z.  So we could possibly have yfeats, ymove, yscore, ycost, (one
	per move), yhead, ysumscore (one per sentence) and their z
	versions.  How about the mincostmove on a zpath or maxscoremove on
	a ypath?  Do not output those.  People can compute them from
	ymove, yscore etc.  The actual ymove, zmove should be the ones
	that are actually executed to get from one state to the next along
	the path.  Careful about feats, it is also used to specify which
	features to use.  Let's rename that fmatrix.

	- vectorparser has 6 steps, here are their prerequisites:
	1. valid:  m.parser
	2. cost:   m.parser, s.head
	3. feats:  m.parser, m.feats, s.wvec
	4. scores: m.SV, m.beta or  m.beta2, feats
	5. update: m.SV, m.beta and m.beta2, feats, cost, scores
	6. move:   m.parser, the rest depends on which move

	- following mincostpath needs 2, 6.
	- following maxscorepath needs 1, 3, 4, 6.
	- test uses maxscorepath.
	- eval also needs 2.
	- train also needs 5 (i.e. all).
	- dump follows mincostpath but also outputs feats (3).
	- 1-3 are on/off.
	- 4, 5 and 6 has different options.
	- 4 options: averaged (beta2) or final (beta) coefficients.
	- 5 options: no-update, and various ways of dealing with ties.
	- 6 options: follow mincost or maxscore or some other if maxscore is invalid.
	- we follow mincost for static train, test.
	- however it is more efficient to dump before static train, test and use perceptron.m.
	- that is one case when we need feats without scores.
	- kernel caching needs to store feats but has to recalculate scores.
	
	- need to design a number of options to determine the choices:
	- which way to move
	- whether/how to update
	- what to dump

	- ok well we could simplify things by dumping everything we use
	- (although for dump mode we don't use features so that has to be an option)
	- average is 0/1 (dogma model_predict argument)
	- move is captured well with predict=0,1 (no dogma equivalent)
	- update is right now 0/1, we could add more types (dogma also has model.update with different semantics)

	- update=1 means 5, which requires the rest, though average and predict can effect 4 and 6.
	- update=0 means no 5, means no 2,3,4.  predict determines 1,2,6 vs 1,3,4,6.
	- (another exception: eval needs cost which also should be an option when update=0).
	- just set whatever extra fields you want (feats,cost) computed extra to empty arrays and they get filled?
	
	update options (not all implemented and tried): (TODO)
	- no update for testing.
	- what to do if maxscoremove is invalid.
	- what to do if there is a single valid move.
	- what to do if there is a tie among maxscore.
	- what to do if there is a tie among mincost.

	+ predict should be a probability! (p=0.9 predicts with prob 0.9)
	+ vectorparser should be a copyable class!  (probably called greedy or deterministic etc.)
	
	do similar analysis for beamparser to decide common names (TODO)

	
	* prioritize:
	- conll07
	-- try rbf featselect: @@b40  (later reoptimize gamma)
	-- try degree=5 fv018a 5epoch: @@b41
	-- try degree>5 featselect: @@b43 (see if better than d5,fv018a)
	- beamparser
	-- try wsjtoken featselect: @@b42 (later optimize kernel)
	-- finish code
	
	* run_5epoch_conll07: Try the better kernel fv018a with degree=5.
	Compare to GN13 hybrid-dynamic 87.62=12.38 and the prev d=3,fv015a run.
	Improvement is possible...

	d=5,fv018a 					d=3,fv015a
	epoch	stat	move	head	word	sent	stat    move    head    word    sent  
	1	0.0437	0.0587	0.1483	0.1340	0.7383	0.0469  0.0586  0.1477  0.1370  0.7664
	2	0.0525	0.0522	0.1299	0.1215	0.7383	0.0524  0.0512  0.1305  0.1229  0.7523
	3	0.0508	0.0493	0.1255	0.1172	0.7477	0.0508  0.0501  0.1259  0.1170  0.7430
	4	0.0499	0.0487	0.1235	0.1154	0.7336	0.0519  0.0503  0.1259  0.1175  0.7477
	5	0.0504	0.0495	0.1245	0.1150	0.7196	0.0513  0.0497  0.1237  0.1150  0.7430
	6	0.0514	0.0488	0.1223	0.1147	0.7336	0.0523  0.0511  0.1271  0.1172  0.7523
	7	0.0511	0.0491	0.1217	0.1136	0.7430	0.0532  0.0512  0.1259  0.1156  0.7477
	8	0.0518	0.0503	0.1237	0.1152	0.7383	0.0535  0.0513  0.1273  0.1177  0.7570
	9	0.0520	0.0497	0.1213	0.1132	0.7243	0.0544  0.0511  0.1267  0.1170  0.7523
	10	0.0519	0.0501	0.1235	0.1152	0.7243	0.0549  0.0516  0.1257  0.1166  0.7477
	
	
	* rbf_kernel: Modifying perceptron_dbg.m to use rbf kernel:
	
	  x_sq = sum(xij.^2, 1);
	  if ns2 == 0
	    s1sq = sum(m.svtr1.^2, 2);
	    val_f = beta1 * exp(-hp.gamma * bsxfun(@plus, x_sq, bsxfun(@plus, s1sq, -2 * (m.svtr1 * xij))));
	    clear s1sq;
	  elseif ns1 == 0
	    s2sq = sum(m.svtr2.^2, 2);
	    val_f = beta2 * exp(-hp.gamma * bsxfun(@plus, x_sq, bsxfun(@plus, s2sq, -2 * (m.svtr2 * xij))));
	    clear s2sq;
	  else
	    s1sq = sum(m.svtr1.^2, 2);
	    s2sq = sum(m.svtr2.^2, 2);
	    val_f = beta1 * exp(-hp.gamma * bsxfun(@plus, x_sq, bsxfun(@plus, s1sq, -2 * (m.svtr1 * xij)))) + ...
	            beta2 * exp(-hp.gamma * bsxfun(@plus, x_sq, bsxfun(@plus, s2sq, -2 * (m.svtr2 * xij))));
	    clear s1sq s2sq;
	  end
	
	Run one epoch:
	
	w0715 =	load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv015a_dump.mat')
	m0 = struct('kerparam', struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
	m0.parser = @archybrid;
	m0.feats = fv015a;
	m0.step = 1e4; 
	m0.batchsize = 1e3;
	m1 = m0;
	m1.kerparam = struct('type', 'rbf', 'gamma', 0.1);
	[m0a,d0a] = perceptron(w0715.trndump.x, w0715.trndump.y, m0);
	nd=1401 nx=763702 nc=3 ns=0
	inst	err	nsv	batch	time	mem	
	763702	7.5355	58050	702	132.87	4.49e+08
	Finding unique SV in 58050...
	Saving 57809 unique SV.
	[m1a,d1a] = perceptron_dbg(w0715.trndump.x, w0715.trndump.y, m1);
	inst	err	nsv	batch	time	mem
	763702	17.5595	134603	171	331.48	3.33e+08
	Finding unique SV in 134603...
	Saving 133915 unique SV.
	
	%%% 132 vs 331 seconds, 2.5x slow down in training not too bad.
	
	[~,~,e0a] = perceptron(w0715.devdump.x, w0715.devdump.y, m0a, 'update', 0)
	inst	err	nsv	batch	time	mem
	92290	4.7340	57809	426	27.84	4.51e+08
	e0a = 0.0473	
	[~,~,e1a] = perceptron_dbg(w0715.devdump.x, w0715.devdump.y, m1a, 'update', 0)
	inst	err	nsv	batch	time	mem
	92290	9.8386	133915	1150	61.50	4.22e+08
	e1a = 0.0984
	
	%%% 61 vs 28 seconds, 2.2x slow down in testing.
	%%% Result a lot worse, but neither gamma nor features have been optimized.
	
	%%% May be worth running featselect for rbf:
	start with archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv084_dump.mat
	use fv008w for initial starting point
	modified featselect_dbg to call perceptron_dbg.
	In fact let's rename these featselect_rbf and perceptron_rbf.
	
	m0 = struct('kerparam', struct('type','rbf','gamma',0.1), 'parser', @archybrid, 'step', 1e6);
	cachefile = 'archybrid_conll07EnglishToken_wikipedia2MUNK-100_rbf_cache.mat';
	w0784 = load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv084_dump.mat');
	featselect_rbf(m0, w0784.trndump, w0784.devdump, cachefile, fv008w);
	@@b40

	(8) 0.122104
	(7) 0.125561
	(9) 0.108809
	(8) 0.109524
	(10) 0.106891

	- This looks kind of hopeless given that poly3 gives 0.0546 in
	epoch=1 with fv008w.  gamma problem?  
	- First confirm 0.0546.  
	19-Aug-2014 23:02:45 # starting(8)	0.0543613	s1w s1r1w s0l1w s0w s0r1w n0l1w n0w n1w 
	- Then optimize gamma using fv008w.  @@b40
	
	[x,fval,exitflag,output] = fminsearch(@(x) f_optimize_gamma(x, x_tr, y_tr, x_te, y_te), 1.0);
	x = 0.537493896484375
	fval = 0.0524108787517608
	exitflag = 1
	output:
	iterations: 20
	funcCount: 48
	algorithm: 'Nelder-Mead simplex direct search'
	message: Optimization terminated:
	  the current x satisfies the termination criteria using OPTIONS.TolX of 1.000000e-04 
	  and F(X) satisfies the convergence criteria using OPTIONS.TolFun of 1.000000e-04 

	- Then rerun featselect if there is hope.
	!rm archybrid_conll07EnglishToken_wikipedia2MUNK-100_rbf_cache.mat
	m0.kerparam.gamma = x
	featselect_rbf(m0, w0784.trndump, w0784.devdump, cachefile, fv008w);
	@@b40
	
	* run/run_conll07_ablation.m:
	= kernel: fselect d=2..9 starting with fv015a+archybrid.
	d=2		d=3		d=4		d=5 		d=6 @@b40/@@b43
	(15) 0.0607758	[15] 0.047340	(15) 0.0465598	(15) 0.0474591	(15) 0.0490844
	(14) 0.0613284	(14) 0.047925	(14) 0.0466464	(14) 0.0475133	(13) 0.0489327
	(15) 0.0592914	(16) 0.047340	(15) 0.0464406	(15) 0.0467981	(14) 0.0477625
	(16) 0.0587713  (17) 0.047611	(16) 0.0455954	(16) 0.0459096	(15) 0.0471665
	(17) 0.0578719			[17] 0.0455629	(17) 0.0454437	(16) 0.0464406
	(18) 0.0575144			(18) 0.0455629	[18] 0.0449453	(17) 0.0462022
	(19) 0.056745 					(19) 0.0449453	[18] 0.0459313
	[20] 0.0566259					
	

	- d=2 hopeless, Restarting from d=4.
	- d=4 better, other degrees? rbf? embeddings?


	* featselect:
	- finish implementing beam parser and compare with ZN11.  First
	step: run featselect starting fv018 on conllWSJToken_wikipedia2MUNK-100.
	- other kernels? d=5 is better in conll07.  test rbf?

	>> run_featselect22('archybrid', 'conllWSJToken_wikipedia2MUNK-100', 'fv130', 'fv018');
	(18) 0.0361038	(16) 0.0357524
	[17] 0.0353489	(15) 0.0359997
	(16) 0.0357524	[17] 0.0353489
	(18) 0.0353750

	% Still same local maximum when started from (16) --b42
	% Should try rbf:
	% set x_tr, y_tr etc. from 'archybrid', 'conllWSJToken_wikipedia2MUNK-100', 'fv008w'.
	[x,fval,exitflag,output] = fminsearch(@(x) f_optimize_gamma(x, x_tr, y_tr, x_te, y_te), 0.5);
	
2014-08-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:

	- run_conll07_15epoch.m:
	+ If this doesn't work:
	- Try other kernel?
	-- Check train vs test, overfitting?
	- Try shuffling?
	- Try other vectorparser update methods?
	- Try p=0.9?
	- Try bansal?
	- Try pa
	- Try to find better feature set?
	-- Try fv031a
	-- Try starting featselect from best 14, 13, etc.
	
	- run/run_conll07_ablation.m:
	+ embedding: fselect bansal100 starting with fv015a+archybrid.
	No significant difference.
	bansal100 best(17): 0.046939
	wiki100   best(15): 0.047339 (fv015a)
	diff: -s0r1c(6) +s1l=(15) +s1r1r<(14) +s1r<(2)
	- features: anything between fv015a and fv031a? starting with 13,14?
	- kernel, degree, rbf?
	- arctype: fselect starting with fv015a
	- perceptron: with or without static training, p=0.9?, update rule?

	- beamparser.m:
	-- implement kernel cache
	-- try pa for faster convergence
	-- compatibility with eval_conll

	= rbf kernel with gpu?
	= Optimize and try rbf kernel calculation and fselect.
	= Other kernel degrees with featselect.
	- look at train/test performance to validate model and decide if
	higher orders are worth trying.

	- vectorparser.m: Debugging the update rule.  --i30
	-- try vectorparser only ignore single choice as well, i.e. three options.
	-- no diff with small data, try bigger data or without static pre-training.

	- featselect_par: --balina. left for later.

	- Implement arceasy.

	- remove model/dump distinction in perceptron etc.  just have one output.
	
	- Embeddings:
	-- test more on conll07.
	-- debug conllWSJToken_dep-100 (good train bad test)
	-- debug out-of-memory on levy, murphy50, mikolovGoogleNews300

	- write matlab gpu tutorial on blog
	OK, another matlab bug: if we have an SV matrix more than half the
	size of gpu memory, we can't add anything to it, we can't change
	it etc. etc. Everything I try (concat, subasgn etc.) ends up
	trying to create a copy.  This is the third bug I found in matlab
	(after max and skinny multiply problems), should send a bug
	report.

	- Old code:
	- trainparser_primal.m: broken, upgrade to use new code.
	- model_sparsify.m: broken, updgreade to use new code.

	--i02 --i03: still oom, do this on balina
	score:0.99 conllWSJToken_levy300 (out of memory at 1405179, sv:126177, nk:11, g:6.4e6)
	score:0.99 conll_levy300 (out of memory at 1596181, sv:165712, nk:263, g:9.8e7)
	score:0.99 conllWSJToken_mikolovGoogleNews300 (out of memory at 1111004, sv:123146, nk:19, g:4.1e6)
	score:0.99 conllWSJToken_murphy50 (out of memory at 1182126, sv:494871, nk:85, g:9.4e7)
	score:0.99 conllWSJToken_murphy50scaled01 (out of memory at 1289463, sv:494698, nk:85, g:9.4e7)



2014-08-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* beamparser_dbg.m:
	+ implement early stopping
	+ implement path construction
	+ implement perceptron update
	+ implement dump update
	+ check beamparser_init
	- implement kernel cache
	- try pa for faster convergence
	- compatibility with eval_conll

	Debugging:

 	w = load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv015a_dump.mat')
	% w.trn: 763702 inst, 398439 word, 16588 sent
	newFeatureVectors;
	m0 = struct('kerparam', struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3),...
                    'parser', @archybrid, 'feats', fv015a, 'step', 1e5, 'batchsize', 1e3);
	m1 = perceptron(w.trndump.x, w.trndump.y, m0);   % 223 secs 57809 unique sv
	m1nox=rmfield(m1,{'x','y'});
	[m2,d2] = vectorparser(m1nox, w.trn(1:200));  % 3.85x/s with cache, 2.65 without, the rest without cache
	[m2a,d2a] = beamparser_dbg(m1nox, w.trn(1:200), 'beam', 1);

	% They do not match because of early stopping!
	% Also beamparser updates at sentence end.
	% Need to compare them in test mode.  The following all match:
	% However beamparser_dbg is much slower, should look into it.

	% Testing with averaged model:
	[m3,d3] = vectorparser(m1nox, w.trn(1:200), 'update', 0, 'predict', 1);  % 3.01x/s
	[m3a,d3a] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 1, 'beam', 1);  % 1.74x/s, 3.01x/s after bsxfun hack

	% This is not identical due to larger beam:
	[m3b,d3b] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 1, 'beam', 10);  % 0.46x/s

	% Testing with final model:
	[m4,d4] = vectorparser(m1nox, w.trn(1:200), 'update', 0, 'predict', 1, 'average', 0);  % 3.06x/s
	[m4a,d4a] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 1, 'beam', 1, 'average', 0);  % 1.66x/s

	% Dumping features:
	[m5,d5] = vectorparser(m1nox, w.trn(1:200), 'update', 0, 'predict', 0);  % 29.2x/s
	[m5a,d5a] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 0, 'beam', 1);  % 17.2x/s

	% This should match (no effect of beam if we are not predicting) but doesn't:
	[m5b,d5b] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 0, 'beam', 10); % 1.47x/s

	% It turns out beam=10 finds better solutions to non-projective sentences.
	% Sentences 10:35 are projective (0 cost) and these match:
	% We shoud output pred in dump mode!
	[m7,d7] = vectorparser(m1nox, w.trn(10:35), 'update', 0, 'predict', 0);  % 39.5x/s
	[m7a,d7a] = beamparser_dbg(m1nox, w.trn(10:35), 'update', 0, 'predict', 0, 'beam', 1);  % 15.1x/s
	[m7b,d7b] = beamparser_dbg(m1nox, w.trn(10:35), 'update', 0, 'predict', 0, 'beam', 10); % 2.04x/s

	% Static oracle training:
	% These should match but don't:
	[m6,d6] = vectorparser(m1nox, w.trn(1:200), 'update', 1, 'predict', 0);  % 2.67x/s
	[m6a,d6a] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 1, 'predict', 0, 'beam', 1);  % 1.55x/s

	% Actually they shouldn't: vectorparser updates after every move,
	beamparser waits until the end of the sentence.  So their training
	is not comparable.  I did confirm however that everything except
	score matches.

	% This doesn't match at all because it picks different oracle y to follow:
	[m6b,d6b] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 1, 'predict', 0, 'beam', 10); % 0.40x/s

	% Starting profile
	[m5,d5] = vectorparser(m1nox, w.trn(1:200), 'update', 0, 'predict', 0);
	[m5a,d5a] = beamparser_dbg(m1nox, w.trn(1:200), 'update', 0, 'predict', 0, 'beam', 1);

	@@i13 @@i30
	DONE: profile
	DONE: use struct array instead of cell array?
	DONE: profile beam=10 testing
	====: make eval_conll work again
	====: kernelcache



	* run_conll07_15epoch.m: Final version of vectorparser with kernel
	caching gets 6 sentences or 153 words per second, which means one
	epoch should be less than an hour!  Now let's run this for 15
	epochs and see what happens.  Until now we always stopped when the
	improvement stopped for one epoch.  Compare to GN13 hybrid-dynamic
	goes to 87.62=12.38 head.

	epoch	stat	move	head	word	sent	nsv(fin-uniq)	time(*)
	1	0.0469	0.0586	0.1477	0.1370	0.7664	63668-63359	255s (static)
	2	0.0524	0.0512	0.1305	0.1229	0.7523	113001-102704	14509s (dynamic)
	3	0.0508	0.0501	0.1259	0.1170	0.7430	143697-129767	13400s
	4	0.0519	0.0503	0.1259	0.1175	0.7477	165416-150682	13881s
	5	0.0513	0.0497	0.1237	0.1150	0.7430	182066-167586	13707s
	6	0.0523	0.0511	0.1271	0.1172	0.7523	195499-181295	13799s
	7	0.0532	0.0512	0.1259	0.1156	0.7477	206538-193049	13616s
	8	0.0535	0.0513	0.1273	0.1177	0.7570	215773-202974	13453s
	9	0.0544	0.0511	0.1267	0.1170	0.7523	223486-211556	13007s
	10	0.0549	0.0516	0.1257	0.1166	0.7477	229936-218751	12937s
	11	0.0545	0.0512	0.1263	0.1168	0.7477	236054-225312	12703s
	12	0.0544	0.0525	0.1301	0.1200	0.7383	240978-230814	8823s
	13	0.0541	0.0513	0.1261	0.1163	0.7383	245332-235830	9401s
	14	0.0551	0.0517	0.1267	0.1172	0.7383	248927-240056	9781s
	15	0.0553	0.0519	0.1271	0.1179	0.7336	252237-243952	10152s

	- No improvement after epoch 5.

	(*) --b41.  Note that the times given are on a half busy gpu,
	around 1.5 sent/sec.  On an empty machine it should run at twice
	this speed.

	- run_conll07_15epoch.mat: models
	- run_conll07_data.mat: data

	+ If this doesn't work:
	- Try other kernel?
	- Try shuffling?
	- Try other vectorparser update methods?
	- Try p=0.9?
	- Try to find better feature set?
	- Try bansal?
	- Try pa
	- Try fv031a
	- Check train vs test, overfitting?
	- Try starting featselect from best 14, 13, etc.


2014-08-16  Deniz Yuret  <dyuret@ku.edu.tr>

	* conllWSJToken:
	- test-split: the 02 directory in conllWSJToken files contain:
	sec	lines			tokens	sentences
	00	48372 (1-48372)		46451	1921 (1-1921)
	01	49626 (48373-97998)	47633	1993 (1922-3914)
	23	59100 (98999-157098)	56684	2416 (3915-6330)
	24	34199 (157099-191297)	32853	1346 (6331-7676)

	- the 00 directory contains contents of 02-21
	- the 01 directory contains contents of 22
	sec	lines	tokens	sentences
	02-21	989860	950028	39832
	22	41817	40117	1700


	* ZN11: Zhang and Nivre, ACL 2011 report 92.9.
	- 02-21 training, 22 devel, 23 testing.
	- using Penn2Malt for dependency conversion.
	- beamsize: 64.
	- sec22 UAS:93.14 LAS:?    UEM:50.12
	- sec23 UAS:92.9  LAS:91.8 UEM:48.0
	- best cited K&C10 model 1: UAS:93.0


	* run_conll07_15epoch.m: Final version of vectorparser with kernel
	caching gets 6 sentences or 153 words per second, which means one
	epoch should be less than an hour!  Now let's run this for 15
	epochs and see what happens.  Until now we always stopped when the
	improvement stopped for one epoch.  Compare to GN13 hybrid-dynamic
	goes to 87.62=12.38 head.

	epoch	stat	move	head	word	sent	nsv(fin-uniq)	time(*)
	1	0.0469	0.0586	0.1477	0.1370	0.7664	63668-63359	255s (static)
	2	0.0524	0.0512	0.1305	0.1229	0.7523	113001-102704	14509s (dynamic)
	3	0.0508	0.0501	0.1259	0.1170	0.7430	143697-129767	13400s
	4	0.0519	0.0503	0.1259	0.1175	0.7477	165416-150682	13881s
	5	0.0513	0.0497	0.1237	0.1150	0.7430	182066-167586	13707s
	6	0.0523	0.0511	0.1271	0.1172	0.7523	195499-181295	13799s
	7	0.0532	0.0512	0.1259	0.1156	0.7477	206538-193049	13616s

	(*) Note that the times given are on a half busy gpu, around 1.5
	sent/sec.  On an empty machine it should run at twice this speed.

	- run_conll07_15epoch.mat: models
	- run_conll07_data.mat: data


2014-08-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* vectorparser.m: Implement kernel cache and test on vectorparser.
	Static training should set most of the SVs and leave a cache for
	the oracle path states using those SVs.  At the beginning of each
	epoch, vectorparser can use the states visited in the last epoch
	and the SVs from the last epoch to construct a new cache in 300
	secs.  The new SVs go into a different matrix
	(use svtr2 again).  Whenever there is a cache hit no need to
	recalculate the scores with the big matrix.  model.X should store
	the instances from the last epoch.  Use mat2str of X vectors for
	keys?

	Before kernel cache:
	w = load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv015a_dump.mat')
	% w.trn: 763702 inst, 398439 word, 16588 sent
	newFeatureVectors;
	m0 = struct('kerparam', struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3),...
                       'parser', @archybrid, 'feats', fv015a, 'step', 1e5, 'batchsize', 1e3);
	m1 = perceptron(w.trndump.x, w.trndump.y, m0);   % 223 secs 57809 unique sv
	m2 = vectorparser(m1, w.trn(1:200));  % 97.58 secs for the first 100 sentences

	After kernel cache:
	>> m1.X = w.trndump.x;
	>> m2b = vectorparser_dbg(m1, w.trn(1:200));
	% 319.54 secs to run the perceptron and compute 763702 cache scores
	% Hopelessly slow trying to cache with mat2str keys.

	For faster cache, use dot product with a random vector:
	>> r = rand(1, 1401);
	>> a = r * w.trndump.x;  % 1,1401 * 1401,763702
	>> size(a)  % 1,763702
	>> min(a)   % -6.9922
	>> max(a)   % 8.9943
	>> mean(a)  % 0.7074
	>> std(a)   % 1.7632 and the distribution is almost gaussian
	>> ux = unique(w.trndump.x', 'rows');  % 756727,1401
	% almost all x vectors are unique
	% how do you map gaussian to uniform?
	>> b = normcdf(a, mean(a), std(a)); % now b is uniform in [0,1]
	>> c = round(b*1e6); % c is uniform int in [0,1e6]
	>> d = unique(c);  % gives 530540 unique values
	>> numel(unique(round(b*1e6)))  % 530540 (70.11%)
	>> numel(unique(round(b*1e7)))  % 728396 (96.26%)
	>> numel(unique(round(b*1e8)))  % 753765 (99.61%)
	>> numel(unique(round(b*1e9)))  % 756470 (99.97%)
	>> numel(unique(round(b*1e10))) % 756710
	>> numel(unique(round(b*1e11))) % 756727 (100.00%)

	Trying again:
	>> [m2c,tmp] = vectorparser_dbg(m1, w.trn(1:200));
	% 326 seconds to compute kernel cache scores
	% 12 seconds to initialize the hash table
	% 65.31 seconds for the first 100 sentences
	% need to profile on an empty machine:
	m2 = vectorparser(m1, w.trn(1:200));  % 36.87 secs for the first 100 sentences
	[m2c,tmp] = vectorparser_dbg(m1, w.trn(1:200));  % 19.65 secs for first 100
	tmp.cache.hit  % 6636
	tmp.cache.miss % 3234

	% it turns out score1 is best computed on the gpu (in case of cache miss)
	% and score2 on the cpu.  At 200 sentences profiling shows:
	score1(gpu,3236): 18.294  score1(cpu):82.748
	score2(cpu,9870): 2.095   score2(gpu):10.794

	% and here is mtimes vs bsxfun:
	score1(gpu,bsxfun): 18.294  score1(gpu,mtimes): 34.101
	score2(cpu,bsxfun): 2.095   score2(cpu,mtimes): 2.179

	% After 12K/18K it is at half speed.  Maybe we should have kept
	score2 in the gpu as well.  How many new SVs are we getting?
	20-50K per epoch.  At that size gpu will win.  Putting everything
	back on gpu.  In a more realistic experiment where svtr1 has 100K
	and svtr2 has 30K support vectors we have for 200 sentences:

	bsxfun:
	score1(gpu,3067):26.591		score1(cpu):181.869
	score2(gpu,9870):34.035		score2(cpu):164.859
	mtimes:
	score1(gpu,3059):52.574 	score1(cpu,3070):396.305
	score2(gpu,9870):59.453 	score2(cpu,9870):547.821

	- no cache: 135.78s for 200sent, nsv: 130242-130067
	- with cache: 78.07s.

	* DONE:
	+ featselect finished, check archybrid_conll07EnglishToken_wikipedia2MUNK-100_cache.mat
	+ check run_conll07_ablation on b40: kernel started.


2014-08-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* GN13: Fig 1 shows that running too many static training
	iterations is not good, they only do it for k=1 epoch.  They do a
	total of 15 epochs (14 dynamic).  Also in dynamic oracle training
	they pick the model move with p=0.9.  Finally they shuffle
	sentences every iteration. (TODO)
	[actually GN13 suggests 1 epoch]
	[Yoav says k=0 and p=1 is also ok, which corresponds to no static]


2014-08-13  Deniz Yuret  <dyuret@ku.edu.tr>

	* beamparser.m: Training. (TODO)
	- We do not update until parse is over.
	- At that point we update if the heads do not match gold.
	- We have the (wrong) z move sequence with its features.
	- We need to compute the correct y sequence with its features.
	- If a state belongs to both y and z and the moves are different we do the usual update.
	- If a state is only in y or in z do we update?  Do we check what the model would have done?
	- Yue says he does update if only in y or z.  Try different variations.

	- early stopping?

	- graph based parser: score agenda states rather than candidate moves?  difference?

	- pa may be worth looking at again for fast convergence.

2014-08-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* beamparser.m: profiling:
	% http://www.mathworks.com/help/matlab/matlab_prog/profiling-for-improving-performance.html
	>> !taskset -pc 0 18893  % set affinity to single cpu
	>> profile -timer real
	>> profile -timer cpu
	% modify score function to return random matrix
	% calling without gpu
	>> profile on
	>> [a,b]=beamparser(m1, wiki.dev(1:10), 'update', 0, 'beam', 10, 'gpu', 0);
	>> profile off
	>> p=profile('info')
	>> profsave(p)
	% profile info saved in html under profile_results.

	* beamparser.m: Computing candidate scores in bulk rather than one
	at a time:

	For 10 sentences, 189 words, 358 moves, 95686 sv, 1401 xdims:
	beam	old	new	wps
	0	8.5	-	22	(vectorparser)
	1	15	12	15.75
	2	23	26	7.27
	5	46	28	6.75
	8	-	29	6.52
	10	86	31	6.10
	16	-	33	5.73
	32	-	40	4.73
	64	475	54	3.50



2014-08-11  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_featselect.m: It does not look like we are going to get a
	better feature set with backward elimination.

	conll07b with bansal100 and fv808 ended with:
	epoch	gmove	move	head	word	sent
	1	0.0492	0.0591	0.1523	0.1374	0.7411	first-static-epoch on dev
	8	0.0425	0.0526	0.1362	0.1235	0.7049	best-static-epoch on dev
	10	0.0458	0.0487	0.1257	0.1157	0.6933	best-dynamic-epoch on dev
	1tst	0.0444	0.0567	0.1417	0.1324	0.7383	first-dynamic-epoch on tst
	0tst	0.0484	0.0540	0.1347	0.1270	0.7383	best-dynamic-epoch on tst

	- GN13 hybrid-static is 86.43=13.57 head.
	- GN13 hybrid-dynamic goes to 87.62=12.38 head.
	There must be better feature/embedding combinations.
	However note also that my experiments are not using the whole training set.

	>> load 'archybrid_conll07EnglishToken_wikipedia2MUNK-100_cache.mat'
	>> [a,b,c] = run_sort_bestfeats(cache);
	best(15): 0.0473399
	>> [find(isfinite(b')) 100*b(isfinite(b))']

	11.0000    5.2476
	12.0000    4.9539	izvlcazmsj (start:5.4892)
	13.0000    4.9149
	14.0000    4.7925
        15.0000    4.7340	izvlcazmsj (switch to diarnxtnht)

	16.0000    4.7340	diarnxtnht (last)
	17.0000    4.7611
	18.0000    4.7611	aefsjihmbv (start:4.8532,done)
	19.0000    4.7925

	28.0000    5.0005
	29.0000    5.0601	vkuvhmnnsm (switch to bxofpzrjcr)
	30.0000    4.8521
	31.0000    4.8304	lvmbqppcab (last)
	32.0000    4.8304
	33.0000    4.8857
	34.0000    4.9409	vkuvhmnnsm (start:5.6658)

	35.0000    5.0406
	36.0000    5.0406
	37.0000    5.0515
	38.0000    5.2281
	39.0000    5.3007	lvmbqppcab (start:5.3007)

	80.0000    7.4093	nslylpeu (last)
	81.0000    7.5729
	82.0000    7.6433
	83.0000    7.8448
	84.0000    7.9976	nslylpeu (start:7.9976)

	* fv015a: is a local minimum.

	See if we can improve over fv015a by testing other features from
	fv130. It turns out not: Numbers below show 1/100 percentage
	points lost (-) or gained (+) from deleting (-) or adding (+) a
	feature to fv015a.
	archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv130_fs.log

	>> load 'archybrid_conll07EnglishToken_wikipedia2MUNK-100_cache.mat'
	>> run_sort_bestfeats(cache) best(15): 0.0473399

	[-201]    '-n0w'   [   0]    '+s0h+'   [  -7]    '+s0ac'   [ -10]    '+s1l1l>' [ -13]    '+s1ac'   [ -17]    '+s0r1-'  [ -22]    '+s2d='   [ -35]    '+s2r1l<' [ -59]    '+n0l1l<'
	[-139]    '-s0w'   [   0]    '+s1h+'   [  -7]    '+n0l1l>' [ -10]    '+s1r1l>' [ -13]    '+s2r>'   [ -17]    '+s1r1c'  [ -24]    '+n0l1-'  [ -37]    '+s1+'    [ -68]    '+s0l1r<'
	[ -71]    '-s1w'   [   0]    '+s2h+'   [  -8]    '+s0l1c'  [ -10]    '+s0r1r>' [ -14]    '+n0l1l=' [ -17]    '+s2l1r=' [ -24]    '+n2w'    [ -37]    '+s1h-'   [ -75]    '+s1l1l<'
	[ -48]    '-n1w'   [  -1]    '+s0r1r=' [  -8]    '+s1l1c'  [ -10]    '+n0l1r>' [ -14]    '+s2l1l=' [ -17]    '+s2l1w'  [ -25]    '+s1l1+'  [ -37]    '+s2+'    [ -80]    '+s1l1r<'
	[ -46]    '-n0l1w' [  -2]    '+s0l1r>' [  -8]    '+s1-'    [ -11]    '+s0aw'   [ -14]    '+s2ac'   [ -17]    '+s0r1r<' [ -25]    '+s0l1-'  [ -37]    '+s2h-'   [ -83]    '+s2l1l<'
	[ -35]    '-s0l1w' [  -2]    '+s0r1l>' [  -8]    '+s2r1w'  [ -11]    '+s2r1c'  [ -14]    '+s2r1l>' [ -17]    '+s1l>'   [ -25]    '+s1r>'   [ -39]    '+n1+'    [ -85]    '+s2l1r<'
	[ -32]    '-s1r1+' [  -3]    '+s0r>'   [  -9]    '+s1r1r=' [ -11]    '+s2r1r=' [ -14]    '+s2r1r>' [ -18]    '+s1aw'   [ -25]    '+s1r1l<' [ -39]    '+s1d>'   [-110]    '+s2l<'
	[ -26]    '-s0r1w' [  -4]    '+s1r1w'  [  -9]    '+s2l1l>' [ -11]    '+s2l1c'  [ -15]    '+s1l1l=' [ -18]    '+s2r='   [ -28]    '+s0l='   [ -39]    '+s2r1r<' [-131]    '+s1l<'
	[ -25]    '-n1c'   [  -5]    '+n0l>'   [  -9]    '+s0l1l>' [ -11]    '+n2-'    [ -15]    '+s2l1+'  [ -18]    '+s1d='   [ -28]    '+s2l>'   [ -43]    '+n0+'    [-134]    '+s2r<'
	[ -19]    '-n0l1c' [  -6]    '+s1l1r=' [ -10]    '+s0-'    [ -12]    '+n1-'    [ -15]    '+s2-'    [ -19]    '+s0r='   [ -29]    '+s2l1-'  [ -43]    '+n2+'    [-145]    '+s1r<'
	[ -18]    '-s2aw'  [  -6]    '+s1l1w'  [ -10]    '+n0l1+'  [ -12]    '+s2l='   [ -15]    '+s1l1r>' [ -20]    '+s2r1l=' [ -29]    '+s1r1r<' [ -44]    '+s0+'    [-157]    '+s0l<'
	[ -17]    '-n0c'   [  -6]    '+s0l1r=' [ -10]    '+n0-'    [ -12]    '+n2c'    [ -16]    '+s0l1+'  [ -20]    '+s0r1l<' [ -32]    '+s1r1-'  [ -44]    '+s0d>'   [-163]    '+n0l<'
	[ -16]    '-s1c'   [  -6]    '+s0r1+'  [ -10]    '+s1r1l=' [ -12]    '+s1r1r>' [ -16]    '+s2r1+'  [ -21]    '+s1l1-'  [ -34]    '+s2d>'   [ -44]    '+s0h-'   [-171]    '+s0r<'
	[ -15]    '-s0c'   [  -6]    '+n0l1r=' [ -10]    '+s2c'    [ -13]    '+s1l='   [ -16]    '+s2l1r>' [ -21]    '+n0l='   [ -34]    '+s2r1-'  [ -50]    '+n0l1r<' [-238]    '+s2d<'
	[  -6]    '-s0r1c' [  -7]    '+s0r1l=' [ -10]    '+s2w'    [ -13]    '+s0l1l=' [ -16]    '+s0d='   [ -21]    '+s0l>'   [ -35]    '+s1r='   [ -58]    '+s0l1l<' [-267]    '+s1d<'
																				       [-300]    '+s0d<'


	* fv031a: is also a local minimum
	run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv130', 'fv031a');
	13-Aug-2014 08:36:39 Loading archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv130_dump.mat


	* beamparser: can we gpu parallelize the beam score calculation?
	we only compute move scores for parent, not the children!?  can we
	do kernel hashing?  Experimenting with conll07b_m2 and dev (fv808
	bansal encoding trn split run_conll07_dbg.m):

	>> load conll07b_m2.mat
	epoch	stat	move	head	word	sent
	10	0.0458	0.0487	0.1257	0.1157	0.6933
	>> load conll07b_dev.mat
	>> tic;[a,b] = beamparser(m2, dev(1:10), 'update', 0, 'beam', 10);toc;
	Elapsed time is 106.003019 seconds.
	>> tic;[c,d] = vectorparser(m2, dev(1:10), 'update', 0);toc;
	Elapsed time is 16.087695 seconds.

	First version of beamparser: can only parse with existing model, no training.

	Way too slow!
	Approx linear with beam size.  Approx eq to vectorparser at beam=1.

	Make it output scores and moves. done.

	Comparison on 100 sentences of dev with vectorparser:
	beamparser made more than twice as many errors, must be bug?

	Looking at the first 10 sentences.  beamparser finds better
	scoring move sequences for 8 of them with beam=10.

	s (len)	vparser(16s)	b1(17s)		b2(28s)		b5(58s)		b10(107s)	b64(604s)
	1 (49)	4.73e+14	4.73e+14	5.65e+14	6.08e+14	6.01e+14	6.31e+14
	2 (5)	2.92e+13	2.92e+13	2.92e+13	2.92e+13	2.92e+13	2.92e+13
	3 (20)	3.72e+14	3.72e+14	3.80e+14	3.85e+14	3.94e+14	3.96e+14
	4 (12)	1.30e+14	1.30e+14	1.30e+14	1.30e+14	1.30e+14	1.34e+14
	5 (29)	5.12e+14	5.12e+14	4.65e+14	5.24e+14	5.28e+14	5.57e+14
	6 (15)	3.75e+14	3.75e+14	4.18e+14	4.18e+14	4.26e+14	4.26e+14
	7 (9)	2.15e+14	2.15e+14	2.15e+14	2.15e+14	2.15e+14	2.15e+14
	8 (8)	1.67e+14	1.67e+14	1.91e+14	2.22e+14	2.22e+14	2.22e+14
	9 (17)	3.86e+14	3.86e+14	3.93e+14	3.90e+14	3.95e+14	4.06e+14
	10 (25)	3.94e+14	3.94e+14	3.99e+14	4.02e+14	4.06e+14	4.06e+14
	head	0.1164		0.1164		0.1693		0.2434		0.2698		0.3175

	+ vectorparser and beam=1 are equal, good.
	+ generally sentence scores increase with beam size, good.
	+ beam=2 is worse than beam=1 in sent=5?
	+ beam=10 is worse than beam=5 in sent=1?
	+ beam=5 is worse than beam=2 in sent=9?
	+ it is totally normal for larger beams to lose the correct answer.
	+ only inf beam guaranteed higher score.

	+ higher beams have progressively worse head error.  unless there
	  is a bug, seems we can't use a model not trained with beamsearch.

	- would it matter if we used only static training?

	>> m1 = run_static('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv015a');
	epoch	nsv	trnerr		deverr
	1	57809	0.0753553	0.0473399
	2	74079	0.0447845	0.0438292
	3	82786	0.0343079	0.0429624
	4	88578	0.0275631	0.0424423
	5	92618	0.0229998	0.0423773
	6	95686	0.0193832	0.0421606	best
	7	98266	0.0169831	0.0422256
	>> wiki = load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_fv015a_dump.mat')
	>> eval_model(m1, wiki.dev, wiki.devdump)
	epoch	stat	move	head	word	sent
	6	0.0422	0.0523	0.1350	0.1203	0.7084
	% compare with conll07b_m2 (bansal100,dynamic10,fv808) stat:4.58 head:12.57
	>> tic;[m10,e10] = beamparser(m1, wiki.dev(1:10), 'update', 0, 'beam', 10);toc;
	Elapsed time is 85.803560 seconds.
	>> eval_conll(wiki.dev(1:10), e10);
	head_pct: 0.2116  sum(e10.s1):1.6775e+09
	>> tic;[m5,e5] = beamparser(m1, wiki.dev(1:10), 'update', 0, 'beam', 5);toc;
	Elapsed time is 46.477789 seconds.
	>> eval_conll(wiki.dev(1:10), e5);
	head_pct: 0.2328  sum(e5.s1):1.6575e+09
	% Beam search helped this time?  It was a fluke.  Here is the full table:
	>> dmp = {e0,e1,e2,e5,e10,e64};
	>> for i=1:10 fprintf('%d',i); for j=1:6 dj=dmp{j}; fprintf('\t%.2e', sentence_score(dj,i)); end; fprintf('\n'); end

	s(len)	vparser(13s)	b1(15s)		b2(23s)		b5(46s)		b10(86s)	b64(475s)
	1	3.67e+08	3.67e+08	3.83e+08	4.17e+08	4.19e+08	4.21e+08
	2	2.69e+07	2.69e+07	2.69e+07	2.69e+07	2.69e+07	2.69e+07
	3	1.97e+08	1.97e+08	2.04e+08	1.99e+08	2.04e+08	2.19e+08
	4	8.10e+07	8.10e+07	8.10e+07	8.10e+07	8.10e+07	8.10e+07
	5	2.73e+08	2.73e+08	2.73e+08	2.81e+08	2.86e+08	2.97e+08
	6	1.45e+08	1.45e+08	1.45e+08	1.45e+08	1.45e+08	1.58e+08
	7	6.80e+07	6.80e+07	7.24e+07	7.24e+07	7.24e+07	7.24e+07
	8	7.01e+07	7.01e+07	7.01e+07	7.01e+07	7.01e+07	7.01e+07
	9	1.10e+08	1.10e+08	1.18e+08	1.18e+08	1.20e+08	1.23e+08
	10	2.26e+08	2.26e+08	2.12e+08	2.47e+08	2.52e+08	2.58e+08
	head	0.1429		0.1429		0.1958		0.2328		0.2116		0.2751

	- figure out how to make it faster.


	* multi-epoch-featselect:
	>> run_static('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv034');
	Try and see if multiple epochs will get rid of irrelevant features. --b42

	This is fv034: (34 features)
	epoch	stat	move	head	word	sent
	1	0.0567	0.0722	0.1842	0.1662	0.7883
	2	0.0490	0.0632	0.1609	0.1448	0.7557
	3	0.0462	0.0600	0.1529	0.1385	0.7401
	4	0.0451	0.0587	0.1498	0.1362	0.7305
	5	0.0446	0.0582	0.1485	0.1346	0.7275	best-epoch
	6	0.0446	0.0583	0.1486	0.1345	0.7285

	Compare to fv808: (18 features)
	1	0.0485	0.0597	0.1518	0.1360	0.7506
	2	0.0453	0.0563	0.1430	0.1284	0.7230
	3	0.0442	0.0549	0.1398	0.1248	0.7119
	4	0.0437	0.0542	0.1375	0.1228	0.7114
	5	0.0435	0.0538	0.1370	0.1224	0.7144

	Doing featselect based on first epoch may not be a good idea!

	Will the larger feature set win with more epochs?
	Another multi-epoch experiment comparing:
        15.0000    4.7340	fv015a
	31.0000    4.8304	fv031a

	fv015a
	epoch	stat	move	head	word	sent
	1	0.0473	0.0580	0.1490	0.1332	0.7441
	2	0.0438	0.0543	0.1399	0.1244	0.7260
	3	0.0430	0.0528	0.1362	0.1209	0.7200
	4	0.0424	0.0524	0.1351	0.1198	0.7114
	5	0.0424	0.0528	0.1360	0.1211	0.7104
	6	0.0422	0.0523	0.1350	0.1203	0.7084	best stat
	7	0.0422	0.0518	0.1347	0.1201	0.7104

	fv031a
	epoch	stat	move	head	word	sent
	1	0.0483	0.0621	0.1587	0.1433	0.7547
	2	0.0448	0.0588	0.1507	0.1357	0.7366
	3	0.0434	0.0562	0.1443	0.1296	0.7240
	4	0.0425	0.0555	0.1424	0.1282	0.7210
	5	0.0421	0.0552	0.1413	0.1272	0.7134	best stat
	6	0.0424	0.0549	0.1416	0.1274	0.7169

	- As anticipated, the larger feature set starts a bit worse and
	closes the gap in multi-epoch, but the advantage is too small to
	be worth trying multi-epoch feature selection.

2014-08-10  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	+ write script to run featselect on short.q.  job id can be random so we can check.
	+ try backward elimination instead of forward selection.
	+ run_static should take corpus, parser name and feature name as args.
	+ check to see if it can load existing mat files.
	+ also compare bansal and wiki on fv808.

	= beam search training may not need dynamic oracle (which cannot be minibatch'ed)!  needs code.
	+ Beam search parser: read paper.  read redshift.  minibatch?
	+ we can start by writing a beam decoder to parse with existing models.  no training.
	+ how often does the gold sequence have a total score better than greedy sequence?


2014-08-09  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_static.m: take corpus, parser name and feature name as args.
	also compare bansal and wiki (and others) on fv808.  running on
	split conll07 trn.

	>> run_static('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv808');

	First Epoch
	epoch	stat	move	head	word	sent	parser
	1	0.0485	0.0597	0.1518	0.1360	0.7506	archybrid
	1	0.0487	0.0592	0.1506	0.1353	0.7536	archybrid13
	1	0.0505	0.0689	0.1649	0.1469	0.7622	arceager13
	1	0.0512	0.0697	0.1682	0.1501	0.7717	arceager

	Fifth Epoch
	epoch	stat	move	head	word	sent	parser
	5	0.0435	0.0538	0.1370	0.1224	0.7144	archybrid (unfinished)
	5	0.0433	0.0539	0.1380	0.1234	0.7174	archybrid13 (unfinished)
	5	0.0454	0.0634	0.1548	0.1379	0.7310	arceager (unfinished)
	5	0.0456	0.0633	0.1527	0.1356	0.7285	arceager13 (unfinished)

	Conclusion: no significant difference with 13 versions.  archybrid
	better but we are not using s0h.  We should try arceager with s0h.

	>> run_static('archybrid', 'conll07EnglishToken_bansal100', 'fv808')
	epoch	stat	move	head	word	sent
	1	0.0492	0.0591	0.1523	0.1374	0.7411
	2	0.0448	0.0551	0.1424	0.1285	0.7134
	3	0.0438	0.0538	0.1386	0.1251	0.7054
	4	0.0431	0.0536	0.1380	0.1246	0.7054
	5	0.0430	0.0531	0.1373	0.1241	0.7044
	6	0.0430	0.0532	0.1374	0.1245	0.7059

	* run_featselect.m: Try backward feature selection.
	Running on split conll07 trn.

	>> run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv084', 'fv018'); aefsjihmbv:done
	>> run_featselect('archybrid', 'conll07EnglishToken_bansal100', 'fv084', 'fv018'); lnaumcolef:stopped
	>> run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv084', 'fv084'); nslylpeu
	>> run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv084', 'fv039'); lvmbqppcab
	>> run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv084', 'fv012'); izvlcazmsj
	>> run_featselect('archybrid', 'conll07EnglishToken_wikipedia2MUNK-100', 'fv084', 'fv034'); vkuvhmnnsm

	The first backtrack immediately gives a relative order of
	features.  However we can get misleading results for redundant
	features.  We need to pick one of +-1 (exists/doesn't), +-9 (head
	exists/doesn't), +-{2,5,6} encodings for children, {3,7,-7}
	encodings for distance, skip 0 and -3 (token features, which are
	concatenations of word+context).  We can try alternatives later.
	fv084 picks = encoding for children and >= encoding for distances,
	but has both bits for 1 and 9.

	The fv084 starting features on wiki100 give 0.0799762 for conll07
	trn split into sec02 dev and the rest trn with archybrid.  This is
	under 0.05 for best feature combination.  A lot of starting
	features have negative effect.

	Here is fv084 (all) vs fv017 (best):			(bansal100)
	best(84): 0.0799762	    best(17): 0.0476108		best(16): 0.0473616
	[-196]    'n0w'   	    [-203]    'n0w'   		[-217]    'n0w'       '[0 0 4]'
	[-119]    's0w'   	    [-125]    's0w'   		[-161]    's0w'       '[-1 0 4]'
	[ -73]    'n1w'   	    [ -59]    's1w'   		[ -74]    's1w'       '[-2 0 4]'
	[ -43]    's1w'   	    [ -59]    'n1w'   		[ -50]    'n1w'       '[1 0 4]'
	[ -20]    'n0l1w' 	    [ -41]    'n0l1w' 		[ -43]    'n0l1w'     '[0 -1 4]'
								[ -26]    's0c'       '[-1 0 -4]'
	[ -20]    'n0c'   	    [ -17]    'n0c'   		[ -23]    's0l1w'     '[-1 -1 4]'
	[ -19]    'n1c'   	    [ -15]    'n1c'   		[ -20]    's1ac'      '[-2 0 -8]'
	[ -16]    's0c'   	    [ -16]    's0c'   		[ -20]    's1r1l='    '[-2 1 -2]'
	[ -11]    's0l1w' 	    [ -21]    's0l1w' 		[ -20]    's0r1w'     '[-1 1 4]'
	[ -10]    's1c'   	    [ -13]    's1c'   		[ -16]    'n0c'       '[0 0 -4]'
								[ -15]    's1c'       '[-2 0 -4]'
	[  -8]    's2l1c' 	    				[ -15]    'n0l1c'     '[0 -1 -4]'
	[  -8]    's0l1c' 	    [ -12]    's0l1c' 		[ -13]    'n1c'       '[1 0 -4]'
	[  -8]    'n0l1c' 	    [ -11]    'n0l1c' 		[  -9]    's0d>'      '[-1 0 7]'
	[  -7]    's2l1r='	    				[  -2]    's0r1c'     '[-1 1 -4]'
	[  -7]    's2aw'
	[  -7]    's1r1+'
	[  -7]    's0ac'
	[  -7]    's0d>'
	[  -6]    's0l1+'
	[  -6]    's0aw'
	[  -5]    's2c'
	[  -5]    's2w'   	    [ -12]    's2w'
	[  -5]    's1l1c'
	[  -5]    's0r1w' 	    [ -21]    's0r1w'
	[  -4]    's2ac'
	[  -4]    'n0l1+'
	[  -3]    's2-'
	[  -3]    's2d>'
	[  -3]    's2r1w'
	[  -3]    's1l1w'
	[  -3]    's1aw'
	[  -3]    's1r1r='
	[  -2]    's1l1l='
	[  -2]    's0l1l='
	[  -1]    's1ac'
	[  -1]    's0l1r='
	[  -1]    's0r1c' 	    [ -12]    's0r1c'
	[   0]    's2r1c'
	[   0]    's0r='
	[   1]    's2l1w'
	[   1]    's2h+'
	[   1]    's2r1+'
	[   1]    's1r1c'
	[   1]    's1r1w'
	[   1]    's0-'
	[   1]    's0r1l='	    [ -19]    's0r1l='
	[   1]    's0r1r='
	[   1]    'n0-'
	[   2]    's2r1l='
	[   2]    's2r1r='
	[   2]    's1l1r='
	[   2]    's1-'
	[   2]    's1d>'
	[   2]    's1h+'
	[   2]    's0h+'
	[   3]    's2l1+'
	[   3]    's2l='
	[   3]    's0l1-'
	[   3]    's0r1+'
	[   3]    'n0l1l='
	[   3]    'n0l1r='
	[   3]    'n1-'
	[   4]    's1l1-'
	[   4]    's1l1+'
	[   5]    's2l1l='
	[   5]    's2l1-'
	[   5]    'n0l='
	[   7]    's1r1l='		[ -30]    's1r1l='
	[   7]    's1r1-'
	[   8]    's2r='
	[   8]    's1r='
	[   9]    's1l='
	[   9]    's0l='
	[   9]    'n0l1-'
	[  11]    's0h-'
	[  11]    's0+'
	[  11]    's0r1-'
	[  12]    'n0+'
	[  14]    's2r1-'
	[  14]    's1+'
	[  14]    'n1+'
	[  15]    's2h-'
	[  15]    's2+'
	[  15]    's1h-'


2014-08-08  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_static: testing other embeddings on conll07 using archybrid
	and fv008w.  Using full conll07 trn (no dev split) and tst in
	multi-epoch static training.  wiki is better than bansal?!

	epoch	stat	move	head	word	sent	corpus
	5	0.0470	0.0574	0.1481	0.1333	0.7523	conll07EnglishToken_wikipedia2MUNK-100
	7	0.0470	0.0603	0.1501	0.1372	0.7897	conll07EnglishToken_rcv1UNK100
	4	0.0477	0.0603	0.1505	0.1395	0.7430	conll07EnglishToken_bansal100
	4	0.0481	0.0594	0.1503	0.1372	0.7617	conll07EnglishToken_wikipedia2MUNK-50
	6	0.0517	0.0639	0.1573	0.1465	0.7617	conll07EnglishToken_stratos100k5000scaled01
	4	0.0545	0.0656	0.1679	0.1560	0.7664	conll07EnglishToken_cw100scaled
	4	0.0578	0.0696	0.1763	0.1594	0.7570	conll07EnglishToken_hlbl100scaled
	9	0.0607	0.0755	0.1909	0.1821	0.8411	conll07EnglishToken_stratos100k200scaled01
	11	0.0640	0.0797	0.2029	0.1878	0.8318	conll07EnglishToken_hpca100scaled01

	* run_max_beam_score.m: We can calculate an upper bound on beam
	parser for an existing model by comparing the total score of the
	correct tree with the total score of the predicted tree.  Using
	the last static model from mxrep.

	>> load mxrep_m1
	>> load mxrep_dev
	% confirmed stat=3.1627 on dev.
	>> [~,devparse] = vectorparser(m1, dev, 'update', 0);
	>> eval_conll(dev, devparse);
	% confirmed move=3.85 head=10.09
	>> [~,devdump.score] = perceptron(devdump.x, devdump.y, m1, 'update', 0);
	% mxrep_devdump does not have scores
	>> [sy,sz,sumy,sumz] = run_max_beam_score(dev, devdump, devparse);
	>> numel(find(sy>sz))/numel(sy) % 0.4895: pct of moves where gold score is better than guess
	>> numel(find(sy==sz))/numel(sy) % 0.0023
	>> numel(find(sy<sz))/numel(sy) % 0.5082
	>> numel(find(sumy>sumz))/numel(sumy) % 0.4465: pct of sentences where total gold score is better than guess
	>> numel(find(sumy==sumz))/numel(sumy) % 0.0082
	>> numel(find(sumy<sumz))/numel(sumy) % 0.5453

	This means our answers would change (at least) in 44% of the
	sentences, possibly for the better.

	* vectorparser.m: Debugging the update rule. --i30

	- trainparser_gpu also have score(c=inf)=-inf, why did that not hurt?
	c = p.oracle_cost(h);
	scores(c==inf) = -inf;
	[~,move] = max(scores);
	[mincost, bestmove] = min(c);
	if c(move) > mincost % update

	- vectorparser has:
	cost = p.oracle_cost(h);
	[mincost, mincostmove] = min(cost);
	[maxscore, maxscoremove] = max(score);
	if cost(maxscoremove) > mincost % update

	- Experiment with conll07 dev vs tst for speed.  Using fv808.
	- Static training.  13-epoch, 6.6820 tst err.
	- vectorparser Dynamic training.  1-epoch:
	epoch	stat	move	head	word	sent
	1	0.0685	0.0776	0.1935	0.1782	0.7991
	- vectorparser_dbg (same as trainparser_gpu), 1-epoch:
	epoch	stat	move	head	word	sent
	1	0.0689	0.0782	0.1949	0.1798	0.7991
	- confirm same as trainparser_gpu... done.
	- vectorparser 5-epochs: (no help at all?)
	epoch	stat	move	head	word	sent
	0	0.0668	0.0764	0.1903	0.1769	0.7850
	1	0.0685	0.0776	0.1935	0.1782	0.7991
	2	0.0690	0.0787	0.1951	0.1800	0.7991
	3	0.0680	0.0783	0.1937	0.1794	0.7991
	4	0.0683	0.0780	0.1941	0.1800	0.7944
	5	0.0687	0.0784	0.1951	0.1807	0.8037
	- vectorparser_dbg 5-epochs:
	epoch	stat	move	head	word	sent
	0	0.0668	0.0764	0.1903	0.1769	0.7850
	1	0.0689	0.0782	0.1949	0.1798	0.7991
	2	0.0687	0.0781	0.1941	0.1791	0.7991
	3	0.0686	0.0790	0.1963	0.1810	0.7991
	4	0.0688	0.0787	0.1955	0.1805	0.8037
	5	0.0684	0.0780	0.1943	0.1789	0.8037

	- At this size dynamic oracle training does not seem to help.  To
	see the difference we need a bigger size problem.



==> run_max_beam_score.m <==
function [sy, sz, sumy, sumz] = run_max_beam_score(dev, devdump, devparse)
y = devdump.y;
z = devparse.z;
sy = devdump.score(sub2ind(size(devdump.score), y, 1:numel(y)));
sum_sy = sum(sy)
sz = devparse.score(sub2ind(size(devparse.score), z, 1:numel(z)));
sum_sz = sum(sz)

ns = numel(dev)
nw = 0;
nt = 0;
for s=1:ns
  w = numel(dev{s}.head);
  t = 2*w-2;
  at = nt + 1;
  bt = nt + t;
  sumy(s) = sum(sy(at:bt));
  sumz(s) = sum(sz(at:bt));
  nw = nw + w;
  nt = nt + t;
end



==> run_sort_bestfeats.m <==
function fsorted = run_sort_bestfeats(cache, imin)

nfeats = 199;
bestfeats = cell(1,nfeats);
besterror = inf(1,nfeats);
cachekeys = keys(cache);
for i=1:numel(cachekeys)
  fstr = cachekeys{i};
  ferr = cache(fstr);
  flen = size(eval(fstr), 1);
  if ferr < besterror(flen)
    besterror(flen) = ferr;
    bestfeats{flen} = fstr;
  end
end

if nargin < 2
  [emin, imin] = min(besterror);
end
emin = besterror(imin);
fmin = eval(bestfeats{imin});
fprintf('best(%d): %g\n', imin, emin);

for i=1:imin
  fsorted{i,1} = 0;
  fsorted{i,2} = fsymbol(fmin(i,:));
  fnew = fmin;
  fnew(i,:) = [];
  fk = mat2str(sortrows(fnew));
  if isKey(cache, fk)
    ferr = cache(fk);
    fsorted{i,1} = round(10000 * (emin - ferr));
  end
end

fsorted = sortrows(fsorted, 1);

end


2014-08-07  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_fs_conll07_arceager13.m: featselect with arceager13 starting
	at fv008 using fv136 (head features), conll07 trn split into trn
	and dev, bansal100 encoding. (--b43)


	* run_fs_conll07_aug7: featselect stopped and restarted from a
	better initial state: fv008.  Still using conll07a_ variables
	(conll07 trn split into dev and trn, bansal100 token encoding).
	Get the new head features in there: fv136.  Eliminate features
	never found useful from fv130: decided not to.  Try eager/eager13
	as well.  Remove single-choice moves from training:done. Speed is
	about 100 secs per feature.  We can try 864 features a day.  That
	decides roughly six features a day.  Started at Aug 7 10:00AM.
	(--b42)

	dev 1989 sent -> 92290 move -> 87202 multi-choice move
	trn 16588 sent -> 763702 move -> 720961 multi-choice move

	- Compare basic feature set with other encoding and other
	arctypes.  (maybe with the addition of s0r=(helps .1%) and s0h(not
	for hybrid) and the constant 1(not helpful)).  Maybe best to wait
	and do an actual fselect.

	- NOTE: we do not have a constant feature but we do have features
	that are always on that will serve the same purpose: s0h- for
	archybrid, s0+ are some of them.  It is probably not a good idea
	to rely on these and we should include them explicitly.  Tried
	adding s0+ to fv008, did not improve.

	- NOTE: fv008 gave 0.0591 on conll07.  fv808 gave 0.0492 first
	epoch on conll07.  It gave 0.0371 on its original data
	conllToken_wikipedia2MUNK-50.  bansal5k experiments used 5k
	sentences from conllWSJToken_bansal100 and got 0.0504 on its own
	dev set. fv808 actually has 1608 dims in this case because
	bansal100 has 100 dim vectors and it includes the context vectors
	for each.  In addition to the 6 basic words in fv008, it has s0l1
	and s2, all the contexts, plus two counts: s1r1l and s0r1l.

	- NOTE: The first feature picked to add to fv008 is s1r1w, which
	was not picked in the previous featselect (but was part of fv804).
	It got rid of both n0c and s0c and replaced them with word
	features s1r1w and s0l1w.  Hopefully this will discover a good
	combo for conll07.

	>> load 'fs_conll07_aug7.mat' (archybrid)
	>> run_sort_bestfeats(cache)
	best(11): 0.0515011   	best(12): 0.0509621	best(13): 0.0502282	best(14): 0.0498842
	[-367]    'n0w'       	[-360]    'n0w'  	[-359]    'n0w'
	[-238]    's0w'       	[-229]    's0w'  	[-230]    's0w'
	[-106]    's1w'       	[-100]    's1w'  	[-104]    's1w'
	[ -94]    'n1w'       	[ -94]    'n1w'  	[ -82]    'n1w'
	[ -81]    'n0l1w'     	[ -63]    'n0l1w'	[ -65]    'n0l1w'
	[ -39]    's1r1w'     	[ -41]    's0r1w'	[ -45]    's1r1w'
	[ -26]    's0r1w'     	[ -36]    's1r1w'	[ -31]    's0l1w'
	[ -26]    'n0c'       	[ -25]    's1c'  	[ -30]    'n0c'
	[ -25]    's1c'       	[ -20]    's0l1w'	[ -28]    's0r1w'
	[ -24]    's0l1w'     	[ -16]    'n0c'  	[ -26]    's1c'
	[  -8]    's0r1l<'    	[ -12]    's2l=' 	[ -15]    's2l='
				[ -12]    'n0l>' 	[  -8]    'n0l>'
							[  -7]    's0r1l<'

	>> load 'fs_conll07_arceager13.mat'
	>> run_sort_bestfeats(cache)
	best(9): 0.0560545	best(10): 0.0542881	best(11): 0.0538064	best(12): 0.0534164	best(13): 0.0526593
	[-494]    'n0w'   	[-510]    'n0w'   	[-502]    'n0w'   	[-507]    'n0w'
	[-324]    's0w'   	[-337]    's0w'   	[-313]    's0w'   	[-268]    's0w'
	[-181]    'n1w'   	[-187]    'n1w'   	[-189]    'n1w'   	[-184]    'n1w'
	[-179]    's1r1r<'	[-177]    's1r1r<'	[-175]    's1r1r<'	[-155]    's1r1r<'
	[ -71]    'n0l1w' 	[ -54]    'n0l1w' 	[ -54]    's1w'   	[ -45]    'n0l1w'
	[ -36]    's1w'   	[ -53]    's1w'   	[ -45]    'n0l1w' 	[ -32]    's0r1w'
	[ -32]    's0r1w' 	[ -42]    's0r1w' 	[ -41]    's0l1w' 	[ -31]    'n2c'
	[ -15]    's0l1w' 	[ -36]    'n2c'   	[ -39]    's0r1w' 	[ -28]    's0l1w'
	[ -15]    'n2c'   	[ -25]    's0l1w' 	[ -35]    'n2c'   	[ -24]    's1w'
				[ -18]    'n0l1r<'	[ -12]    'n0l1r<'	[ -16]    'n0l1r<'
							[  -5]    'n0l1c' 	[ -14]    'n0l1c'
										[  -4]    's1c'


	* conll07b: rerun conll07 experiments with fv808 and new code.  bansal100 encoding.
	- try scode instead of bansal?
	- g&n13 hybrid-static is 86.43=13.57 head.
	- g&n13 hybrid-dynamic goes to 87.62=12.38 head.
	- feature optimization for conll07 & difft arctypes skipping single-choice.
	- fix featselect so it doesn't backup shorter than initfeats.
	- use the whole training set after determining epochs and feats.
	--b41

	Dynamic oracle:
	epoch	gmove	move	head	word	sent
	(8)	0.0425	0.0526	0.1362	0.1235	0.7049 (last static epoch)
	1tst	0.0444	0.0567	0.1417	0.1324	0.7383 (1st dynamic epoch on test set)
	1	0.0451	0.0535	0.1389	0.1262	0.7205 rest of the results on dev set
	2	0.0451	0.0522	0.1351	0.1229	0.7094
	3	0.0456	0.0518	0.1337	0.1221	0.7084
	4	0.0456	0.0510	0.1320	0.1205	0.7074
	5	0.0454	0.0504	0.1303	0.1193	0.7019
	6	0.0458	0.0502	0.1295	0.1189	0.7014
	7	0.0459	0.0500	0.1288	0.1183	0.7024
	8	0.0458	0.0495	0.1277	0.1174	0.6983
	9	0.0455	0.0490	0.1265	0.1164	0.6948
	10	0.0458	0.0487	0.1257	0.1157	0.6933	best-dynamic-epoch
	11	0.0460	0.0489	0.1266	0.1166	0.6928
	0	0.0484	0.0540	0.1347	0.1270	0.7383	final-testing

	* mxrep: replicate mx experiments with new code.  Note that the
	difference in the last static model is due to the older experiment
	removing single choice moves from training.  In dynamic training
	older code ignored scores of impossible moves completely.  One
	epoch takes about 7 hours which is consistent with before. --b40.

	Dynamic oracle:
	epoch	gmove	move	head	word	sent
	(4)	0.0316	0.0385	0.1009	0.0887	0.6165 (last static epoch)
	1	0.0384	0.0376	0.0997	0.0883	0.6300
	2	0.0382	0.0368	0.0977	0.0865	0.6200
	3	0.0383	0.0359	0.0955	0.0843	0.6165
	4	0.0382	0.0350	0.0930	0.0822	0.6118
	5	0.0382	0.0346	0.0925	0.0816	0.6100
	6	0.0381	0.0344	0.0917	0.0808	0.6065
	7	0.0382	0.0344	0.0916	0.0811	0.6076
	8	0.0383	0.0342	0.0904	0.0800	0.6071
	9	0.0381	0.0339	0.0897	0.0792	0.6059
	10	out-of-memory

	Compare to original mx results:
	--i11			TEST				TRAIN
	model	nsv	time	gtrans	ptrans	phead	whead	ptrans	phead	whead
	mx5	169135	963	3.34	3.76	9.83	8.63	-	-
	mx5d1	204369	22472	4.07	3.78	10.02	8.83	3.02	8.37	7.42
	mx5d2	232151	?	4.02	3.63	9.59	8.47	2.73	7.60	6.77
	mx5d3	254535	?	3.99	3.49	9.23	8.16	2.45	6.88	6.13
	mx5d4	273226	?	4.00	3.47	9.20	8.12	2.23	6.28	5.61
	mx5d5	289077	?	3.99	3.39	9.02	7.98	2.02	5.73	5.11
	mx5d6	302335	?	3.99	3.39	8.96	7.92	1.83	5.24	4.67
	mx5d7	314292	?	4.00	3.37	8.90	7.88	1.71	4.92	4.40

==> run/run_conll07_dbg.m <==
% function run_conll07_dbg(parser, feats, trn, dev, tst, savefile)
function run_conll07_dbg(savefile)

if 0
fprintf('%s saving arguments\n', datestr(now));
save([savefile '_parser'], 'parser', '-v7.3');
save([savefile '_feats'], 'feats', '-v7.3');
save([savefile '_trn'], 'trn', '-v7.3');
save([savefile '_dev'], 'dev', '-v7.3');
save([savefile '_tst'], 'tst', '-v7.3');

fprintf('%s initializing model\n', datestr(now));
m0.kerparam = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
m0.parser = parser;
m0.feats = feats;
m0.step = 1e5;
m0.batchsize = 1e3;
save([savefile '_m0'], 'm0', '-v7.3');

fprintf('%s preparing dumps.\n', datestr(now));
[~,trndump] = vectorparser(m0, trn, 'predict', 0, 'update', 0);
save([savefile '_trndump'], 'trndump', '-v7.3');
[~,devdump] = vectorparser(m0, dev, 'predict', 0, 'update', 0);
save([savefile '_devdump'], 'devdump', '-v7.3');
[~,tstdump] = vectorparser(m0, tst, 'predict', 0, 'update', 0);
save([savefile '_tstdump'], 'tstdump', '-v7.3');

fprintf('%s static oracle training.\n', datestr(now));
fprintf('epoch\tgmove\tmove\thead\tword\tsent\n');
m1 = m0;
e1 = inf;
for epoch=1:100
  m = perceptron(trndump.x, trndump.y, m1);
  [e, f] = eval_model(m, dev, devdump, epoch);
  if e < e1
    e1 = e;
    f1 = f;
    m1 = m;
    save([savefile '_m1'], 'm1', '-v7.3');
  else
    break;
  end
end

else % if 0
% load([savefile '_parser']);
% load([savefile '_feats']);
load([savefile '_trn']);
load([savefile '_dev']);
load([savefile '_tst']);
% load([savefile '_m0']);
% load([savefile '_trndump']);
load([savefile '_devdump']);
load([savefile '_tstdump']);
end % if 0

fprintf('%s dynamic oracle training.\n', datestr(now));
load([savefile '_m2']);
[e2, f2] = eval_model(m2, dev, devdump, 1);

for epoch=2:100
  m2save = m2;
  m2 = vectorparser(m2, trn);
  save([savefile '_m2'], 'm2', '-v7.3');
  [e, f] = eval_model(m2, dev, devdump, epoch);
  if f.head_pct < f2.head_pct           % this can go bad first iteration
    f2 = f;
  else
    break;
  end
end

m2 = m2save;
save([savefile '_m2'], 'm2', '-v7.3');

fprintf('%s final testing.\n', datestr(now));
eval_model(m2, tst, tstdump, 0);

end % main

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [e, f] = eval_model(m, dev, devdump, epoch)
[~,s] = perceptron(devdump.x, devdump.y, m, 'update', 0);
[~,z] = max(s);
e = numel(find(z ~= devdump.y))/numel(z);
[~,mdev] = vectorparser(m, dev, 'update', 0);
f = eval_conll(dev, mdev)
fprintf('%d\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\n', epoch, e, ...
        f.move_pct, f.head_pct, f.word_pct, f.sent_pct);
end % eval_model


==> run/run_sort_bestfeats.m <==
function fsorted = run_sort_bestfeats(cache)

nfeats = 199;
bestfeats = cell(1,nfeats);
besterror = inf(1,nfeats);
cachekeys = keys(cache);
for i=1:numel(cachekeys)
  fstr = cachekeys{i};
  ferr = cache(fstr);
  flen = size(eval(fstr), 1);
  if ferr < besterror(flen)
    besterror(flen) = ferr;
    bestfeats{flen} = fstr;
  end
end

[emin, imin] = min(besterror)
fmin = eval(bestfeats{imin});

for i=1:imin
  fsorted{i,1} = 0;
  fsorted{i,2} = fsymbol(fmin(i,:));
  fnew = fmin;
  fnew(i,:) = [];
  fk = mat2str(sortrows(fnew));
  if isKey(cache, fk)
    ferr = cache(fk);
    fsorted{i,1} = round(10000 * (emin - ferr));
  end
end

fsorted = sortrows(fsorted, 1);

end


==> run_fs_conll07_arceager13.m <==
load mat/conll07a_trn.mat
load mat/conll07a_dev.mat
whos trn dev

newFeatureVectors;
load mat/conll07a_m0.mat
m0.feats = fv136;
m0.parser = @arceager13;

[~,devdump] = vectorparser(m0, dev, 'update', 0, 'predict', 0)
dev_finite_count = sum(isfinite(devdump.cost));
devdump.x = devdump.x(:,dev_finite_count > 1);
devdump.y = devdump.y(:,dev_finite_count > 1);
devdump.cost = devdump.cost(:,dev_finite_count > 1)
save('run_fs_conll07_arceager13_devdump', 'devdump', '-v7.3');

[~,trndump] = vectorparser(m0, trn, 'update', 0, 'predict', 0)
trn_finite_count = sum(isfinite(trndump.cost));
trndump.x = trndump.x(:,trn_finite_count > 1);
trndump.y = trndump.y(:,trn_finite_count > 1);
trndump.cost = trndump.cost(:,trn_finite_count > 1)
save('run_fs_conll07_arceager13_trndump', 'trndump', '-v7.3');

featselect_gpu(m0, trndump, devdump, 'fs_conll07_arceager13.mat', fv008);


==> run_fs_conll07_aug7.m <==
load mat/conll07a_trn.mat
load mat/conll07a_dev.mat
whos trn dev

newFeatureVectors;
load mat/conll07a_m0.mat
m0.feats = fv136;

[~,devdump] = vectorparser(m0, dev, 'update', 0, 'predict', 0)
dev_finite_count = sum(isfinite(devdump.cost));
devdump.x = devdump.x(:,dev_finite_count > 1);
devdump.y = devdump.y(:,dev_finite_count > 1);
devdump.cost = devdump.cost(:,dev_finite_count > 1)
save('run_fs_conll07_aug7_devdump', 'devdump', '-v7.3');

[~,trndump] = vectorparser(m0, trn, 'update', 0, 'predict', 0)
trn_finite_count = sum(isfinite(trndump.cost));
trndump.x = trndump.x(:,trn_finite_count > 1);
trndump.y = trndump.y(:,trn_finite_count > 1);
trndump.cost = trndump.cost(:,trn_finite_count > 1)
save('run_fs_conll07_aug7_trndump', 'trndump', '-v7.3');

featselect_gpu(m0, trndump, devdump, 'fs_conll07_aug7.mat', fv008);



2014-08-06  Deniz Yuret  <dyuret@ku.edu.tr>

	* mxrep: replicate mx experiments with new code. --b40.  Static oracle
	results are better in gmove but .25 worse in parsing performance.
	Difference?  Not using single option moves for training?  Some
	other change from dumpfeatures/trainparser to vectorparser?

	Static oracle:
	epoch	gmove	move	head	word	sent
	1	0.0363	0.0431	0.1129	0.1002	0.6624
	2	0.0334	0.0399	0.1048	0.0929	0.6329
	3	0.0322	0.0389	0.1021	0.0898	0.6241
	(4)	0.0316	0.0385	0.1009	0.0887	0.6165
	5	0.0318	0.0386	0.1012	0.0887	0.6176

	Compare to the original mx Static Oracle:
	model	aer	time	nsv	gtrans	ptrans	phead	whead	notes
	mx1	6.09	314	104757	3.75	4.26	11.18		fv808 feature-set batch=1000
	mx2	4.89	727	133856	3.50	3.95	10.36		all mx are fv808 batch=1000
	mx3	4.11	860	150207	3.38	3.80	 9.96
	mx4	3.54	930	161208	3.36	3.80	 9.88
	mx5	3.21	963	169135	3.34	3.76	 9.83	8.63	best epoch for fv808: gtrans:3.34 phead:9.83 whead:7.60

	- try removing single choice moves, results should be identical
	-- instances go from 1820392 to 1720986
	-- first epoch ends with 103524 sv (103062 unique).
	-- dev error is 5.3180?
	-- eliminate single choice moves from dev as well. 76384 -> 72555.
	-- recovered gtrans=3.7392.  (did we do this on cost based perceptron? doesn't matter it screwed up on parsing)
	-- vectorparser on dev: move:4.26 head:11.20 word:9.84 sent:65.88.

	- this works because vectorparser is not asking the perceptron
	anything for single choice moves.  so perceptron would be wasting
	its capacity on useless instances.  for multichoice moves it
	shouldn't matter what score is assigned to inf cost moves, since
	vectorparser is not going to ask about them either?

	-- ok try to replicate the above result with cost based.
	perceptron ignoring the single-choice moves itself.  Gives us
	103286 sv.  3.77 gtrans.  It won't be exactly the same because
	minibatch borders will shift.  We can use the 1720986 trn to get
	exactly identical result.  It does.  Now add ignoring scores for
	inf cost moves.  102510 sv.  6.86 gtrans.  Of course this is not
	eliminating the inf moves like in training.  Parser does, so
	vectorparser comparison more meaningful.  vectorparser on dev:
	move:4.29 (+0.03) head:11.36 (+0.16) word:9.90 (+0.06) sent:66.41
	(+0.53).  OK it gets worse, don't ask again.

	Conclusion: it gains a bit to ignore single choice moves.  it does
	not gain to ignore the score given to inf cost moves in the
	multi-choice case.


	* conll07b: rerun conll07 experiments with fv808 and new code.  --b41.
	>> run_conll07_dbg('conll07b');

	Static oracle:
	epoch	gmove	move	head	word	sent
	1	0.0492	0.0591	0.1523	0.1374	0.7411
	2	0.0448	0.0551	0.1424	0.1285	0.7134
	3	0.0438	0.0538	0.1386	0.1251	0.7054
	4	0.0431	0.0536	0.1380	0.1246	0.7054
	5	0.0430	0.0531	0.1373	0.1241	0.7044
	6	0.0430	0.0532	0.1374	0.1245	0.7059
	7	0.0426	0.0529	0.1367	0.1238	0.7039
	(8)	0.0425	0.0526	0.1362	0.1235	0.7049
	9	0.0426	0.0526	0.1362	0.1235	0.7029


	* rerun: featselect for conll07, archybrid, fv130, new train/dev
	split.  (--b42) Note that fv130 does not include the new head
	feature but that is not useful for archybrid.

	* DONE:
	- copy run scripts to ChangeLog.
	+ move run_* to logs. split log/run/mat.
	+ stop featselect.
	+ test 10-epoch best model for parsing
	+ start dynamic oracle training
	+ figure out conll07 diff,
	+ rerun featselect.
	+ rinse and repeat.

	* DONE:
	- severe connections with dogma:
	- remove model.n_cla and other useless fields from vectorparser.
	- edit perceptron to do testing as well.

	* DONE: add a head feature for arceager and redo feature selection.

	* DONE: correct perceptron update rule when there can be more than
	one maxscore and more than one mincost answer.

	* DONE: perceptron.m:
	+ experiment with cost vs answer training.
	+ skipping single choice answers?
	? arceager vs archybrid parse error - move error relation?
	+ write testing function for perceptron instead of model_predict_gpu.
	+ run_arctype_comparison should dump first instead of training.

	* DONE:
	x sparsification: isn't necessary, compactify is enough.
	+ multi-epoch: ~10-12 static epochs seem sufficient, 5 epochs for fv808.
	+ features: fv808 converges faster but same static performance.
	+ dynamic oracle: it starts working after epoch 2, best at 5-10 epochs
	+ embeddings: bansal seems best.
	+ arctype
	? beam-search
	? kernel-type

	* DONE: test if saving single-choice moves in dump makes a big
	difference in static oracle training, if it does find a way to
	filter the multi-choice instances.

	* DONE:
	+ Let --i00 finish so we can use the dumps.
	- Verify the dumps.
	- Run regular perceptron training.
	- Figure out the difference between PTB and conll.
	- Figure out if we need separate featselect for conll.

	* DONE: in fact, why don't we use the scaled averaged beta during
	training?  If we know it is better it will make less mistakes,
	accumulate fewer support vectors.  (Try this in perceptron.m after
	debugging). -- did not work.


==> run/run_conll07_debug.m <==
% % function run_conll07(parser, feats, trn, dev, tst, savefile)

%% savefile = 'conll07a';

% fprintf('%s saving arguments\n', datestr(now));
% save([savefile '_parser'], 'parser', '-v7.3');
% save([savefile '_feats'], 'feats', '-v7.3');
% save([savefile '_trn'], 'trn', '-v7.3');
% save([savefile '_dev'], 'dev', '-v7.3');
% save([savefile '_tst'], 'tst', '-v7.3');

% fprintf('%s initializing model\n', datestr(now));
% path('dogma',path);
% m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
% m0.parser = parser;
% m0.feats = feats;
% m0.step = 1e5;
% m0.batchsize = 1e3;
% tmp = feval(parser, 1);
% m0.n_cla = tmp.NMOVE;
% clear tmp;
% save([savefile '_m0'], 'm0', '-v7.3');

%% load([savefile '_m0']);

% fprintf('%s preparing dumps.\n', datestr(now));
% [~,trndump] = vectorparser(m0, trn, 'predict', 0, 'update', 0);
% save([savefile '_trndump'], 'trndump', '-v7.3');
% [~,devdump] = vectorparser(m0, dev, 'predict', 0, 'update', 0);
% save([savefile '_devdump'], 'devdump', '-v7.3');
% [~,tstdump] = vectorparser(m0, tst, 'predict', 0, 'update', 0);
% save([savefile '_tstdump'], 'tstdump', '-v7.3');

%% load([savefile '_trndump']);
%% load([savefile '_devdump']);
%% load([savefile '_dev']);

% m1 = k_perceptron_multi_train_gpu(trndump.x, trndump.y, m0);
m2 = perceptron(trndump.x, trndump.y, m0);
% m3 = perceptron(trndump.x, trndump.cost, m0);
% m4 = perceptron_dbg(trndump.x, trndump.y, m0);
% m5 = perceptron_dbg(trndump.x, trndump.cost, m0);
% m6 = perceptron2_dbg(trndump.x, trndump.y, m0);
% m7 = perceptron2_dbg(trndump.x, trndump.cost, m0);

% save('run_conll07_debug_models.mat', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', '-v7.3');

% [g1,c1,e1] = run_conll07_debug_eval(m1, devdump, dev);
[g2,c2,e2] = run_conll07_debug_eval(m2, devdump, dev);
% [g3,c3,e3] = run_conll07_debug_eval(m3, devdump, dev);
% [g4,c4,e4] = run_conll07_debug_eval(m4, devdump, dev);
% [g5,c5,e5] = run_conll07_debug_eval(m5, devdump, dev);
% [g6,c6,e6] = run_conll07_debug_eval(m6, devdump, dev);
% [g7,c7,e7] = run_conll07_debug_eval(m7, devdump, dev);

fprintf('gtrans\tctrans\tptrans\tphead\twhead\tsent\tmodel\n');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g1, c1, e1.move_pct, e1.head_pct, e1.word_pct, e1.sent_pct, 'k_perceptron_multi_train');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g2, c2, e2.move_pct, e2.head_pct, e2.word_pct, e2.sent_pct, 'perceptron(y)');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g3, c3, e3.move_pct, e3.head_pct, e3.word_pct, e3.sent_pct, 'perceptron(cost)');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g4, c4, e4.move_pct, e4.head_pct, e4.word_pct, e4.sent_pct, 'perceptron_dbg(y)');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g5, c5, e5.move_pct, e5.head_pct, e5.word_pct, e5.sent_pct, 'perceptron_dbg(cost)');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g6, c6, e6.move_pct, e6.head_pct, e6.word_pct, e6.sent_pct, 'perceptron2_dbg(y)');
fprintf('%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\t%s\n', g7, c7, e7.move_pct, e7.head_pct, e7.word_pct, e7.sent_pct, 'perceptron2_dbg(cost)');

% fprintf('%s static oracle training.\n', datestr(now));
% m1 = m0;
% e1 = inf;
% for epoch=1:100
%   m = perceptron(trndump.x, trndump.cost, m1);
%   p = model_predict_gpu(devdump.x, m, 1);
%   c = devdump.cost(sub2ind(size(devdump.cost),p,1:numel(p)));
%   [cmin,imin] = min(devdump.cost);
%   e = numel(find(c~=cmin))/numel(c);
%   fprintf('%s Epoch %d dev gtrans error: %g\n', datestr(now), epoch, e);
%   if e < e1
%     e1 = e;
%     m1 = m;
%     save([savefile '_m1'], 'm1', '-v7.3');
%     save([savefile '_p'], 'p', '-v7.3');
%   else
%     break;
%   end
% end

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% fprintf('%s Evaluating m1\n', datestr(now));
% [~,m1dev] = vectorparser(m1, dev, 'update', 0);
% save([savefile '_m1dev'], 'm1dev', '-v7.3');
% e1dev = eval_conll(dev, m1dev)
% save([savefile '_e1dev'], 'e1dev', '-v7.3');

% fprintf('%s dynamic oracle training.\n', datestr(now));
% m2 = m1;
% e2 = e1dev.head_pct;
% for epoch=1:100
%   m = vectorparser(m2, trn);
%   [~,mdev] = vectorparser(m, dev, 'update', 0);
%   edev = eval_conll(dev, mdev)
%   e = edev.head_pct;
%   fprintf('%s Epoch %d dev phead error: %g\n', datestr(now), epoch, e);
%   if e < e2
%     e2 = e;
%     m2 = m;
%     save([savefile '_m2'], 'm2', '-v7.3');
%     save([savefile '_mdev'], 'mdev', '-v7.3');
%     save([savefile '_edev'], 'edev', '-v7.3');
%   else
%     break;
%   end
% end

% fprintf('%s final testing.\n', datestr(now));
% [~,m2tst] = vectorparser(m2, tst, 'update', 0);
% save([savefile '_m2tst'], 'm2tst', '-v7.3');
% e2tst = eval_conll(tst, m2tst)
% save([savefile '_e2tst'], 'e2tst', '-v7.3');


==> run/run_conll07.m <==
function run_conll07(savefile, opts)

parser = getval('parser', @()(opts.parser));
feats = getval('feats', @()(opts.feats));
trn = getval('trn', @()(opts.trn));
dev = getval('dev', @()(opts.trn));
tst = getval('tst', @()(opts.trn));

trndump = getval('trndump', @()(getdump(m0,trn)));
devdump = getval('devdump', @()(getdump(m0,dev)));
tstdump = getval('tstdump', @()(getdump(m0,tst)));

m0 = getval('m0', @()(struct('kerparam', struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3),...
                             'parser', parser, 'feats', feats, 'step', 1e5, 'batchsize', 1e3)));

m3 = getval('m3', @()[]);
if isempty(m3)

  m2 = getval('m2', @()[]);
  if isempty(m2)

    m1 = getval('m1', @()[]);
    if isempty(m1)
      m1 = m0;
      e1 = inf;
      epoch1 = 1;
    else
      e1 = getval('e1', @()(error('No e1')));
      epoch1 = getval('epoch1', @()(error('No epoch1')));
    end

    fprintf('%s static oracle training.\n', datestr(now));
    fprintf('epoch\tgmove\tmove\thead\tword\tsent\n');

    while epoch1 < 100
      m = perceptron(trndump.x, trndump.y, m1);
      [e, f] = eval_model(m, dev, devdump);
      if e < e1
        e1 = e;
        f1 = f;
        m1 = m;
        save([savefile '_m1'], 'm1', '-v7.3');
        save([savefile '_e1'], 'e1', '-v7.3');
        save([savefile '_f1'], 'f1', '-v7.3');
        save([savefile '_epoch1'], 'epoch1', '-v7.3');
      else
        break;
      end
    end % while epoch1 < 100
    m2 = m1;
    f2 = f1;
    epoch2 = 1;
  end % if isempty(m2)

  f2 = getval('f2', @()(error('No f2')));
  epoch2 = getval('epoch2', @()(error('No epoch2')));

  fprintf('%s dynamic oracle training.\n', datestr(now));
  while epoch2 < 100
    m = vectorparser(m2, trn);
    [e, f] = eval_model(m, dev, devdump);
    if f.head_pct < f2.head_pct
      f2 = f;
      m2 = m;
      save([savefile '_m2'], 'm2', '-v7.3');
      save([savefile '_f2'], 'f2', '-v7.3');
      save([savefile '_epoch2'], 'epoch2', '-v7.3');
    else
      break;
    end
  end % while epoch2 < 100

  m3 = m2;
  f3 = f2;
end % if isempty(m3)

fprintf('%s final testing.\n', datestr(now));
[e3, f3] = eval_model(m3, tst, tstdump);


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [e, f] = eval_model(m, dev, devdump)
[~,s] = perceptron(devdump.x, devdump.y, m, 'update', 0);
[~,z] = max(s);
e = numel(find(z ~= devdump.y))/numel(z);
[~,mdev] = vectorparser(m, dev, 'update', 0);
f = eval_conll(dev, mdev)
fprintf('%d\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\n', epoch, e, ...
        f.move_pct, f.head_pct, f.word_pct, f.sent_pct);
end % eval_model

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function v = getval(varname, lambda)
fprintf('Getting %s\n', varname);
filename = [savefile '_' varname '.mat'];
if (exist(varname))
  v = eval(varname);
elseif (exist(filename, 'file'))
  load(filename);
  v = eval(varname);
else
  v = feval(lambda);
end
if ~exist(filename, 'file')
  save filename varname
end

end % getval

end % main


==> run_fs_conll07_aug6.m <==
load mat/conll07a_trn.mat
load mat/conll07a_dev.mat
whos trn dev

newFeatureVectors;
load mat/conll07a_m0.mat
m0.feats = fv136;


2014-08-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* run/run_conll07.m: 5 epochs of static oracle training best,
	nsv=71496, gtrans=.0352.  However bad ptrans error.  Let's compare
	with our old mx runs:

	mx run:
	corpus:ptb3
	encoding:conllToken_wikipedia2MUNK-50
	feats:fv808
	parser:ArcHybrid
	static-learner:k_perceptron_multi_train_gpu (run_multi_epoch2.m)
	dynamic-learner:trainparser_gpu (run_dynamic_oracle2.m)

	Static Oracle:
	model	aer	time	nsv	gtrans	ptrans	phead	whead	notes
	mx1	6.09	314	104757	3.75	4.26	11.18		fv808 feature-set batch=1000
	mx2	4.89	727	133856	3.50	3.95	10.36		all mx are fv808 batch=1000
	mx3	4.11	860	150207	3.38	3.80	 9.96
	mx4	3.54	930	161208	3.36	3.80	 9.88
	mx5	3.21	963	169135	3.34	3.76	 9.83	8.63	best epoch for fv808: gtrans:3.34 phead:9.83 whead:7.60

	Dynamic Oracle:
	--i11			TEST				TRAIN
	model	nsv	time	gtrans	ptrans	phead	whead	ptrans	phead	whead
	mx5	169135	963	3.34	3.76	9.83	8.63	-	-
	mx5d1	204369	22472	4.07	3.78	10.02	8.83	3.02	8.37	7.42
	mx5d2	232151	?	4.02	3.63	9.59	8.47	2.73	7.60	6.77
	mx5d3	254535	?	3.99	3.49	9.23	8.16	2.45	6.88	6.13
	mx5d4	273226	?	4.00	3.47	9.20	8.12	2.23	6.28	5.61
	mx5d5	289077	?	3.99	3.39	9.02	7.98	2.02	5.73	5.11
	mx5d6	302335	?	3.99	3.39	8.96	7.92	1.83	5.24	4.67
	mx5d7	314292	?	4.00	3.37	8.90	7.88	1.71	4.92	4.40

	conll07 run:
	corpus:conll07 (trn nx:763702 nd:1384)
	encoding:bansal100
	feats:best of mat/bansal5k_archybrid_fv130_cache.mat
	parser:archybrid
	static-learner:perceptron (run_conll07.m)
	dynamic-learner:vectorparser (run_conll07.m)
	evaluation:gtrans measured using cost, not correct answer!

	Note: in static oracle gtrans not equivalent to prev exp, it is
	cost based (let's call this ctrans), prev was answer based
	(gtrans).

	Note: in dynamic oracle ptrans=move_pct, phead=head_pct,
	whead=word_pct, and the new sent_pct measures exact sentence match
	error.

	Static Oracle: (run_conll07_experiment1.out)
	epoch	ctrans
	1	4.02
	2	3.72
	3	3.57
	4	3.53
	5	3.52 (nsv:71496 gtrans:5.38 ptrans:7.20 phead:18.30)

	Dynamic Oracle: (run_conll07_experiment2.out --i01)
	epoch	move	head	word	sent
	0	7.20	18.30	16.04	76.97
	1	5.14	13.39	12.31	70.69
	2	4.99	13.01	11.92	70.54
	3	4.92	12.76	11.75	70.04
	4	4.89	12.68	11.67	69.88
	5	4.85	12.55	11.57	69.33
	6	4.82	12.50	11.51	69.48
	7	4.79	12.43	11.45	69.08
	8	4.75	12.36	11.37	68.78

	Summary: Static oracle is not easy to compare.  In mx, correct
	answer based gtrans went from 3.75 to 3.34 in 5 epochs.  In
	conll07 cost based gtrans went from 4.02 to 3.52 (which
	corresponds to 5.38 answer based).  So we are doing 2% worse in
	static oracle.  Theories:
	1. smaller training corpus
	1a. different dev set (02 may be more difficult than 22)
	2. bansal vs scode encoding
	3. fv808 vs new features
	4. bug-fix in archybrid?
	5. perceptron vs k_perceptron_multi_train_gpu.

	In dynamic oracle, mx starts with 3.34 gtrans translating to a
	close 3.76 ptrans, which gives the x2.6 9.83 phead.  whead is
	about 1% better.  conll07 starts with 5.38 gtrans translating to a
	2-point worse 7.20 ptrans, which gives the x2.6 18.30 phead.  At
	epoch 4 mx improves ptrans 0.3 to 3.47 and phead x2.6 is 9.20.  At
	epoch 4 conll07 improves ptrans 2.31 to 4.89 and phead x2.6 gives
	12.68.

	Static oracle is leaving a model 2.04% gtrans worse (which could
	be explained by data size etc.) but the gtrans/ptrans difference
	at that point is huge (1.82% vs 0.42% for mx)!  That is partly
	compensated by dynamic oracle improvement (2.31% vs 0.29% for mx).

		gtrans0	gtrans1	ptrans1	ptrans4	phead4
	mx	3.75	3.34	3.76	3.47	9.20
	conll07	?	5.38	7.20	4.89	12.68

	Theories:
	1. find out the answer based gtrans0 for conll07.
	2. problem definitely in static oracle training.
	3. 1-epoch perceptron vs k-perceptron, ctrans vs gtrans.
	4. Look into perceptron update rule for cost/score ties.


	** 1-epoch-experiment: Compare the effect of different update
	rules.

	1. k_perceptron:
	Yi = gpuArray(int32(Y(i:j)) + model.n_cla*int32(0:ij-1)); % 1018us
	tmp=val_f;                            % 922us
	tmp(Yi)=-inf;                         % 1200us
	[mx_val,idx_mx_val]=max(gather(tmp));         % 983us
	tr_val = val_f(Yi);                   % 996us
	updates = find(tr_val <= mx_val);     % 1219us

	2. perceptron(Y):
	First converts Y into cost matrix and then applies the rule below:

	3. perceptron(cost):
	score = compute_scores();           % score(nc,nk): scores for X(:,i:j)
	costij = Y(:,i:j);                  % costij(nc,nk): costs for X(:,i:j)
	[maxscore, maxscore_i] = max(score); % compare the cost of maxscore answers
	[mincost, mincost_i] = min(costij); % to the mincost answers
	mycost = costij(sub2ind(size(costij), maxscore_i, 1:nk)); % cost of maxscore answers
	updates = find(mycost > mincost);

	4. vectorparser:
	[mincost, bestmove] = min(cost);
	[maxscore, maxmove] = max(score);
	update if cost(maxmove) > mincost

	4.1 But maxmove is arbitrary in case of tie?
	4.2 It is not used for transition if its cost is inf.
	4.3 All ties are broken picking the lowest numbered move.

	First experiment shows that using the cost for training
	(naturally) gives best result in cost, but significantly worse in
	guessing "correct" answer. (run_conll07_debug.m)

--i02	gtrans	ctrans	ptrans	phead	whead	sent	model
	0.0471	0.0426	0.0605	0.1549	0.1397	0.7285	k_perceptron_multi_train
	0.0476	0.0428	0.0612	0.1576	0.1418	0.7466	perceptron(y)
	0.0581	0.0402	0.0752	0.1899	0.1668	0.7969	perceptron(cost)

	How do we decide "correct" other than arbitrarily during dump?
	[mincost, bestmove] = min(cost);

	So the fact that we do not match arbitrary correct answer should
	not hurt the head accuracy.  But it seems less able to generalize
	to the dynamic parsing situation, i.e. ptrans is a lot worse.

	Trying a different update rule: (perceptron_dbg.m)
	maxscore = max(score, [], 1);       % maxscore answers according to model
	minycost = min(ycost, [], 1);       % mincost answers according to data
	dbeta = bsxfun(@eq, ycost, minycost) - bsxfun(@eq, score, maxscore);
	updates = find(any(dbeta));         % nonzero columns of dbeta

	gtrans	ctrans	ptrans	phead	whead	sent	model
	0.0471	0.0426	0.0605	0.1549	0.1397	0.7285	k_perceptron_multi_train
	0.0476	0.0428	0.0612	0.1576	0.1418	0.7466	perceptron(y)
	0.0581	0.0402	0.0752	0.1899	0.1668	0.7969	perceptron(cost)
	0.0481	0.0433	0.0619	0.1584	0.1424	0.7436	perceptron_dbg(y)
	0.0713	0.0533	0.0887	0.2324	0.2095	0.8421	perceptron_dbg(cost)

	Try to normalize the updates: (perceptron2_dbg.m) This makes sense
	if we assume one pair of mincost,maxscore entries is picked at
	random and we are looking at the expected update.

	score = compute_scores();           % score(nc,nk): scores for X(:,i:j)
	costij = cost(:,i:j);               % costij(nc,nk): costs for X(:,i:j)
	maxscore = max(score);              % maxscore in each column of score
	mincost = min(costij);              % mincost in each column of costij
	incbeta = bsxfun(@eq, costij, mincost); % +1 beta for mincost answers
	decbeta = bsxfun(@eq, score, maxscore); % -1 beta for maxscore answers
	incbeta = bsxfun(@rdivide, incbeta, sum(incbeta));   % normalize in case multiple mincost answers
	decbeta = bsxfun(@rdivide, decbeta, sum(decbeta));   % normalize in case multiple maxscore answers
	dbeta = incbeta - decbeta;          % this should cancel out if y_mincost = y_maxscore
	updates = find(any(dbeta));         % nonzero columns of dbeta are the updates

	gtrans	ctrans	ptrans	phead	whead	sent	model
	0.0471	0.0426	0.0605	0.1549	0.1397	0.7285	k_perceptron_multi_train
	0.0476	0.0428	0.0612	0.1576	0.1418	0.7466	perceptron(y)
	0.0581	0.0402	0.0752	0.1899	0.1668	0.7969	perceptron(cost)
	0.0481	0.0433	0.0619	0.1584	0.1424	0.7436	perceptron_dbg(y)
	0.0713	0.0533	0.0887	0.2324	0.2095	0.8421	perceptron_dbg(cost)
	0.0481	0.0433	0.0619	0.1584	0.1424	0.7436	perceptron2_dbg(y)
	0.0587	0.0408	0.0733	0.1864	0.1642	0.7924	perceptron2_dbg(cost)

	This version gives the same answers in y mode.  Why?  Different
	beta, same SV?  Look at non-compactified version.

	However ptrans and the rest still bad, even worse when using cost
	matrix.

	Theories:
	- Look at this cost matrix and make sure its contents are good.
	- What kinds of instances have multiple correct answers or score ties?
	- Can we just emulate k_perceptron_multi_train?
	- Compare resulting beta with k_perceptron_multi_train?
	- How do cost based algorithms in dogma do it?
	- Does it have to do with move ordering (min picking the lowest)?
	- If we sort them differently does the regular k_perceptron screw up?
	- If it is important do we have the right ordering for other arctypes?
	- Replicate the mx results with perceptron.m.

	CONCLUSION: Even though costs and scores can be tied, in reality
	the oracle or the parser only picks one move!  When the learner
	behaves as if other moves could have been chosen it does not gain
	any advantage.  So we should get back to the original perceptron
	update rule.  The actual moves that are made in vectorparser
	should be compatible with learner:

    if opts.predict
      zscore = score;
      zscore(~valid) = -inf;
      [~,zmove] = max(zscore);
      p.transition(zmove);
    else
      [mincost, bestmove] = min(cost);
      p.transition(bestmove);
    end


==> run/run_conll07_debug_eval.m <==
function [gtrans, ctrans, ev] = run_conll07_debug_eval(model, dump, corpus)

p = model_predict_gpu(dump.x, model, 1);
gtrans = numel(find(p~=dump.y))/numel(p);
c = dump.cost(sub2ind(size(dump.cost),p,1:numel(p)));
[cmin,imin] = min(dump.cost);
ctrans = numel(find(c~=cmin))/numel(c);

[~,d] = vectorparser(model, corpus, 'update', 0);
ev = eval_conll(corpus,d);

end


2014-08-04  Deniz Yuret  <dyuret@ku.edu.tr>

	* fsymbol.m: get feature name from feature vector f(1,3).

	* conll07bansal.mat: we need conll07 dev for featselect and epoch
	decisions.  let us use sec02 for dev: top 50123 lines, 1989
	sentences, 48134 tokens.  We'll save it as split.  Just concat dev
	and trn to get original data.

	* run_arctype_comparison2.m: --i00: Determine which arctype is
	best.  The scores *c are cost based, *a are answer based.  trn and
	tst are from conll07, dev is from conllWSJToken.  All after one
	epoch cost-based training with perceptron.m with features
	optimized on conllWSJToken trn(1:5000)/dev.

	NOTE: these used a buggy version of perceptron (cost based)
	update, a bad dev set and possibly bad features (because they were
	optimized on the bad dev set albeit with the correct
	k_perceptron_multi_train).  Need to rerun.

	1-epoch results: (run_arctype_comparison2.m)
	arc	trnc	trna	devc	deva	tstc	tsta	feats
	eager	2.76	4.69	7.78	9.14	4.23	6.41	17
	hybrid	2.75	4.46	7.30	8.55	4.23	6.30	27
	eag13	2.76	4.87	7.62	9.22	4.08	6.43	23
	hyb13	2.58	4.31	7.43	8.66	4.24	6.25	27

	10-epoch results: (run_arctype_comparison3.m)
	arc	trnc	trna	devc	deva	tstc	tsta	feats
	eager	0.35	2.32	7.70	9.13	3.85	6.06	17
	hybrid	0.39	2.18	6.99	8.32	3.65	5.76	27
	eag13	0.36	2.43	7.52	9.12	3.71	5.96	23
	hyb13	0.30	2.08	7.02	8.33	3.78	5.89	27

==> run/run_arctype_comparison2.m <==
% from previous script:
% save -v7.3 conll07bansal trnconll devconll tstconll
% savefile = ['conll07bansal_' arc];
% save(savefile, 'model', 'trndump', 'devdump', 'tstdump', 'deveval', 'tsteval', '-v7.3');

path('dogma',path);

fprintf('%s initializing model\n', datestr(now));
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
m0.step = 1e5;
m0.batchsize = 1e3;
% m0.parser = parser;
% m0.feats = feats;

fprintf('%s loading conll07bansal\n', datestr(now));
load conll07bansal;

for arctype={'arceager', 'archybrid', 'arceager13', 'archybrid13'}
  arc = arctype{1};
  m0.parser = eval(['@' arc]);
  savefile = ['conll07bansal_' arc];
  fprintf('%s loading %s\n', datestr(now), savefile);
  load(savefile);
  m0.feats = trndump.feats;
  fprintf('%s training\n', datestr(now));
  m1 = perceptron(trndump.x, trndump.cost, m0)

  fprintf('%s trn eval\n', datestr(now));
  p1 = model_predict_gpu(trndump.x, m1, 1);
  c1=trndump.cost(sub2ind(size(trndump.cost),p1,1:numel(p1)));
  [min1,imin1] = min(trndump.cost);
  trn_cost_eval = numel(find(c1~=min1))/numel(c1)
  trn_answer_eval = numel(find(p1~=trndump.y))/numel(p1)

  fprintf('%s dev eval\n', datestr(now));
  [~,devdump] = vectorparser(m0, devconll, 'predict', 0, 'update', 0);
  p2 = model_predict_gpu(devdump.x, m1, 1);
  c2=devdump.cost(sub2ind(size(devdump.cost),p2,1:numel(p2)));
  [min2,imin2] = min(devdump.cost);
  dev_cost_eval = numel(find(c2~=min2))/numel(c2)
  dev_answer_eval = numel(find(p2~=devdump.y))/numel(p2)

  fprintf('%s tst eval\n', datestr(now));
  [~,tstdump] = vectorparser(m0, tstconll, 'predict', 0, 'update', 0);
  p3 = model_predict_gpu(tstdump.x, m1, 1);
  c3=tstdump.cost(sub2ind(size(tstdump.cost),p3,1:numel(p3)));
  [min3,imin3] = min(tstdump.cost);
  tst_cost_eval = numel(find(c3~=min3))/numel(c3)
  tst_answer_eval = numel(find(p3~=tstdump.y))/numel(p3)

end

==> run/run_arctype_comparison3.m <==
% from previous script:
% save -v7.3 conll07bansal trnconll devconll tstconll
% savefile = ['conll07bansal_' arc];
% save(savefile, 'model', 'trndump', 'devdump', 'tstdump', 'deveval', 'tsteval', '-v7.3');

path('dogma',path);

fprintf('%s initializing model\n', datestr(now));
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
m0.step = 1e5;
m0.batchsize = 1e3;
% m0.parser = parser;
% m0.feats = feats;

fprintf('%s loading conll07bansal\n', datestr(now));
load conll07bansal;

for arctype={'arceager', 'archybrid', 'arceager13', 'archybrid13'}
  arc = arctype{1};
  m0.parser = eval(['@' arc]);
  savefile = ['conll07bansal_' arc];
  fprintf('%s loading %s\n', datestr(now), savefile);
  load(savefile);
  m0.feats = trndump.feats;

  fprintf('%s training\n', datestr(now));
  m1 = m0;
  for i=1:10
    m1 = perceptron(trndump.x, trndump.cost, m1);
  end
  m1_10_epoch = m1

  fprintf('%s trn eval\n', datestr(now));
  p1 = model_predict_gpu(trndump.x, m1, 1);
  c1=trndump.cost(sub2ind(size(trndump.cost),p1,1:numel(p1)));
  [min1,imin1] = min(trndump.cost);
  trn_cost_eval = numel(find(c1~=min1))/numel(c1)
  trn_answer_eval = numel(find(p1~=trndump.y))/numel(p1)

  fprintf('%s dev eval\n', datestr(now));
  [~,devdump] = vectorparser(m0, devconll, 'predict', 0, 'update', 0);
  p2 = model_predict_gpu(devdump.x, m1, 1);
  c2=devdump.cost(sub2ind(size(devdump.cost),p2,1:numel(p2)));
  [min2,imin2] = min(devdump.cost);
  dev_cost_eval = numel(find(c2~=min2))/numel(c2)
  dev_answer_eval = numel(find(p2~=devdump.y))/numel(p2)

  fprintf('%s tst eval\n', datestr(now));
  [~,tstdump] = vectorparser(m0, tstconll, 'predict', 0, 'update', 0);
  p3 = model_predict_gpu(tstdump.x, m1, 1);
  c3=tstdump.cost(sub2ind(size(tstdump.cost),p3,1:numel(p3)));
  [min3,imin3] = min(tstdump.cost);
  tst_cost_eval = numel(find(c3~=min3))/numel(c3)
  tst_answer_eval = numel(find(p3~=tstdump.y))/numel(p3)

end

==> run/run_conll07_experiment1.m <==
path('dogma',path);
path('run',path);
savefile = 'run_conll07_experiment01.mat';
if 0
load('mat/conll07bansal.mat'); % trn, dev, tst
load('mat/bansal5k_archybrid_fv130_cache.mat'); % cache
[emin, imin] = min(besterror);
fprintf('%s Features(%d:%g) %s\n', datestr(now), imin, emin, bestfeats{imin});
feats = eval(bestfeats{imin});
parser = @archybrid;

run_conll07(parser, feats, trn, dev, tst, savefile);

else
run_conll07([], [], [], [], [], savefile);

end


2014-08-03  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_arctype_comparison.m: --i00 running all arctypes with best
	features on conll07.  The first results (arceager) look really
	bad, trouble with vectorparser?  Different from using in dump
	mode?  Trouble with feature set?

	Debugging:
	load conll07bansal_arceager
	trndump.x [1022x855992]
	trnconll 18577	446573
	2*446573-2*18577 = 855992 ok
	? 17 feats =? 1022 dims
	m1=perceptron(trndump.x, trndump.y, m0);
	855992	65049	992	108.25	4.13e+08
	Finding unique SV in 65049...
	Saving 64828 unique SV.
	p1=model_predict_gpu(devdump.x, m1, 1);
	numel(find(p1 ~= devdump.y))/numel(devdump.y)
	0.2267 wtf? this should be 0.0542!

	ok perceptron instead of k_perceptron
	k_perceptron gives 0.2271.
	5k instead of full data
	size of 5k data: 3538x230120
	size of full data: 1022x855992

	both x and y are different even though the source data the same.
	in featselect trn comes from:
	vectorparser(m0, trn, 'update', 0, 'predict', 0);
	in run_arctype_comparison:
	[model,trndump] = vectorparser(m0, trnconll, 'predict', 0);
	trying to replicate with small data tstconll.
	no difference!

	compare devs in bansal5k_arceager_fv130 which comes from
	conllWSJToken_bansal100 with conll07bansal_arceager which comes
	from conll07bansal.

	a1: Aug  1 17:52 conllWSJToken_bansal100.mat (?)
	a2: Aug  2 13:33 bansal5k_arceager_fv130.mat (run_dump_bansal5k, run_featselect5k)
	b1: Aug  3 13:21 conll07bansal.mat (run_arctype_comparison)
	b2: Aug  3 19:46 conll07bansal_arceager.mat (run_arctype_comparison)

	conllWSJToken_bansal100 and conll07bansal dev.head and dev.wvec
	identical.  bansal5k_arceager_fv130 has 130 feats
	conll07bansal_arceager has 17.  Their y's do not match!  OK
	devdump.y will not match because conll07bansal_arceager is dumping
	it during testing thus following model moves?  We need that to get
	real parsing accuracy.  Like gtrans vs ptrans.  During testing we
	are measuring ptrans.  But trndumps should have identical y!  They
	do not.

	Heads are different in conll07!  OK, the training data is
	different.  run_featselect5k used conllWSJToken_bansal100 for trn
	(1:5000, 230120 instances) and dev.  run_arctype uses conll07 trn
	(855992 instances).

	Looking at training set accuracy:
	p=model_predict_gpu(b2.trndump.x, b2.model, 1);
	numel(find(p~=b2.trndump.y))/numel(p) => 0.1647

	b2.model has 50646 SV out of 855992 instances.  should be 5.92?
	Retry training with k_perceptron instead of vectorparser:
	b2: [model,trndump] = vectorparser(m0, trnconll, 'predict', 0);
	model2 = k_perceptron_multi_train_gpu(b2.trndump.x, b2.trndump.y, a2.m0);
	nd=1022 nx=855992 nc=4 ns=0
	#855992 g:4.12515e+08 nk:1000 SV: 7.57(64823)	AER: 0.00	t=112.736
	p2=model_predict_gpu(b2.trndump.x, model2, 1);
	numel(find(p2~=b2.trndump.y))/numel(p2)  % 0.0328

	ok, b2 model is totally fdup.  problem is with vectorparser training.
	how about dev and test?
	p3=model_predict_gpu(b2.devdump.x, model2, 1);
	numel(find(p3~=b2.devdump.y))/numel(p3)  % 0.2271
	p4=model_predict_gpu(b2.tstdump.x, model2, 1);
	numel(find(p4~=b2.tstdump.y))/numel(p4)  % 0.1723
	but these y's are not the original y's!

	get the plain dumps of tst and dev:
	[~,tstdump2] = vectorparser(b2.model, b1.tstconll, 'predict', 0, 'update', 0)
	p5=model_predict_gpu(tstdump2.x, model2, 1);
	numel(find(p5~=tstdump2.y))/numel(p5)   % 0.0537 OK
	[~,devdump2] = vectorparser(b2.model, b1.devconll, 'predict', 0, 'update', 0)
	p6=model_predict_gpu(devdump2.x, model2, 1);
	numel(find(p6~=devdump2.y))/numel(p6)   % 0.0901

	OK: definitely difference between dev and tst.  we need to find
	some other dev or figure out the difference.

	5.37 is not great, we used to get under 4.00 for gtrans in one
	epoch: but different corpus format and half the training data and
	using arceager with a new feature set.

	3.28 train-gtrans, 5.37 test-gtrans, 9.16 for test-ptrans.

	old model5k used to get 5.43 for test-gtrans and 5.78 for test-ptrans.

	ptrans number may have a problem because of vectorparser bug.

	This gives 16.47 train error:
	[b2.model,b2.trndump] = vectorparser(m0, trnconll, 'predict', 0);

	This gives 3.28 train error with same data:
	model2 = k_perceptron_multi_train_gpu(b2.trndump.x, b2.trndump.y, a2.m0);

	This gives 3.31 train error with same data:
	model3 = perceptron(b2.trndump.x, b2.trndump.y, a2.m0);

	Test if cost based eval is different:

	- answer train / answer eval / cost eval (k_perceptron):
	model2 = k_perceptron_multi_train_gpu(b2.trndump.x, b2.trndump.y, a2.m0);
	p2=model_predict_gpu(b2.trndump.x, model2, 1);
	c2=b2.trndump.cost(sub2ind(size(b2.trndump.cost),p2,1:numel(p2)));
	[min2,imin2] = min(b2.trndump.cost);
	numel(find(c2~=min2))/numel(c2) % 0.0290
	numel(find(p2~=b2.trndump.y))/numel(p2) % 0.0328

	- answer train / answer eval / cost eval (perceptron):
	model3 = perceptron(b2.trndump.x, b2.trndump.y, a2.m0);
	p3=model_predict_gpu(b2.trndump.x, model3, 1);
	c3=b2.trndump.cost(sub2ind(size(b2.trndump.cost),p3,1:numel(p3)));
	[min3,imin3] = min(b2.trndump.cost);
	numel(find(c3~=min3))/numel(c3) => 0.0292
	numel(find(p3~=b2.trndump.y))/numel(p3) => 0.0331

	% slight difference due to how equalities are handled:

	% perceptron never punishes an equal cost move:
	% update if cost(maxscoremove) > mincost
	[maxscore, maxscore_i] = max(score); % compare the cost of maxscore answers
	[mincost, mincost_i] = min(costij); % to the mincost answers
	mycost = costij(sub2ind(size(costij), maxscore_i, 1:nk)); % cost of maxscore answers
	updates = find(mycost > mincost);
	model.beta2 = model.beta2 + model.beta;

	% k_perceptron initially will, when all val_f = 0:
	% update if score(correctmove) <= score(maxscoreincorrectmove)
	tmp=val_f;                            % 922us
	tmp(Yi)=-inf;                         % 1200us
	[mx_val,idx_mx_val]=max(gather(tmp)); % 983us
	tr_val = val_f(Yi);                   % 996us
	updates = find(tr_val <= mx_val);     % 1219us

	- cost train / answer eval / cost eval (perceptron):
	% gives the best cost eval but worst answer eval (as expected):
	model4 = perceptron(b2.trndump.x, b2.trndump.cost, a2.m0);
	p4=model_predict_gpu(b2.trndump.x, model4, 1);
	c4=b2.trndump.cost(sub2ind(size(b2.trndump.cost),p4,1:numel(p4)));
	[min4,imin4] = min(b2.trndump.cost);
	numel(find(c4~=min4))/numel(c4) % 0.0276
	numel(find(p4~=b2.trndump.y))/numel(p4) % 0.0469


	Could be the same mistake as in the perceptron:
	score(cost==inf) = -inf;

	Try on small data:
	[m1,d1] = vectorparser(m0, b1.tstconll, 'predict', 0); % 0.1886
	m2 = perceptron(d1.x, d1.y, m0); % 0.0534
	% After commenting out the above line...
	% DONE: if predict, we need to not do impossible moves!
	[m3,d3] = vectorparser(m0, b1.tstconll, 'predict', 0); % 0.0678
	% At this point should be the same with perceptron?
	% But vectorparser uses cost based training:
	m4 = perceptron(d1.x, d1.cost, m0);  % 0.0696
	% still not identical?  There is a difference in beta2!
	% fixed beta2 bug in perceptron:
	m5 = perceptron(d1.x, d1.cost, m0);  % 0.0678

	So cost based training is worse than answer based.  Testing on
	dynamic data (ptrans) makes things bad.  And ignoring scores of
	impossible moves is by far the worst!  Why?

	* conll07:

	- goldberg-nivre-2013: gives results for conll07 english using
	greedy parsers.  training is 02-11 (half regular PTB size), look
	at where the test data comes from (it is part of wsj23).

	- results on conll07.  check the source of the test corpus: paper
	claims subset of wsj23.  It seems to be wsj_2300 to part of
	wsj_2308.  Best UAS in conll07 paper: 90.63.  Goldberg-Nivre-2013
	best deterministic parser: 89.41.  Both include punctuation.

		 nsent	nword
	trnconll 18577	446573
	tstconll 214	5003

	Short	UAS+p	Paper
	Car07	90.63	X. Carreras. 2007. Experiments with a high-order projective dependency parser. In Proc. of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
	GN13	89.41	Yoav Goldberg and Joakim Nivre. 2013. Training Deterministic Parsers with Non-Deterministic Oracles.. TACL, vol 1, pp 403--414.

	- results on PTB.  For beam parsers Zhang-Nivre-2011 test on PTB
	and report 92.9 on test (wsj23), 93.14 on dev (wsj22).  They also
	report two transition parsers: ZC08transition (91.4) HS10 (91.4),
	and four graph based parsers: MST (91.5), K08 (92.0), KC10 (93.0,
	92.9) These exclude punctuation: ZC08 says "Like McDonald et
	al. (2005), we evaluate the parsing accuracy by the precision of
	lexical heads (the percentage of input words, excluding
	punctuation, that have been assigned the correct parent) and by
	the percentage of complete matches, in which all words excluding
	punctuation have been assigned the correct parent."

		Secs	Sents 	Words
	Train 	221 	39832	950028
	Dev 	22 	1700	40117
	Test 	23 	2416	56684

	Short	UAS-p	Paper
	ZN11	92.9	Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Asso- ciation for Computational Linguistics: Human Language Technologies, pages 188193.
	ZC08	92.1	Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 562571.
	ZC08tr	91.4	just transition based features of ZC08.
	MST05	90.9	Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL, pages 9198, Ann Arbor, Michigan, June.
	MST2	91.5	R McDonald and F Pereira. 2006. Online learning of approximate dependency parsing algorithms. In In Proc. of EACL, pages 8188, Trento, Italy, April.
	MST1	90.7	First order result according to MP06.

	- conll07-ptb difference:

	from conll-xi/data/english/ptb/doc/README:
	The head and dependency relation fields were converted using the
	algorithms described in
	> Richard Johansson and Pierre Nugues (tentative title)
	> "Extended Constituent-to-Dependency Conversion for English"
	> (Submitted)
	> http://www.lucas.lth.se/lt/pennconverter
	This was run with the arguments: -conjAsHead -prepAsHead

	new website:
	http://nlp.cs.lth.se/software/treebank_converter
	http://fileadmin.cs.lth.se/nlp/software/pennconverter/pennconverter.jar

	The new arg -conll2007 is supposed to generate the conll07
	conventions.  But files are not identical.  It seems the old
	version of pennconverter (and thus the original conll07 data) had
	some conjunction bugs which were later fixed.  220/5003 (4.40%)
	head difference is the closest I can get to conll07.  On the
	training set about 5 sentences were deleted from ptb to conll07.
	Of the 446573 heads the closest I can get is 20298 difference
	(4.55%).  It seems if we are going to work with conll07 data we
	better use part of train as devel instead of trying to regenerate.

==> run/run_arctype_comparison.m <==
if 0
path('dogma',path);

trnfile = 'embedded/conll07EnglishToken_bansal100/00/english_0001.dp';
tstfile = 'embedded/conll07EnglishToken_bansal100/01/english_0101.dp';
devfile = 'embedded/conllWSJToken_bansal100/01/wsj_0101.dp';

fprintf('%s loading %s\n', datestr(now), trnfile);
trnconll = loadCoNLL(trnfile);
fprintf('%s loading %s\n', datestr(now), devfile);
devconll = loadCoNLL(devfile);
fprintf('%s loading %s\n', datestr(now), tstfile);
tstconll = loadCoNLL(tstfile);
fprintf('%s saving %s\n', datestr(now), 'conll07bansal');
save -v7.3 conll07bansal trnconll devconll tstconll
end

fprintf('%s initializing model\n', datestr(now));
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
% m0.parser = parser;
% m0.feats = feats;
m0.step = 1e5;
m0.batchsize = 1e3;

for arctype={'arceager', 'archybrid', 'arceager13', 'archybrid13'}
  arc = arctype{1};
  m0.parser = eval(['@' arc]);
  cachefile = ['bansal5k_' arc '_fv130_cache.mat'];
  fprintf('%s loading %s\n', datestr(now), cachefile);
  load(cachefile);
  [emin, imin] = min(besterror);
  fprintf('%s %g %s\n', datestr(now), emin, bestfeats{imin});
  m0.feats = eval(bestfeats{imin});
  fprintf('%s training\n', datestr(now));
  [model,trndump] = vectorparser(m0, trnconll, 'predict', 0);
  fprintf('%s parsing dev\n', datestr(now));
  [~,devdump] = vectorparser(model, devconll, 'update', 0);
  deveval = eval_conll(devconll, devdump)
  fprintf('%s parsing tst\n', datestr(now));
  [~,tstdump] = vectorparser(model, tstconll, 'update', 0);
  tsteval = eval_conll(tstconll, tstdump)
  savefile = ['conll07bansal_' arc];
  fprintf('%s saving to %s\n', datestr(now), savefile);
  save(savefile, 'model', 'trndump', 'devdump', 'tstdump', 'deveval', 'tsteval', '-v7.3');
end


==> run/run_convert_cache.m <==
function run_convert_cache(trn, cachefile, newfile)

load(cachefile);
oldcache = cache;
nfeats = numel(trn.fidx);
bestfeats = cell(1,nfeats);
besterror = inf(1,nfeats);
cachekeys = keys(oldcache);
cache = containers.Map();
for i=1:numel(cachekeys)
  fstr = cachekeys{i};
  ferr = oldcache(fstr)/100;
  fvec = eval(fstr);
  flen = numel(fvec);
  fk = fkey(fvec);
  cache(fk) = ferr;
  if ferr < besterror(flen)
    besterror(flen) = ferr;
    bestfeats{flen} = fk;
  end
end

save(newfile, 'cache', 'besterror', 'bestfeats');

function fk = fkey(f)
% f is an array of indices into trn.fidx and rows of trn.feats
fk = mat2str(sortrows(trn.feats(f,:)));
end % fkey

end


==> run/run_sort_bestfeats_dbg.m <==
function fsorted = run_sort_bestfeats_dbg(bestfeats, besterror, cache)

[emin, imin] = min(besterror);
imin = imin-1
emin = besterror(imin)
fmin = eval(bestfeats{imin});

for i=1:imin
  fsorted{i,1} = 0;
  fsorted{i,2} = fsymbol(fmin(i,:));
  fnew = fmin;
  fnew(i,:) = [];
  fk = mat2str(sortrows(fnew));
  if isKey(cache, fk)
    ferr = cache(fk);
    fsorted{i,1} = round(10000 * (emin - ferr));
  end
end

fsorted = sortrows(fsorted, 1);

function s = fsymbol(f)
a = {'s2','s1','s0','n0','n1','n2'};
b = {'l2','l1','','r1','r2'};
c = {'ac','d<','l<','l>','c','at','l=','-','t','+','r=','d=','w','r>','r<','d>','aw'};
s = [ a{f(1)+4} b{f(2)+3} c{f(3)+9} ];
end

end


==> run/run_to_cache.pl <==
#!/usr/bin/perl -w
use strict;

my @score;
my @feats;

while(<>) {
    next unless /^\d+\./;
    chop;
    my @a = split /\t/;
    push @score, $a[1];
    push @feats, $a[4];
}

print "cache = containers.Map({\n";
for (my $i = 0; $i <= $#feats; $i++) {
    print "'$feats[$i]'\n";
}
print "}, [\n";
for (my $i = 0; $i <= $#score; $i++) {
    print "$score[$i]\n";
}
print "]);\n";


==> run/run_watchfile.pl <==
#!/usr/bin/perl -w
use strict;
use Data::Dumper;
use File::Copy;
my $fh;
my $last_timestamp=0;

while (1) {
    sleep(1);
    open($fh, '814724.mat');
    my $epoch_timestamp = (stat($fh))[9];
    if ($epoch_timestamp != $last_timestamp) {
	$last_timestamp = $epoch_timestamp;
	copy '814724.mat', "814724-$last_timestamp.mat";
    }
    close($fh);
}


2014-08-02  Deniz Yuret  <dyuret@ku.edu.tr>

	* perceptron.m: The cost version.  Not really comparable to the
	old k_perceptron unfortunately: perceptron updates whenever the
	cost of maxscore is higher than mincost.  k_perceptron updates
	whenever the score of the correct (mincost) answer is <= any of
	the other scores.  In particular, during the first batch all
	scores are 0, so k_perceptron makes all support vectors.
	perceptron only updates when maxscore class (which in this case
	will be 1 by default) is not correct.  However confirmed that it
	has better accuracy and equal speed on at least one dataset.

	Training with Y vs training with cost: reduced nsv from 30553 to
	27042 on bansal5k, the dev error went from 7.42 to 13.34??  We
	should be careful using the cost for training until we understand
	what is going on.  For now perceptron also accepts correct answer
	vector.  Maybe we should not forgive picking inf cost moves.  Yes,
	that is correct, deleting the line:

	score(isinf(costij)) = -inf;        % do not punish for impossible answers

	recovered the normal score.

	* run_featselect_bansal5k.m: running on --biyofiz.q.  Something
	seems wrong with arceager.
	+ check the featselect moves for arceager: backtracking seems ok.  except the first round multiple repeats.
	+ put actual feature vectors in cache
	+ write a cost based perceptron: do not punish when single move, or when multiple equiv moves.
	- transition error / parse error ratio may be different for arceager.
	- we may get different results with full training set.

	>> load('bansal5k_arceager13_fv130_cache.mat');
	>> run_sort_bestfeats(bestfeats, besterror, cache)
	% displays best feature combination (numfeats:besterror),
	% sorted by the loss of each feature:

	hybrid13(27:5.10)	hybrid(34:5.04) 	eager13(33:5.37)	eager(25:5.30)
	[-284]    'n0w'   	[-274]    'n0w'   	[-338]    'n0w'   	[-271]    'n0w'
	[-268]    's0w'   	[-234]    's0w'   	[-246]    's0w'   	[-237]    's0w'
	[-119]    's1w'   	[-106]    'n1w'   	[-155]    'n1w'   	[-104]    'n1w'
	[-110]    'n1w'   	[ -98]    's1w'   	[-150]    's1r1r<'	[ -40]    'n0c'
	[ -55]    'n0l1w' 	[ -52]    's0r1w' 	[ -45]    's0r1w' 	[ -35]    's0r1w'

	[ -38]    'n0l1c' 	[ -45]    'n0l1w' 	[ -44]    'n0c'   	[ -35]    'n0l1w'
	[ -36]    's0r1w' 	[ -35]    's0r1r<'	[ -39]    's1w'   	[ -31]    's1w'
	[ -32]    's0c'   	[ -32]    's0r1c' 	[ -39]    'n0l1w' 	[ -30]    's1r='
	[ -32]    's0r='  	[ -30]    's0r='  	[ -28]    's0c'   	[ -28]    's0r='
	[ -32]    'n0c'   	[ -30]    's0d>'  	[ -27]    's0l1w' 	[ -27]    's0l>'

	[ -27]    's2c'   	[ -29]    's1l1-' 	[ -27]    's0r1-' 	[ -26]    's0c'
	[ -27]    's1d='  	[ -28]    's2r='  	[ -27]    'n2-'   	[ -24]    'n1c'
	[ -27]    's0r1-' 	[ -28]    's0ac'  	[ -23]    's0r='  	[ -23]    's0r1r>'
	[ -25]    's0l1l='	[ -28]    's0c'   	[ -22]    'n1-'   	[ -22]    's2r1-'
	[ -25]    's0r1r<'	[ -28]    'n0c'   	[ -21]    's2l1r>'	[ -21]    's1-'

	[ -23]    's1r1r<'	[ -27]    'n0l1c' 	[ -21]    's2d>'  	[ -20]    's0-'
	[ -18]    's2r1l<'	[ -23]    's1r1r<'	[ -21]    's0r1+' 	[ -18]    'n0l>'
	[ -18]    's1l1l>'	[ -22]    's2c'   	[ -17]    's0l1r='	[ -16]    's1c'
	[ -18]    'n0l>'  	[ -20]    's2l1r='	[ -16]    's0-'   	[ -15]    's1l>'
	[ -16]    'n1-'   	[ -16]    's1r<'  	[ -15]    's2l='  	[ -15]    's1r1r='

	[ -15]    's1r1r>'	[ -14]    's1d>'  	[ -15]    's0l1l='	[ -13]    'n0l1l>'
	[ -12]    's1r1l>'	[ -14]    's0r1l<'	[ -15]    's0l1-' 	[ -11]    's1r1l>'
	[ -12]    's0r1l>'	[ -14]    'n1-'   	[ -14]    'n0l1l>'	[  -9]    's1r1-'
	[ -11]    's2l1l>'	[ -13]    's0l1w' 	[ -13]    's2r>'  	[  -8]    'n0-'
	[  -9]    's2r1r='	[ -11]    's2r1r='	[ -13]    's1r1r>'	[   0]    's2r1l='

	[  -8]    's0l1-' 	[ -11]    's0-'   	[ -12]    's2r1-'
	[  -5]    'n0l1-' 	[ -10]    's2r1l>'	[ -12]    's0r1r>'
				[ -10]    's2r1r>'	[ -11]    's2-'
				[  -6]    'n0l1l>'	[ -10]    'n0l1r<'
				[  -5]    's2d>'  	[  -8]    's2r1l>'
				[  -4]    'n0-'   	[  -7]    's2l1r='
				[  -3]    's0l1l='	[  -7]    's0l1l>'
				[  -3]    's0l1r>'	[  -6]    'n0-'
				[  -3]    'n0l1r>'



==> run/run_dump_bansal5k.m <==
function run_dump_bansal5k(parser, featname, dumpfile)

fprintf('%s Initializing features %s\n', datestr(now), featname);
newFeatureVectors;
feats = eval(featname);
size_feats = size(feats)

fprintf('%s Initializing model\n', datestr(now));
path('dogma',path);
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
m0.parser = parser;
m0.feats = feats;
m0.step = 1e7;
m0.batchsize = 1e3;

fprintf('%s Loading logs/conllWSJToken_bansal100.mat...\n', datestr(now));
load logs/conllWSJToken_bansal100.mat;  % trn,dev,tst; 7 mins

fprintf('%s trndump...\n', datestr(now)); % 20 mins
[~,trndump] = vectorparser(m0, trn(1:5000), 'update', 0, 'predict', 0);

fprintf('%s devdump...\n', datestr(now)); % 7 mins
[~,devdump] = vectorparser(m0, dev, 'update', 0, 'predict', 0);

fprintf('Saving...\n');
save(dumpfile, 'm0', 'trndump', 'devdump', '-v7.3');

end



==> run/run_featselect_bansal5k.m <==
function run_featselect_bansal5k(parser, featname, cachefile)

fprintf('%s Initializing features %s\n', datestr(now), featname);
newFeatureVectors;
feats = eval(featname);
size_feats = size(feats)

fprintf('%s Initializing model\n', datestr(now));
path('dogma',path);
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
m0.parser = parser;
m0.feats = feats;
m0.step = 1e7;
m0.batchsize = 1e3;

fprintf('%s Loading logs/conllWSJToken_bansal100.mat...\n', datestr(now));
load logs/conllWSJToken_bansal100.mat;  % trn,dev,tst; 7 mins

fprintf('%s trndump...\n', datestr(now)); % 20 mins
[~,trndump] = vectorparser(m0, trn(1:5000), 'update', 0, 'predict', 0);

fprintf('%s devdump...\n', datestr(now)); % 7 mins
[~,devdump] = vectorparser(m0, dev, 'update', 0, 'predict', 0);

fprintf('%s Starting featselect_gpu\n', datestr(now));
[a,b,c] = featselect_gpu(m0,trndump,devdump);

fprintf('%s this will never happen :)\n', datestr(now));
end


2014-08-01  Deniz Yuret  <dyuret@ku.edu.tr>

	* features.m:
	+ Separate word and context vectors.
	+ Put >= and <= as well as == numeric encodings.
	+ Average in-between feature.
	+ Output feature names and indices.
	+ The new fv102 defined in newFeatureVectors outputs 102 features in 2438 dims.
	+ Update featselect to use the new arctypes:
	+ use mat2str(sortrows(fv)) to get a string representation of feature vectors for caching.
	+ for that to work change fv from 3xn to nx3.
	+ actually that doesn't work, we need to use indices into fidx.
	+ run_loadCoNLL on top two token and type (do we need type? we can just filter using fv!) on --b40,42,43.
	+ confirm that conll_bansal100 is identical to the first half of conllWSJToken_bansal100.
	= yes except for some 0 vectors (unknowns?) in conll_bansal100.
	+ what if fidx is out of order and featselect is doing logical indexing?
	= fine, fidx is never out of order, f (index into fidx) might be but that is no problem.
	x debug run on --b41.
	x Saved dump data as trnhybrid.mat and devhybrid.mat. (fv808 arceager), but need fidx!
	x logs/trndump.mat includes the fv102 arceager dumps of trn1 and dev1 from conllToken_wikipedia2MUNK-50.
	+ use fv130 which separates all tokens into word/context and includes s2/n2.
	+ saving initial models:
	+ save logs/model130 archybrid_130 archybrid13_130 arceager_130 arceager13_130

	* archybrid,archybrid13,arceager,arceager13: The xx13 versions are
	replicating Goldberg and Nivre, allowing for multiple
	root-children.  The non-13 versions allow for a single root-child.
	All four make 2n-2 moves for each sentence, the first and the last
	moves are not counted: The first move which always shifts the
	first word is done during initialization, and the last move which
	assigns a root head to the last word in stack is not performed
	(the last word already has head=0).

	- all four oracle_costs verified on dev1.

	- extract all features for feature selection?  or use features
	with different vectors every time?  using vectorparser is out of
	the question, goes one move at a time, not mini-batch.  But then
	we need featureindices, symbolic names etc.  How fast is
	vectorparser in dump mode?  680 wps = 1400secs/ptb.  Too slow
	given that training takes 300secs.  So featselect has to rely on
	the full feature set and explicit indices with names.  (DONE)
	rewrite features to have a compact feature matrix representation.

==> run/run_featselect5k.m <==
function run_featselect5k(parser, feats)

fprintf('%s Loading logs/conllWSJToken_bansal100.mat...\n', datestr(now));
% load logs/conllWSJToken_bansal100.mat;  % trn,dev,tst

fprintf('%s Initializing model\n', datestr(now));
path('dogma',path);
m0 = model_init(@compute_kernel,struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3));
m0.parser = parser;
m0.feats = feats;
m0.step = 1e7;
m0.batchsize = 1e3;

fprintf('%s trndump...\n', datestr(now));
[~,trndump] = vectorparser(m0, trn(1:5000), 'update', 0, 'predict', 0);

fprintf('%s devdump...\n', datestr(now));
[~,devdump] = vectorparser(m0, dev, 'update', 0, 'predict', 0);

fprintf('%s Starting featselect_gpu\n', datestr(now));
[a,b,c] = featselect_gpu(m0,trndump,devdump);

fprintf('%s this will never happen :)\n', datestr(now));
end


==> run/run_loadCoNLL.m <==
function run_loadCoNLL(emi)

root = 'embedded';
fprintf('Reading %s\n', emi);
tic;
  trn = {};
  dev = {};
  tst = {};
  if isdir([root '/' emi '/22'])
    for j=2:21                          % read train
      dir_j = sprintf('%s/%s/%02d', root, emi, j); % e.g. embedded/conllToken_rcv1UNK/02
      fprintf('Reading %s\n', dir_j);
      dp = dir(dir_j);  % e.g. dir('embedded/conllToken_rcv1UNK/02')
      for k=1:numel(dp);
        if dp(k).isdir continue; end
        dpk = dp(k).name;               % e.g. wsj_0201.dp
        if (~strcmp(dpk(end-2:end), '.dp')) continue; end
        trn_k = loadCoNLL([dir_j '/' dpk]); % e.g. embedded/conllToken_rcv1UNK/02/wsj_0201.dp
        trn = [trn trn_k];
      end % for k=1:numel(dp);
    end % for j=2:21

    for j=[0,1,23,24]                          % read test
      dir_j = sprintf('%s/%s/%02d', root, emi, j); % e.g. embedded/conllToken_rcv1UNK/02
      fprintf('Reading %s\n', dir_j);
      dp = dir(dir_j);  % e.g. dir('embedded/conllToken_rcv1UNK/02')
      for k=1:numel(dp);
        if dp(k).isdir continue; end
        dpk = dp(k).name;               % e.g. wsj_0201.dp
        if (~strcmp(dpk(end-2:end), '.dp')) continue; end
        tst_k = loadCoNLL([dir_j '/' dpk]); % e.g. embedded/conllToken_rcv1UNK/02/wsj_0201.dp
        tst = [tst tst_k];
      end % for k=1:numel(dp);
    end % for j=2:21

    dir_j = sprintf('%s/%s/%02d', root, emi, 22);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);    % e.g. dir('embedded/conllToken_rcv1UNK/22')
    for k=1:numel(dp);
      if dp(k).isdir continue; end
      dpk = dp(k).name;                 % e.g. wsj_2201.dp
      if (~strcmp(dpk(end-2:end), '.dp')) continue; end
      dev_k = loadCoNLL([dir_j '/' dpk]);
      dev = [dev dev_k];
    end % for k=1:numel(dp)

  elseif isdir([root '/' emi '/01'])
    dir_j = sprintf('%s/%s/%02d', root, emi, 0);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    trn = loadCoNLL([dir_j '/' dp3]);

    dir_j = sprintf('%s/%s/%02d', root, emi, 1);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    dev = loadCoNLL([dir_j '/' dp3]);

    dir_j = sprintf('%s/%s/%02d', root, emi, 2);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    tst = loadCoNLL([dir_j '/' dp3]);

  else
    fprintf('%s is empty\n', emi);
    return;
  end %if
toc;

outfile = ['logs/' emi '.mat'];
save(outfile, 'trn', 'dev', 'tst', '-v7.3');


2014-07-30  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:

	+ Try ArcEager.  -- should be easy 1-epoch, needs code
	= need feature optimization again!
	+ Nivre says it is important not to put the root on the left?
	+ multipe versions of archybrid, arceager (one root-child vs many, see readings in Nivre's mail)

	+ archybrid.m,dumpfeatures2.m: temporary new versions to replace
	the old when scripts finish.  need to keep these separate until
	featselect etc. is updated.  or replace featselect with trainparser.

	+ experiment with different number encodings for distance and
	children.
	+ experiment with in-between features?
	+ try separating the type and context parts of token vectors.


==> run/run_embeddings1.m <==
function run_embeddings1(emi)

gpu = gpuDeviceCount();
if gpu==0 error('No GPU'); end

path('dogma', path);
fv804 = [
%n0 s0 s1 n1 n0l1 s0r1 s0l1 s1r1 s0r
  0 -1 -2  1  0   -1   -1   -2   -1;
  0  0  0  0 -1    1   -1    1    0;
  0  0  0  0  0    0    0    0    2;
];
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;

root = 'embedded';
fprintf('Reading %s\n', emi);
tic;
  trn_i = {};
  dev_i = {};
  if isdir([root '/' emi '/22'])
    for j=2:21                          % read train
      dir_j = sprintf('%s/%s/%02d', root, emi, j); % e.g. embedded/conllToken_rcv1UNK/02
      fprintf('Reading %s\n', dir_j);
      dp = dir(dir_j);  % e.g. dir('embedded/conllToken_rcv1UNK/02')
      for k=1:numel(dp);
        if dp(k).isdir continue; end
        dpk = dp(k).name;               % e.g. wsj_0201.dp
        if (~strcmp(dpk(end-2:end), '.dp')) continue; end
        trn_k = loadCoNLL([dir_j '/' dpk]); % e.g. embedded/conllToken_rcv1UNK/02/wsj_0201.dp
        trn_i = [trn_i trn_k];
      end % for k=1:numel(dp);
    end % for j=2:21
    dir_j = sprintf('%s/%s/%02d', root, emi, 22);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);    % e.g. dir('embedded/conllToken_rcv1UNK/22')
    for k=1:numel(dp);
      if dp(k).isdir continue; end
      dpk = dp(k).name;                 % e.g. wsj_2201.dp
      if (~strcmp(dpk(end-2:end), '.dp')) continue; end
      dev_k = loadCoNLL([dir_j '/' dpk]);
      dev_i = [dev_i dev_k];
    end % for k=1:numel(dp)

  elseif isdir([root '/' emi '/01'])
    dir_j = sprintf('%s/%s/%02d', root, emi, 0);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    trn_i = loadCoNLL([dir_j '/' dp3]);

    dir_j = sprintf('%s/%s/%02d', root, emi, 1);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    dev_i = loadCoNLL([dir_j '/' dp3]);

  else
    fprintf('%s is empty\n', emi);
    return;
  end %if
toc;
fprintf('dumpfeatures...\n');
tic; [trn_x, trn_y] = dumpfeatures2(trn_i, fv804); toc;
tic; [dev_x, dev_y] = dumpfeatures2(dev_i, fv804); toc;
fprintf('k_perceptron_multi_train_gpu...\n');
gpuDevice(1);
tic;m1=k_perceptron_multi_train_gpu(trn_x,trn_y,m0); toc;
tic;[a,b]=model_predict_gpu(dev_x, m1, 1); toc;
score = numel(find(a ~= dev_y))/numel(dev_y);
fprintf('score:%g\t%s\n', score, emi);

==> run/run_embeddings1.sh <==
#!/bin/bash
#$ -N job_name
#$ -S /bin/bash
#$ -q ilac.q
#$ -cwd
#$ -o job_name.out
#$ -e job_name.err
#$ -M login_id@ku.edu.tr
#$ -m bea

# We have a file named bigtest.m
/share/apps/matlab/R2011a/bin/matlab -nojvm -nodisplay -r bigtest > output.txt

==> run/run_embeddings2.m <==
path('dogma', path);
fv804 = [
%n0 s0 s1 n1 n0l1 s0r1 s0l1 s1r1 s0r
  0 -1 -2  1  0   -1   -1   -2   -1;
  0  0  0  0 -1    1   -1    1    0;
  0  0  0  0  0    0    0    0    2;
];
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;

root = 'embedded';
em = dir(root);
for i=1:numel(em)
  emi = em(i).name;                     % e.g. conllToken_rcv1UNK
  if ~any(strcmp(emi, {
    % 'conllWSJToken_levy300',		% error
    % 'conllWSJToken_bansal100',	% .0370
    % 'conllWSJToken_dep-WSJ',		% .0490
    % 'conllWSJToken_ungar2-0.1',	% .0520
    % 'conllWSJToken_stratos50k5000scaled01', % .0604
    % 'conllWSJToken_gsng1M',		% .0610
    % 'conllWSJToken_gsngAE',		% .0631
    % 'conllWSJToken_gsngALL',		% .0662
    % 'conllWSJToken_murphy50scaled01',	% error
    % 'conllWSJToken_murphy50'		% error
                      }))
    continue;
  end
  fprintf('Reading %s\n', emi);
  tic;
  trn_i = {};
  dev_i = {};
  if isdir([root '/' emi '/24'])
    for j=2:21                          % read train
      dir_j = sprintf('%s/%s/%02d', root, emi, j); % e.g. embedded/conllToken_rcv1UNK/02
      fprintf('Reading %s\n', dir_j);
      dp = dir(dir_j);  % e.g. dir('embedded/conllToken_rcv1UNK/02')
      for k=1:numel(dp);
        if dp(k).isdir continue; end
        dpk = dp(k).name;               % e.g. wsj_0201.dp
        if (~strcmp(dpk(end-2:end), '.dp')) continue; end
        trn_k = loadCoNLL([dir_j '/' dpk]); % e.g. embedded/conllToken_rcv1UNK/02/wsj_0201.dp
        trn_i = [trn_i trn_k];
      end % for k=1:numel(dp);
    end % for j=2:21
    dir_j = sprintf('%s/%s/%02d', root, emi, 22);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);    % e.g. dir('embedded/conllToken_rcv1UNK/22')
    for k=1:numel(dp);
      if dp(k).isdir continue; end
      dpk = dp(k).name;                 % e.g. wsj_2201.dp
      if (~strcmp(dpk(end-2:end), '.dp')) continue; end
      dev_k = loadCoNLL([dir_j '/' dpk]);
      dev_i = [dev_i dev_k];
    end % for k=1:numel(dp)

  elseif isdir([root '/' emi '/02'])
    dir_j = sprintf('%s/%s/%02d', root, emi, 0);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    trn_i = loadCoNLL([dir_j '/' dp3]);

    dir_j = sprintf('%s/%s/%02d', root, emi, 1);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    dev_i = loadCoNLL([dir_j '/' dp3]);

  else
    fprintf('%s is empty\n', emi);
    continue;
  end %if
  toc;
  fprintf('dumpfeatures...\n');
  tic; [trn_x, trn_y] = dumpfeatures2(trn_i, fv804); toc;
  tic; [dev_x, dev_y] = dumpfeatures2(dev_i, fv804); toc;
  fprintf('k_perceptron_multi_train_gpu...\n');
  gpuDevice(1);
  tic;m1=k_perceptron_multi_train_gpu(trn_x,trn_y,m0); toc;
  tic;[a,b]=model_predict_gpu(dev_x, m1, 1); toc;
  score = numel(find(a ~= dev_y))/numel(dev_y);
  fprintf('score:%g\t%s\n', score, emi);
end % for i=1:numel(em)

==> run/run_embeddings.m <==
path('dogma', path);
fv804 = [
%n0 s0 s1 n1 n0l1 s0r1 s0l1 s1r1 s0r
  0 -1 -2  1  0   -1   -1   -2   -1;
  0  0  0  0 -1    1   -1    1    0;
  0  0  0  0  0    0    0    0    2;
];
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;

root = 'embedded';
em = dir(root);
for i=1:numel(em)
  emi = em(i).name;                     % e.g. conllToken_rcv1UNK
  if strcmp(emi(1),'.') continue; end
  if any(strcmp(emi, {
'conllWSJToken_bansal100',
'conllWSJToken_wikipedia2MUNK-200',
'conllWSJToken_wikipedia2MUNK-100',
'conllToken_wikipedia2MUNK-50',
'conllToken_wikipedia2MUNK-25',
'conll_bansal100',
'conllWSJToken_rcv1UNK50',
'conllToken_rcv1UNK',
'conllWSJToken_rcv1UNK',
'conllWSJToken_rcv1UNK25',
'conllWSJToken_dep-100',
'conll_cw100scaled',
'conllWSJToken_huang-0.1',
'conll_4scode+f50',
'conll_2scode+f100',
'conll_3scode+f100',
'conll_6scode+f50',
'conll_4scode+f100',
'conll_6scode+f100',
'conll_5scode+f100',
'conllWSJToken_dep-WSJ',
'conll_10scode+f50',
'conll_7scode+f50',
'conll_2scode+f50',
'conll_1scode+f100',
'conll_1scode+f50',
'conll_9scode+f50',
'conll_3scode+f50',
'conll_5scode+f50',
'conll_8scode+f50',
'conll_3scode+f25',
'conllWSJToken_cw50scaled',
'conll_cw50scaled',
'conllWSJToken_ungar2-0.1',
'conllWSJToken_hlbl50scaled',
'conll_cw',
'conllWSJToken_mikolovWiki-0.1',
'conll_dep-100',
'conll_dep-WSJ',
'conll_cw25+cw25',
'conll_cw25',
'conllWSJToken_cw25',
'conllWSJToken_stratos50k5000scaled01',
'conllWSJToken_gsng1M',
'conllWSJToken_mikolovRCV50scaled01',
'conllWSJToken_cw25scaled',
'conll_cw25scaled',
'conllWSJToken_gsngAE',
'conllWSJToken_gsngALL',
'conll_dep100+wiki25',
'conllWSJToken_yogamata52scaled01',
'conllWSJToken_yogamata52',
'conllWSJToken_stratos50k200scaled01',
'conll_german+german',
'conllWSJToken_manaal48',
'conllWSJToken_hpca50scaled01',
'conllWSJToken_manaal48scaled01',
'conll_german',
'conllWSJToken_rnn80',
'conll_dep100+mik',
'conllWSJToken_levy300',
'conllWSJToken_mikolovGoogleNews300',
'conllWSJToken_murphy50',
'conllWSJToken_murphy50scaled01',
'conllWSJToken_wikipedia2MUNK-25',
'conll_10scode+f50',
'conll_german2',

                     }))
    fprintf('Skipping %s\n', emi);
    continue;
  end

  fprintf('Reading %s\n', emi);
  tic;
  trn_i = {};
  dev_i = {};
  if isdir([root '/' emi '/22'])
    for j=2:21                          % read train
      dir_j = sprintf('%s/%s/%02d', root, emi, j); % e.g. embedded/conllToken_rcv1UNK/02
      fprintf('Reading %s\n', dir_j);
      dp = dir(dir_j);  % e.g. dir('embedded/conllToken_rcv1UNK/02')
      for k=1:numel(dp);
        if dp(k).isdir continue; end
        dpk = dp(k).name;               % e.g. wsj_0201.dp
        if (~strcmp(dpk(end-2:end), '.dp')) continue; end
        trn_k = loadCoNLL([dir_j '/' dpk]); % e.g. embedded/conllToken_rcv1UNK/02/wsj_0201.dp
        trn_i = [trn_i trn_k];
      end % for k=1:numel(dp);
    end % for j=2:21
    dir_j = sprintf('%s/%s/%02d', root, emi, 22);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);    % e.g. dir('embedded/conllToken_rcv1UNK/22')
    for k=1:numel(dp);
      if dp(k).isdir continue; end
      dpk = dp(k).name;                 % e.g. wsj_2201.dp
      if (~strcmp(dpk(end-2:end), '.dp')) continue; end
      dev_k = loadCoNLL([dir_j '/' dpk]);
      dev_i = [dev_i dev_k];
    end % for k=1:numel(dp)

  elseif isdir([root '/' emi '/01'])
    dir_j = sprintf('%s/%s/%02d', root, emi, 0);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    trn_i = loadCoNLL([dir_j '/' dp3]);

    dir_j = sprintf('%s/%s/%02d', root, emi, 1);
    fprintf('Reading %s\n', dir_j);
    dp = dir(dir_j);
    assert(numel(dp) == 3, dir_j);
    dp3 = dp(3).name;
    assert(strcmp(dp3(end-2:end), '.dp'), dir_j);
    dev_i = loadCoNLL([dir_j '/' dp3]);

  else
    fprintf('%s is empty\n', emi);
    continue;
  end %if
  toc;
  fprintf('dumpfeatures...\n');
  tic; [trn_x, trn_y] = dumpfeatures2(trn_i, fv804); toc;
  tic; [dev_x, dev_y] = dumpfeatures2(dev_i, fv804); toc;
  fprintf('k_perceptron_multi_train_gpu...\n');
  gpuDevice(1);
  tic;m1=k_perceptron_multi_train_gpu(trn_x,trn_y,m0); toc;
  tic;[a,b]=model_predict_gpu(dev_x, m1, 1); toc;
  score = numel(find(a ~= dev_y))/numel(dev_y);
  fprintf('score:%g\t%s\n', score, emi);
end % for i=1:numel(em)


2014-07-28  Deniz Yuret  <dyuret@ku.edu.tr>

	* eval_conll.m: this is not giving the same result as
	below.  testing vectorparser.m.
	- OK, all whead numbers were wrong! Fixed trainparser_gpu.
	- ptrans were different, fixed eval_conll.m
	- Confirm that total cost is equal to total head error.
	- It is not, more debugging of archybrid.oracle_cost is needed.
	- Done, total cost is equal to total head error.
	- DONE: do not use old ArcHybrid.m any more.

	* archybrid.m:
	- However now ptrans does not match old results exactly either!
	- Neither does the number of transitions?
  	  Why: only multi-choice transitions were used and their number changed with new archybrid.
	  Confirm: dev1 has 1700 sents, 40117 words, 2n-2=76834 transitions.

	  old dev_x: (1768,72551)
	  [x0,y0] = dumpfeatures(dev_w, dev_h); % (1768,72551): uses old ArcHybrid.m, only multi-choice transitions
	  [x1,y1] = dumpfeatures(dev_w, dev_h, 1); % (1768,76834): all transitions with ArcHybrid.m
	  [x2,y2] = dumpfeatures2(dev1, fv1768); % (1768,72555): uses new archybrid.m
	  [x3,y3] = dumpfeatures2(dev1, fv1768, 1); % (1768,76834): all transitions
	  m0.parser = @archybrid; m0.feats=fv1768;
	  [~,d4] = vectorparser(m0, dev1, 'update', 0, 'predict', 0); % (1768,76834): all transitions
	  check multi-choice transition count: 72555 confirmed.
	  numel(find(sum(isinf(d4.cost)) < 2)) => 72555

	* vectorparser.m: trainparser, testparser, dumpfeatures etc do
	very similar things.  We'll combine them all in this program.

	- Confirmed works correctly for dump, however all moves are
	dumped, not just the multi-choice ones.

	[m,d] = vectorparser(m0, dev1, 'update', 0, 'predict', 0);
	[x0,y0]=dumpfeatures2(dev1, fv808, 1);
	all(d.x(:) == x0(:)) => 1
	all(d.y(:) == y0(:)) => 1

	- Confirm for test.

	r=trainparser_gpu(mx5d1, dev1, fv808)
	[mz5,dz5]=vectorparser(mx5d1, dev1, 'update', 0);
	eval_conll(dev1, dz5)  % gives the same results

	- Confirm for train.

	mx5d1.parser = @archybrid; mx5d1.feats=fv808;
	[mz5d2,dz5d2] = vectorparser(mx5d1, trn1)
	[rt5d2, mx5d2] = trainparser_gpu(mx5d1, trn1, fv808)
	all(mx5d2.SV(:)==mz5d2.SV(:)) => 1
	all(mx5d2.beta(:)==mz5d2.beta(:)) => 1
	all(mx5d2.beta2(:)==mz5d2.beta2(:)) => 1

	- test if saving single-choice moves in dump makes a big
	difference in static oracle training, if it does find a way to
	filter the multi-choice instances.

	hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
	m0=model_init(@compute_kernel,hp);
	m0.parser=@archybrid; m0.feats=fv808;
	[~,trndump] = vectorparser(m0, trn1, 'update', 0, 'predict', 0);
	mz1 = k_perceptron_multi_train_gpu(trndump.x, trndump.y, m0);
	[~,devdump] = vectorparser(m0, dev1, 'update', 0, 'predict', 0);
	az1 = model_predict_gpu(devdump.x, mz1, 1);
	numel(find(az1 ~= devdump.y))/numel(devdump.y) => 0.0363=2786/76834
	Compare to mx1 which had 0.0375=2723/72551 or 0.0354=2723/76834

	So it does help a bit in first epoch (0.1%) to filter single-choice moves before training.
	Saved dump data as trnhybrid.mat and devhybrid.mat.

	DONE- Dump, train, test, and confirm for arceager.

	DONE- Remove trainparser, testparser, dumpfeatures variants.

	DONE- Start featselect with arceager.

	* arceager.m: should modify trainparser to also do dumpfeatures so
	we can test.  Checking oracle_cost.  Now checking archybrid oracle
	cost.  They both passed the test.  DONE: create two versions of
	arceager for testing, my version and the GoldbergNivre version.
	First try with a standard feature set, then do feature
	optimization.

	* run_dynamic_oracle2.m: with the new fv808 embeddings and mx5 model.  best at 7 epochs.
	--i11			TEST				TRAIN
	model	nsv	time	gtrans	ptrans	phead	whead	ptrans	phead	whead
	mx5	169135	963	3.34	3.76	9.83	8.63	-	-
	mx5d1	204369	22472	4.07	3.78	10.02	8.83	3.02	8.37	7.42
	mx5d2	232151	?	4.02	3.63	9.59	8.47	2.73	7.60	6.77
	mx5d3	254535	?	3.99	3.49	9.23	8.16	2.45	6.88	6.13
	mx5d4	273226	?	4.00	3.47	9.20	8.12	2.23	6.28	5.61
	mx5d5	289077	?	3.99	3.39	9.02	7.98	2.02	5.73	5.11
	mx5d6	302335	?	3.99	3.39	8.96	7.92	1.83	5.24	4.67
	mx5d7	314292	?	4.00	3.37	8.90	7.88	1.71	4.92	4.40
	mx5d8	324433	?	4.02	3.42	9.05	8.00	1.56	4.54	4.06
	mx5d9	333380	?	4.04	3.41	9.02	7.98	1.46	4.25	3.80
	mx5d10	341257	?	4.04	3.41	9.00	7.94	1.36	4.00	3.59

	* run_dynamic_oracle.m: --i10 best results in 9 dynamic oracle epochs.  However this is with fv804, fv808 is better.
				TEST				TRAIN
	model	nsv	time	gtrans	ptrans	phead	whead	ptrans	phead	whead
	m10	536294	21252	3.40	3.76	 9.89
	m10c	202888	-	3.40	3.76	 9.89	8.68
	m10d	230798	27415	4.07	3.84	10.15		2.84	7.90
	m10d2	251746	30510	4.06	3.67	 9.68		2.58	7.21
	m10d3	268378	?	4.07	3.59	 9.46		2.35	6.60	5.89
	m10d4	282674	31102	4.07	3.56	 9.39	8.22	2.21	6.23	5.57
	m10d5	303011	?	4.05	3.55	 9.33	8.19	2.06	5.84	5.22
	m10d6   305964	?	4.03	3.50	 9.18	8.06	1.93	5.51	4.93
	m10d7	315482	?	4.05	3.48	 9.15	8.05	1.82	5.20	4.69
	m10d8	324031	?	4.06	3.49	 9.16	8.05	1.73	4.98	4.49
	m10d9	331568	?	4.04	3.46	 9.12	8.03	1.62	4.69	4.23
	m10d10	338353	?	4.06	3.47	 9.16	8.08	1.54	4.45	4.03

	* run_embeddings.m: restarted again on after solving the issues
	below.  Compare the following to m1 with 0.0401 (aer:6.48) from
	conllWSJToken_wikipedia2MUNK-50.  sv is number of support vectors
	at the end of one epoch.  n is the training set size, 1720842 when
	(or 1720986 after Jul-29) not specified.

	score:0.0370498	conllWSJToken_bansal100 (sv:5.58)
	score:0.0395859	conllWSJToken_wikipedia2MUNK-200 (sv:6.28)
	score:0.0396549	conllWSJToken_wikipedia2MUNK-100 (sv:6.44)
	score:0.0401373	conllToken_wikipedia2MUNK-50 (sv:6.48)  -- this is m1
	score:0.0404659	conllWSJToken_wikipedia2MUNK-50
	score:0.0409643	conllToken_wikipedia2MUNK-25 (sv:6.56)
	score:0.0410309	conllWSJToken_wikipedia2MUNK-25
	score:0.0414881	conllWSJToken_rcv1UNK50 (sv:6.72)
	score:0.042067	conllToken_rcv1UNK (sv:6.82)
	score:0.042067	conllWSJToken_rcv1UNK (sv:6.82)
	score:0.042067	conllWSJToken_rcv1UNK25 (sv:6.82)
	score:0.0464087	conllWSJToken_dep-100 (sv:6.07)
	score:0.0477044	conllWSJToken_huang-0.1 (sv:7.30)
	score:0.049	conllWSJToken_dep-WSJ (sv:6.41)
	score:0.0516464	conllWSJToken_cw50scaled (sv:8.62)
	score:0.0519772	conllWSJToken_ungar2-0.1 (sv:8.56)
	score:0.0523769	conllWSJToken_hlbl50scaled (sv:8.78)
	score:0.0532729	conllWSJToken_mikolovWiki-0.1 (sv:8.05)
	score:0.0596959	conllWSJToken_cw25 (sv:9.74)
	score:0.0603713	conllWSJToken_stratos50k5000scaled01 (sv:9.84)
	score:0.0610054	conllWSJToken_gsng1M (sv:9.83)
	score:0.0611018	conllWSJToken_mikolovRCV50scaled01 (sv:10.18)
	score:0.0626732	conllWSJToken_cw25scaled (sv:10.27)
	score:0.063128	conllWSJToken_gsngAE (sv:10.33)
	score:0.0661604	conllWSJToken_gsngALL (sv:10.56)
	score:0.068793	conllWSJToken_yogamata52scaled01 (sv:10.57)
	score:0.0733691	conllWSJToken_yogamata52 (sv:11.03)
	score:0.0763187	conllWSJToken_stratos50k200scaled01 (sv:12.57)
	score:0.0796681	conllWSJToken_manaal48 (sv:13.16)
	score:0.0829072	conllWSJToken_hpca50scaled01 (sv:13.54)
	score:0.0929829	conllWSJToken_manaal48scaled01 (sv:15.55)
	score:0.104547	conllWSJToken_rnn80 (sv:16.37)

	score:0.0409781	conll_bansal100 (sv:5.99)
	score:0.04591	conll_wikipedia2MUNK-200
	score:0.0461581	conll_wikipedia2MUNK-50
	score:0.0463373	conll_wikipedia2MUNK-100
	score:0.0469161	conll_rcv1UNK200
	score:0.0474563	conll_cw100scaled (sv:7.44)
	score:0.0477431	conll_wikipedia2MUNK-25
	score:0.0477844	conll_rcv1UNK100
	score:0.0479085	conll_rcv1UNK50
	score:0.0483108	conll_4scode+f50 (sv:8.00)
	score:0.0485589	conll_2scode+f100 (sv:7.96)
	score:0.0487381	conll_3scode+f100 (sv:7.93)
	score:0.048876	conll_6scode+f50 (sv:8.01)
	score:0.0488897	conll_4scode+f100 (sv:7.93)
	score:0.0489173	conll_6scode+f100 (sv:7.98)
	score:0.0489311	conll_5scode+f100 (sv:7.96)
	score:0.0489973	conll_rcv1UNK
	score:0.049	conll_10scode+f50 (sv:7.96)
	score:0.0490276	conll_7scode+f50 (sv:8.02)
	score:0.0491792	conll_2scode+f50 (sv:8.00)
	score:0.049193	conll_1scode+f100 (sv:7.97)
	score:0.0492619	conll_1scode+f50 (sv:7.99)
	score:0.0494135	conll_9scode+f50 (sv:8.00)
	score:0.0494686	conll_3scode+f50 (sv:8.01)
	score:0.0494686	conll_5scode+f50 (sv:8.02)
	score:0.0495927	conll_8scode+f50 (sv:8.04)
	score:0.0505685	conll_hlbl100scaled
	score:0.0513266	conll_ungar2
	score:0.0519634	conll_cw50scaled (sv:8.52)
	score:0.0523603	conll_ungar2-0.1
	score:0.0529391	conll_stratos100k5000scaled01
	score:0.0531597	conll_wsj-dep
	score:0.0545685	conll_dep-100 (sv:6.72)
	score:0.0549544	conll_dep-WSJ (sv:7.34)
	score:0.0589346	conll_mikolov50
	score:0.0598994	conll_hlbl50scaled
	score:0.0621597	conll_mikolovReuters50
	score:0.0630591	conll_cw25scaled (sv:10.30)
	score:0.0645579	conll_hlbl
	score:0.0658811	conll_stratos100k200scaled01
	score:0.0667255	conll_dep100+wiki25 (sv:10.89)
	score:0.0679622	conll_mikolovWikipedia50
	score:0.0746882	conll_huang-0.1
	score:0.0803253	conll_hpca100scaled01
	score:0.0834953	conll_gsngAE
	score:0.0899869	conll_manaal48
	score:0.0907036	conll_mikolovWikipedia25
	score:0.0923024	conll_yogamata52
	score:0.0934188	conll_gsngALL
	score:0.0940666	conll_gsng1M
	score:0.112555	conll_dep100+mik (sv:17.72)
	score:0.118269	conll_rnn80
	score:0.405623	conll_murphy50
	score:0.401681	conll_murphy50scaled01

	score:0.0396251	conll_wiki50+wiki50 (n:1891402)
	score:0.0399145	conll_wiki25+wiki25 (n:1891402)
	score:0.0414444	conll_rcv1UNK25+rcv1UNK+25 (n:1891402/1891272)
	score:0.0502267	conll_3scode+f25 (sv:8.05 n:1891272)
	score:0.0524734	conll_cw (sv:8.21 n:1891272)
	score:0.0583452	conll_cw25+cw25 (sv:9.44 n:1891272)
	score:0.0586484	conll_cw25 (sv:9.35 n:1891272)
	score:0.0687203	conll_mikW25+mikW25 (n:1891402 after Jul-29)

	score:0.0523636	conll07Token_english (sv:8.39 n:808163)
	score:0.0613307	conll07English_wikipedia2MUNK-50 (sv:9.90 n:808163)

	score:0.0865817	conll07Token_czech (sv:13.04 n:757433)
	score:0.0978749	conll07Czech_czechSKETCH50 (sv:15.10 n:757433)

	score:0.0773805	conll_german+german (sv:12.77 n:1254045)
	score:0.0898668	conll_german2+german2 (n:1261947new)
	score:0.0944709	conll_german (sv:15.02 n:1254045)
	score:0.0998288	conll_german2 (sv:14.43 n:1254045)

	score:0.124902	conll_hungarian (n:301537)
	score:0.126377	conll_hungarian+f (n:301537)

	score:0.146755	conll_polish (n:107202)
	score:0.15181	conll_polish+f (n:107202)

	score:0.132375	conll_swedish (n:131239)
	score:0.13461	conll_swedish+f (n:131239)

	score:0.172941	conll_turkishSKETCH-25 (n:87228)

	--i02 --i03: still oom, do this on balina
	score:0.99 conllWSJToken_levy300 (out of memory at 1405179, sv:126177, nk:11, g:6.4e6)
	score:0.99 conll_levy300 (out of memory at 1596181, sv:165712, nk:263, g:9.8e7)
	score:0.99 conllWSJToken_mikolovGoogleNews300 (out of memory at 1111004, sv:123146, nk:19, g:4.1e6)
	score:0.99 conllWSJToken_murphy50 (out of memory at 1182126, sv:494871, nk:85, g:9.4e7)
	score:0.99 conllWSJToken_murphy50scaled01 (out of memory at 1289463, sv:494698, nk:85, g:9.4e7)

	score:0.99 conll_french has two test files
	score:0.99 conll_french+f has two test files

	score:0.99 conll08_wikipedia2MUNK-25 is empty
	score:0.99 conllToken_cw25scaled is empty
	score:0.99 conllToken_rnn80 is empty

2014-07-27  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_multi_epoch2.m:

	model	aer	time	nsv	gtrans	ptrans	phead	whead	notes
	m_trn		15841	101517	3.78	4.25	11.02		old fulltrain+static model
	m_trn01		52817	251480	3.89	3.94	10.15		old fulltrain+static+dynamic
	m1	6.47	321	111431	4.01	4.47	11.75		one epoch batchsize=1250 fv804
	mx1	6.09	314	104757	3.75	4.26	11.18		fv808 feature-set batch=1000
	m2	5.94	6105	183901	3.64	4.09	10.80		second epoch batchsize=1000
	mx2	4.89	727	133856	3.50	3.95	10.36		all mx are fv808 batch=1000
	m3	5.09	8781	245190	3.50	3.99	10.37		third epoch batchsize=1000
	mx3	4.11	860	150207	3.38	3.80	 9.96
	m4	4.58	11049	299268	3.44	3.91	10.17
	mx4	3.54	930	161208	3.36	3.80	 9.88
	m5	4.21	12990	347577	3.42	3.90	10.17
	mx5	3.21	963	169135	3.34	3.76	 9.83	8.63	best epoch for fv808: gtrans:3.34 phead:9.83 whead:7.60
	m6	3.92	15004	391424	3.42	3.89	10.13
	mx6	2.92	1000	175089	3.35	3.77	 9.86
	m7	3.69	16784	431962	3.43	3.83	10.02
	mx7	2.69	1019	179795	3.35	3.77	 9.90
	m8	3.49	18120	469130	3.42	3.82	10.00
	mx8	2.50	1030	183630	3.36	3.81	 9.95
	m9	3.33	19897	503651	3.41	3.79	 9.99
	mx9	2.35	1047	186983	3.35	3.83	10.01		first epoch where fv808 is worse than fv804
	m10x	3.18	21252	536294	3.40	3.76	 9.89
	m10	3.18	?	204438	3.40	3.76	 9.89
	mx10	2.22	1051	189857	3.34	3.83	10.01
	m11x	3.05	22599	566656	3.39	3.75	 9.85		out of memory, compactify
	m11	3.01	1182	207516	3.39	3.76	 9.89		new gpu version, diff due to reduced batchsize towards the end
	mx11	2.11	1062	192309	3.35	3.87	10.11
	m12x	2.90	1196	210024	3.41	3.76	 9.94		compactified, run on gpu
	m12	2.86	1197	210313	3.39	3.74	 9.86	8.64	best epoch for fv804: ptrans:3.74
	mx12	2.01	1070	194373	3.35	3.86	10.09
	m13	2.70	1215	212685	3.39	3.76	 9.94
	mx13	1.92	1081	196166	3.36	3.86	10.09
	m14	2.56	1233	214830	3.40	3.78	 9.98
	mx14	1.84	1087	197736	3.36	3.85	10.08
	m15	2.44	1239	216702	3.41	3.80	10.05
	mx15	1.76	1095	199189	3.37	3.85	10.06
	m16	2.34	1234	218294	3.41	3.79	10.02
	mx16	1.69	1097	200424	3.37	3.83	10.04
	m17	2.25	1234	219771	3.41	3.80	10.05
	mx17	1.63	1100	201609	3.37	3.85	10.11
	m18	2.17	1226	221062	3.41	3.82	10.07
	mx18	1.57	1105	202676	3.38	3.84	10.07		b43
	m19	2.10	1230	222344	3.42	3.84	10.11
	mx19	1.52	1107	203550	3.37	3.85	10.10
	m20	2.04	1241	223532	3.43	3.82	10.08		b40
	mx20	1.47	1112	204417	3.39	3.89	10.19
	model	aer	time	nsv	gtrans	ptrans	phead	whead	notes


	* featselect_gpu.m: Trying the best feature vector:
	fv808 = [
	%n0 s0 s1 n1 n0l1 s0r1 s1r1l s0l1 s0r1l s2
	  0 -1 -2  1  0   -1   -2    -1   -1    -3;
          0  0  0  0 -1    1    1    -1    1     0;
	  0  0  0  0  0    0   -2     0   -2     0;
	];

	First confirm it does give the same result as featselect:
	dyuret@yunus:~/vectorparser[0]$ grep 'n0,n0l1,n1,s0,s0l1,s0r1,s0r1l,s1,s1r1l,s2' logs/0726-biyofiz-4-3b.log
	5.09	3.71	102805	427.76	n0,n0l1,n1,s0,s0l1,s0r1,s0r1l,s1,s1r1l,s2
	>> newFeatureVectors
	trn1=loadCoNLL('embedded/conllWSJToken_wikipedia2MUNK-50/00/wsj_0001.dp');
	save logs/trn1 trn1
	hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3)
	model_bak = model_init(@compute_kernel,hp);
	model_bak.step = 200;
	model_bak.batchsize = 500;
	[trn1x, trn1y] = dumpfeatures2(trn1, fv808);
	[dev1x, dev1y] = dumpfeatures2(dev1, fv808);
	tic;gpuDevice(1);
	model_perceptron = k_perceptron_multi_train_gpu(trn1x,trn1y,model_bak);
	pred_perceptron_last = model_predict_gpu(dev1x,model_perceptron,0);
	pred_perceptron_av = model_predict_gpu(dev1x,model_perceptron,1);
	telapsed = toc();
	err_last = numel(find(pred_perceptron_last~= dev1y))/numel(dev1y)*100;
	err_av = numel(find(pred_perceptron_av~=dev1y))/numel(dev1y)*100;
	nsv = size(model_perceptron.beta, 2);
	fprintf('%.2f\t%.2f\t%d\t%.2f\t%s\n', ...
          err_last, err_av, nsv, telapsed, fkey);
	5.09	3.71	102805	404.16	Confirmed!

	After confirming this is the right feature set, rerun the
	multi-epoch (don't forget to compactify after every epoch, or
	modify k_perceptron to do multi-epoch and print stats, or output a
	sequence of models).

	* run_multi_epoch.m: When training the second epoch (starting
	with 100K sv) the same relation does not hold, bigger batches are
	faster.  The max speed is about 10x slower than the beginning.  It
	takes 20-25 secs to do 10K.  So one epoch starting at 100K sv
	should take about an hour or two.  Looking at multi-epoch static
	oracle performance.

	* featselect_gpu.m: we can do feature selection with the
	full data!  (using batchsize=500)

	3	8.56	6.01	165880	424.36	n0,s0,s1
	4	7.43	5.19	143744	389.52	n0,(n1),s0,s1
	5	6.14	4.52	123114	374.21	n0,(n0l1),n1,s0,s1
	6	5.63	4.15	113844	427.12	n0,n0l1,n1,s0,(s0r1),s1
	7	5.46	3.93	108832	416.48	n0,n0l1,n1,s0,s0r1,s1,(s1r1l)
	8	5.44	3.88	104300	411.61	n0,n0l1,n1,s0,s0l1,s0r1,s1,(s1r1)  -- previous (5k) best8
	8	5.27	3.82	104968	992.87	n0,n0l1,n1,s0,(s0l1),s0r1,s1,s1r1l
	9	5.29	3.86	105390	399.62	n0,n0l1,n1,s0,s0l1,(s0r),s0r1,s1,s1r1  -- previous (5k) best9
	9	5.13	3.76	104270	398.77	n0,n0l1,n1,s0,s0l1,s0r1,(s0r1l),s1,s1r1l
	10	5.09	3.71	102805	427.76	n0,n0l1,n1,s0,s0l1,s0r1,s0r1l,s1,s1r1l,(s2)
	11	5.27	3.70	103133	430.94	n0,n0l1,n1,s0,s0l1,s0r1,s0r1l,s1,(s1l1r),s1r1l,s2

	Got interrupted.  To restart we need to reconstruct the cache from
	textual output:

	$ ./featselect-cache.pl ~/yunus/parser/logs/0724-ilac-2-1.log >	~/yunus/parser/logs/0724-ilac-2-1.cache
	>> c0 = featselect_cache('logs/0724-ilac-2-1.cache');
	>> save logs/0724-ilac-2-1.mat c0
	>> [a,b,c] = featselect_gpu(trn_x, trn_y, dev_x, dev_y, 3, 'n0,s0,s1', c0);


	* dynamic-oracle:
	>> load m10
	>> m10c = compactify(m10)
	>> trn1=loadCoNLL('embedded/conllWSJToken_wikipedia2MUNK-50/00/wsj_0001.dp');
	>> newFeatureVectors;
	>> [rt10d, m10d] = trainparser_gpu(m10c, trn1, fv804);
	>> [at10d,~]=model_predict_gpu(trn_x(idx,:), m10d, 1);
	>> r10d = trainparser_gpu(m10d, dev1, fv804);  % check same as t10d
	>> [a10d,~]=model_predict_gpu(dev_x(idx,:), m10d, 1);
	>> [rt10d2, m10d2] = trainparser_gpu(m10d, trn1, fv804); % second epoch
	>> [rt10d3, m10d3] = trainparser_gpu(m10d2, trn1, fv804); % third epoch

	DONE: compactify does not work after trainparser_gpu.  It is
	hopeless to try to do this in a multi-epoch experiment with a
	dynamic oracle because the sequence of instances keep changing.
	We need to modify compactify and model_sparsify to not look at
	model.S.  Fixed compactify and added it to the end of
	trainparser_gpu.  Still need to fix model_sparsify (LATER) and
	k_perceptron_multi_train_gpu (compactify not part of dogma?).



==> run/run_dynamic_oracle2.m <==
load logs/mx5
newFeatureVectors
idx808 = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s1r1l','s0l1','s0r1l','s2'});

toc;tic;[rt10d1, mx5d1] = trainparser_gpu(mx5, trn1, fv808)
toc;tic;[a10d1,~]=model_predict_gpu(dev_x(idx808,:), mx5d1, 1);
numel(find(a10d1 ~= dev_y))/numel(dev_y)
toc;tic;r10d1 = trainparser_gpu(mx5d1, dev1, fv808)
save -v7.3 logs/mx5d1 mx5d1 rt10d1 r10d1 a10d1

toc;tic;[rt10d2, mx5d2] = trainparser_gpu(mx5d1, trn1, fv808)
toc;tic;[a10d2,~]=model_predict_gpu(dev_x(idx808,:), mx5d2, 1);
numel(find(a10d2 ~= dev_y))/numel(dev_y)
toc;tic;r10d2 = trainparser_gpu(mx5d2, dev1, fv808)
save -v7.3 logs/mx5d2 mx5d2 rt10d2 r10d2 a10d2

toc;tic;[rt10d3, mx5d3] = trainparser_gpu(mx5d2, trn1, fv808)
toc;tic;[a10d3,~]=model_predict_gpu(dev_x(idx808,:), mx5d3, 1);
numel(find(a10d3 ~= dev_y))/numel(dev_y)
toc;tic;r10d3 = trainparser_gpu(mx5d3, dev1, fv808)
save -v7.3 logs/mx5d3 mx5d3 rt10d3 r10d3 a10d3

toc;tic;[rt10d4, mx5d4] = trainparser_gpu(mx5d3, trn1, fv808)
toc;tic;[a10d4,~]=model_predict_gpu(dev_x(idx808,:), mx5d4, 1);
numel(find(a10d4 ~= dev_y))/numel(dev_y)
toc;tic;r10d4 = trainparser_gpu(mx5d4, dev1, fv808)
save -v7.3 logs/mx5d4 mx5d4 rt10d4 r10d4 a10d4

toc;tic;[rt10d5, mx5d5] = trainparser_gpu(mx5d4, trn1, fv808)
toc;tic;[a10d5,~]=model_predict_gpu(dev_x(idx808,:), mx5d5, 1);
numel(find(a10d5 ~= dev_y))/numel(dev_y)
toc;tic;r10d5 = trainparser_gpu(mx5d5, dev1, fv808)
save -v7.3 logs/mx5d5 mx5d5 rt10d5 r10d5 a10d5

toc;tic;[rt10d6, mx5d6] = trainparser_gpu(mx5d5, trn1, fv808)
toc;tic;[a10d6,~]=model_predict_gpu(dev_x(idx808,:), mx5d6, 1);
numel(find(a10d6 ~= dev_y))/numel(dev_y)
toc;tic;r10d6 = trainparser_gpu(mx5d6, dev1, fv808)
save -v7.3 logs/mx5d6 mx5d6 rt10d6 r10d6 a10d6

toc;tic;[rt10d7, mx5d7] = trainparser_gpu(mx5d6, trn1, fv808)
toc;tic;[a10d7,~]=model_predict_gpu(dev_x(idx808,:), mx5d7, 1);
numel(find(a10d7 ~= dev_y))/numel(dev_y)
toc;tic;r10d7 = trainparser_gpu(mx5d7, dev1, fv808)
save -v7.3 logs/mx5d7 mx5d7 rt10d7 r10d7 a10d7

toc;tic;[rt10d8, mx5d8] = trainparser_gpu(mx5d7, trn1, fv808)
toc;tic;[a10d8,~]=model_predict_gpu(dev_x(idx808,:), mx5d8, 1);
numel(find(a10d8 ~= dev_y))/numel(dev_y)
toc;tic;r10d8 = trainparser_gpu(mx5d8, dev1, fv808)
save -v7.3 logs/mx5d8 mx5d8 rt10d8 r10d8 a10d8

toc;tic;[rt10d9, mx5d9] = trainparser_gpu(mx5d8, trn1, fv808)
toc;tic;[a10d9,~]=model_predict_gpu(dev_x(idx808,:), mx5d9, 1);
numel(find(a10d9 ~= dev_y))/numel(dev_y)
toc;tic;r10d9 = trainparser_gpu(mx5d9, dev1, fv808)
save -v7.3 logs/mx5d9 mx5d9 rt10d9 r10d9 a10d9

toc;tic;[rt10d10, mx5d10] = trainparser_gpu(mx5d9, trn1, fv808)
toc;tic;[a10d10,~]=model_predict_gpu(dev_x(idx808,:), mx5d10, 1);
numel(find(a10d10 ~= dev_y))/numel(dev_y)
toc;tic;r10d10 = trainparser_gpu(mx5d10, dev1, fv808)
save -v7.3 logs/mx5d10 mx5d10 rt10d10 r10d10 a10d10


==> run/run_dynamic_oracle.m <==
toc;tic;[rt10d4, m10d4] = trainparser_gpu(m10d3, trn1, fv804)
toc;tic;[a10d4,~]=model_predict_gpu(dev_x(idx,:), m10d4, 1);
numel(find(a10d4 ~= dev_y))/numel(dev_y)
toc;tic;r10d4 = trainparser_gpu(m10d4, dev1, fv804)
save -v7.3 logs/m10d4 m10d4 rt10d4 r10d4 a10d4

toc;tic;[rt10d5, m10d5] = trainparser_gpu(m10d4, trn1, fv804)
toc;tic;[a10d5,~]=model_predict_gpu(dev_x(idx,:), m10d5, 1);
numel(find(a10d5 ~= dev_y))/numel(dev_y)
toc;tic;r10d5 = trainparser_gpu(m10d5, dev1, fv804)
save -v7.3 logs/m10d5 m10d5 rt10d5 r10d5 a10d5

toc;tic;[rt10d6, m10d6] = trainparser_gpu(m10d5, trn1, fv804)
toc;tic;[a10d6,~]=model_predict_gpu(dev_x(idx,:), m10d6, 1);
numel(find(a10d6 ~= dev_y))/numel(dev_y)
toc;tic;r10d6 = trainparser_gpu(m10d6, dev1, fv804)
save -v7.3 logs/m10d6 m10d6 rt10d6 r10d6 a10d6

toc;tic;[rt10d7, m10d7] = trainparser_gpu(m10d6, trn1, fv804)
toc;tic;[a10d7,~]=model_predict_gpu(dev_x(idx,:), m10d7, 1);
numel(find(a10d7 ~= dev_y))/numel(dev_y)
toc;tic;r10d7 = trainparser_gpu(m10d7, dev1, fv804)
save -v7.3 logs/m10d7 m10d7 rt10d7 r10d7 a10d7

toc;tic;[rt10d8, m10d8] = trainparser_gpu(m10d7, trn1, fv804)
toc;tic;[a10d8,~]=model_predict_gpu(dev_x(idx,:), m10d8, 1);
numel(find(a10d8 ~= dev_y))/numel(dev_y)
toc;tic;r10d8 = trainparser_gpu(m10d8, dev1, fv804)
save -v7.3 logs/m10d8 m10d8 rt10d8 r10d8 a10d8

toc;tic;[rt10d9, m10d9] = trainparser_gpu(m10d8, trn1, fv804)
toc;tic;[a10d9,~]=model_predict_gpu(dev_x(idx,:), m10d9, 1);
numel(find(a10d9 ~= dev_y))/numel(dev_y)
toc;tic;r10d9 = trainparser_gpu(m10d9, dev1, fv804)
save -v7.3 logs/m10d9 m10d9 rt10d9 r10d9 a10d9

toc;tic;[rt10d10, m10d10] = trainparser_gpu(m10d9, trn1, fv804)
toc;tic;[a10d10,~]=model_predict_gpu(dev_x(idx,:), m10d10, 1);
numel(find(a10d10 ~= dev_y))/numel(dev_y)
toc;tic;r10d10 = trainparser_gpu(m10d10, dev1, fv804)
save -v7.3 logs/m10d10 m10d10 rt10d10 r10d10 a10d10



==> run/run_multi_epoch2.m <==
fv808 = [
%n0 s0 s1 n1 n0l1 s0r1 s1r1l s0l1 s0r1l s2
  0 -1 -2  1  0   -1   -2    -1   -1    -3;
  0  0  0  0 -1    1    1    -1    1     0;
  0  0  0  0  0    0   -2     0   -2     0;
];

path('dogma',path);
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
mx0=model_init(@compute_kernel,hp);mx0.batchsize=1000; mx0.step=8;

tic;mx1=k_perceptron_multi_train_gpu(trn1x,trn1y,mx0)
toc;tic;mx1 = compactify(mx1)
toc;tic;[a1,~]=model_predict_gpu(dev1x, mx1, 1);
toc;tic;gtrans1 = numel(find(a1 ~= dev1y))/numel(dev1y)
toc;tic;r1=trainparser_gpu(mx1, dev1, fv808)
toc;tic;save -v7.3 logs/mx1 mx1 a1 r1
toc;

tic;mx2=k_perceptron_multi_train_gpu(trn1x,trn1y,mx1)
toc;tic;mx2 = compactify(mx2)
toc;tic;[a2,~]=model_predict_gpu(dev1x, mx2, 1);
toc;tic;gtrans2 = numel(find(a2 ~= dev1y))/numel(dev1y)
toc;tic;r2=trainparser_gpu(mx2, dev1, fv808)
toc;tic;save -v7.3 logs/mx2 mx2 a2 r2
toc;

tic;mx3=k_perceptron_multi_train_gpu(trn1x,trn1y,mx2)
toc;tic;mx3 = compactify(mx3)
toc;tic;[a3,~]=model_predict_gpu(dev1x, mx3, 1);
toc;tic;gtrans3 = numel(find(a3 ~= dev1y))/numel(dev1y)
toc;tic;r3=trainparser_gpu(mx3, dev1, fv808)
toc;tic;save -v7.3 logs/mx3 mx3 a3 r3
toc;

tic;mx4=k_perceptron_multi_train_gpu(trn1x,trn1y,mx3)
toc;tic;mx4 = compactify(mx4)
toc;tic;[a4,~]=model_predict_gpu(dev1x, mx4, 1);
toc;tic;gtrans4 = numel(find(a4 ~= dev1y))/numel(dev1y)
toc;tic;r4=trainparser_gpu(mx4, dev1, fv808)
toc;tic;save -v7.3 logs/mx4 mx4 a4 r4
toc;

tic;mx5=k_perceptron_multi_train_gpu(trn1x,trn1y,mx4)
toc;tic;mx5 = compactify(mx5)
toc;tic;[a5,~]=model_predict_gpu(dev1x, mx5, 1);
toc;tic;gtrans5 = numel(find(a5 ~= dev1y))/numel(dev1y)
toc;tic;r5=trainparser_gpu(mx5, dev1, fv808)
toc;tic;save -v7.3 logs/mx5 mx5 a5 r5
toc;

tic;mx6=k_perceptron_multi_train_gpu(trn1x,trn1y,mx5)
toc;tic;mx6 = compactify(mx6)
toc;tic;[a6,~]=model_predict_gpu(dev1x, mx6, 1);
toc;tic;gtrans6 = numel(find(a6 ~= dev1y))/numel(dev1y)
toc;tic;r6=trainparser_gpu(mx6, dev1, fv808)
toc;tic;save -v7.3 logs/mx6 mx6 a6 r6
toc;

tic;mx7=k_perceptron_multi_train_gpu(trn1x,trn1y,mx6)
toc;tic;mx7 = compactify(mx7)
toc;tic;[a7,~]=model_predict_gpu(dev1x, mx7, 1);
toc;tic;gtrans7 = numel(find(a7 ~= dev1y))/numel(dev1y)
toc;tic;r7=trainparser_gpu(mx7, dev1, fv808)
toc;tic;save -v7.3 logs/mx7 mx7 a7 r7
toc;

tic;mx8=k_perceptron_multi_train_gpu(trn1x,trn1y,mx7)
toc;tic;mx8 = compactify(mx8)
toc;tic;[a8,~]=model_predict_gpu(dev1x, mx8, 1);
toc;tic;gtrans8 = numel(find(a8 ~= dev1y))/numel(dev1y)
toc;tic;r8=trainparser_gpu(mx8, dev1, fv808)
toc;tic;save -v7.3 logs/mx8 mx8 a8 r8
toc;

tic;mx9=k_perceptron_multi_train_gpu(trn1x,trn1y,mx8)
toc;tic;mx9 = compactify(mx9)
toc;tic;[a9,~]=model_predict_gpu(dev1x, mx9, 1);
toc;tic;gtrans9 = numel(find(a9 ~= dev1y))/numel(dev1y)
toc;tic;r9=trainparser_gpu(mx9, dev1, fv808)
toc;tic;save -v7.3 logs/mx9 mx9 a9 r9
toc;

tic;mx10=k_perceptron_multi_train_gpu(trn1x,trn1y,mx9)
toc;tic;mx10 = compactify(mx10)
toc;tic;[a10,~]=model_predict_gpu(dev1x, mx10, 1);
toc;tic;gtrans10 = numel(find(a10 ~= dev1y))/numel(dev1y)
toc;tic;r10=trainparser_gpu(mx10, dev1, fv808)
toc;tic;save -v7.3 logs/mx10 mx10 a10 r10
toc;

tic;mx11=k_perceptron_multi_train_gpu(trn1x,trn1y,mx10)
toc;tic;mx11 = compactify(mx11)
toc;tic;[a11,~]=model_predict_gpu(dev1x, mx11, 1);
toc;tic;gtrans11 = numel(find(a11 ~= dev1y))/numel(dev1y)
toc;tic;r11=trainparser_gpu(mx11, dev1, fv808)
toc;tic;save -v7.3 logs/mx11 mx11 a11 r11
toc;

tic;mx12=k_perceptron_multi_train_gpu(trn1x,trn1y,mx11)
toc;tic;mx12 = compactify(mx12)
toc;tic;[a12,~]=model_predict_gpu(dev1x, mx12, 1);
toc;tic;gtrans12 = numel(find(a12 ~= dev1y))/numel(dev1y)
toc;tic;r12=trainparser_gpu(mx12, dev1, fv808)
toc;tic;save -v7.3 logs/mx12 mx12 a12 r12
toc;

tic;mx13=k_perceptron_multi_train_gpu(trn1x,trn1y,mx12)
toc;tic;mx13 = compactify(mx13)
toc;tic;[a13,~]=model_predict_gpu(dev1x, mx13, 1);
toc;tic;gtrans13 = numel(find(a13 ~= dev1y))/numel(dev1y)
toc;tic;r13=trainparser_gpu(mx13, dev1, fv808)
toc;tic;save -v7.3 logs/mx13 mx13 a13 r13
toc;

tic;mx14=k_perceptron_multi_train_gpu(trn1x,trn1y,mx13)
toc;tic;mx14 = compactify(mx14)
toc;tic;[a14,~]=model_predict_gpu(dev1x, mx14, 1);
toc;tic;gtrans14 = numel(find(a14 ~= dev1y))/numel(dev1y)
toc;tic;r14=trainparser_gpu(mx14, dev1, fv808)
toc;tic;save -v7.3 logs/mx14 mx14 a14 r14
toc;

tic;mx15=k_perceptron_multi_train_gpu(trn1x,trn1y,mx14)
toc;tic;mx15 = compactify(mx15)
toc;tic;[a15,~]=model_predict_gpu(dev1x, mx15, 1);
toc;tic;gtrans15 = numel(find(a15 ~= dev1y))/numel(dev1y)
toc;tic;r15=trainparser_gpu(mx15, dev1, fv808)
toc;tic;save -v7.3 logs/mx15 mx15 a15 r15
toc;

tic;mx16=k_perceptron_multi_train_gpu(trn1x,trn1y,mx15)
toc;tic;mx16 = compactify(mx16)
toc;tic;[a16,~]=model_predict_gpu(dev1x, mx16, 1);
toc;tic;gtrans16 = numel(find(a16 ~= dev1y))/numel(dev1y)
toc;tic;r16=trainparser_gpu(mx16, dev1, fv808)
toc;tic;save -v7.3 logs/mx16 mx16 a16 r16
toc;

tic;mx17=k_perceptron_multi_train_gpu(trn1x,trn1y,mx16)
toc;tic;mx17 = compactify(mx17)
toc;tic;[a17,~]=model_predict_gpu(dev1x, mx17, 1);
toc;tic;gtrans17 = numel(find(a17 ~= dev1y))/numel(dev1y)
toc;tic;r17=trainparser_gpu(mx17, dev1, fv808)
toc;tic;save -v7.3 logs/mx17 mx17 a17 r17
toc;

tic;mx18=k_perceptron_multi_train_gpu(trn1x,trn1y,mx17)
toc;tic;mx18 = compactify(mx18)
toc;tic;[a18,~]=model_predict_gpu(dev1x, mx18, 1);
toc;tic;gtrans18 = numel(find(a18 ~= dev1y))/numel(dev1y)
toc;tic;r18=trainparser_gpu(mx18, dev1, fv808)
toc;tic;save -v7.3 logs/mx18 mx18 a18 r18
toc;

tic;mx19=k_perceptron_multi_train_gpu(trn1x,trn1y,mx18)
toc;tic;mx19 = compactify(mx19)
toc;tic;[a19,~]=model_predict_gpu(dev1x, mx19, 1);
toc;tic;gtrans19 = numel(find(a19 ~= dev1y))/numel(dev1y)
toc;tic;r19=trainparser_gpu(mx19, dev1, fv808)
toc;tic;save -v7.3 logs/mx19 mx19 a19 r19
toc;

tic;mx20=k_perceptron_multi_train_gpu(trn1x,trn1y,mx19)
toc;tic;mx20 = compactify(mx20)
toc;tic;[a20,~]=model_predict_gpu(dev1x, mx20, 1);
toc;tic;gtrans20 = numel(find(a20 ~= dev1y))/numel(dev1y)
toc;tic;r20=trainparser_gpu(mx20, dev1, fv808)
toc;tic;save -v7.3 logs/mx20 mx20 a20 r20
toc;

==> run/run_multi_epoch.m <==
m10=compactify(m10)

tic;m11=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m10)
toc;tic;m11 = compactify(m11)
toc;tic;[a11,b11]=model_predict_gpu(dev_x(idx,:), m11, 1);
toc;tic;gtrans11 = numel(find(a11 ~= dev_y))/numel(dev_y)
toc;tic;r11=trainparser_gpu(m11, dev1, fv804)
toc;tic;save -v7.3 logs/m11 m11 a11 b11 r11
toc;

tic;m12=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m11)
toc;tic;m12 = compactify(m12)
toc;tic;[a12,b12]=model_predict_gpu(dev_x(idx,:), m12, 1);
toc;tic;gtrans12 = numel(find(a12 ~= dev_y))/numel(dev_y)
toc;tic;r12=trainparser_gpu(m12, dev1, fv804)
toc;tic;save -v7.3 logs/m12 m12 a12 b12 r12
toc;

tic;m13=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m12)
toc;tic;m13 = compactify(m13)
toc;tic;[a13,b13]=model_predict_gpu(dev_x(idx,:), m13, 1);
toc;tic;gtrans13 = numel(find(a13 ~= dev_y))/numel(dev_y)
toc;tic;r13=trainparser_gpu(m13, dev1, fv804)
toc;tic;save -v7.3 logs/m13 m13 a13 b13 r13
toc;

tic;m14=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m13)
toc;tic;m14 = compactify(m14)
toc;tic;[a14,b14]=model_predict_gpu(dev_x(idx,:), m14, 1);
toc;tic;gtrans14 = numel(find(a14 ~= dev_y))/numel(dev_y)
toc;tic;r14=trainparser_gpu(m14, dev1, fv804)
toc;tic;save -v7.3 logs/m14 m14 a14 b14 r14
toc;

tic;m15=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m14)
toc;tic;m15 = compactify(m15)
toc;tic;[a15,b15]=model_predict_gpu(dev_x(idx,:), m15, 1);
toc;tic;gtrans15 = numel(find(a15 ~= dev_y))/numel(dev_y)
toc;tic;r15=trainparser_gpu(m15, dev1, fv804)
toc;tic;save -v7.3 logs/m15 m15 a15 b15 r15
toc;

tic;m16=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m15)
toc;tic;m16 = compactify(m16)
toc;tic;[a16,b16]=model_predict_gpu(dev_x(idx,:), m16, 1);
toc;tic;gtrans16 = numel(find(a16 ~= dev_y))/numel(dev_y)
toc;tic;r16=trainparser_gpu(m16, dev1, fv804)
toc;tic;save -v7.3 logs/m16 m16 a16 b16 r16
toc;

tic;m17=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m16)
toc;tic;m17 = compactify(m17)
toc;tic;[a17,b17]=model_predict_gpu(dev_x(idx,:), m17, 1);
toc;tic;gtrans17 = numel(find(a17 ~= dev_y))/numel(dev_y)
toc;tic;r17=trainparser_gpu(m17, dev1, fv804)
toc;tic;save -v7.3 logs/m17 m17 a17 b17 r17
toc;

tic;m18=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m17)
toc;tic;m18 = compactify(m18)
toc;tic;[a18,b18]=model_predict_gpu(dev_x(idx,:), m18, 1);
toc;tic;gtrans18 = numel(find(a18 ~= dev_y))/numel(dev_y)
toc;tic;r18=trainparser_gpu(m18, dev1, fv804)
toc;tic;save -v7.3 logs/m18 m18 a18 b18 r18
toc;

tic;m19=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m18)
toc;tic;m19 = compactify(m19)
toc;tic;[a19,b19]=model_predict_gpu(dev_x(idx,:), m19, 1);
toc;tic;gtrans19 = numel(find(a19 ~= dev_y))/numel(dev_y)
toc;tic;r19=trainparser_gpu(m19, dev1, fv804)
toc;tic;save -v7.3 logs/m19 m19 a19 b19 r19
toc;

tic;m20=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m19)
toc;tic;m20 = compactify(m20)
toc;tic;[a20,b20]=model_predict_gpu(dev_x(idx,:), m20, 1);
toc;tic;gtrans20 = numel(find(a20 ~= dev_y))/numel(dev_y)
toc;tic;r20=trainparser_gpu(m20, dev1, fv804)
toc;tic;save -v7.3 logs/m20 m20 a20 b20 r20
toc;


2014-07-26  Deniz Yuret  <dyuret@ku.edu.tr>

	* k_perceptron_multi_train_gpu.m: check to see if during
	multi-epoch we are using cpu!  compactify during multi epoch!
	run_multi_epoch.m figure out the memory problem with k_train in
	run_embeddings!  We probably were not using gpu in multi_epoch
	before and now it blows up around 186K sv.  So all speeds in
	run_multi_epoch are wrong, all nsv's are wrong because of
	redundancy but the accuracy results should be correct.

	After fixing the bug, multi-epoch became ten times faster and I
	confirmed the same results for m1->m2 (at least up to the point
	where batchsize was reduced to fit memory).  This, combined with
	compactify should allow us to run one epoch in < 20 mins.

==> run/gpu_train_script.m <==
gpuDevice(1);
m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;
tic();m1=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m0);toc();
tic();m2=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m1);toc();
tic();m3=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m2);toc();
tic();m4=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m3);toc();
tic();m5=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m4);toc();


2014-07-25  Deniz Yuret  <dyuret@ku.edu.tr>

	* model_sparsify.m: Added unique to sort(model.S): in multi-epoch
	models, same index gets added multiple times.  Since we are not
	shuffling they should still be correct.  For example m9 has 201060
	unique SVs out of a 503651 total.  (DONE: take advantage of this
	for multi-epoch training).  Running with m9.  In fact with dynamic
	oracle things are even more confusing, the data we see depends on
	the moves.  So either go to the S+X convention and do not have SV,
	or (better) forget about keeping the original SV and just pick
	randomly (DONE).

	time	iter	nsv	err_tr	err_te	maxdiff	note
	0	0	503651	24373	2475	0	m9 (201060 uniq sv)
	72721	754468	79425	28008	2535	3.37138e+06 m9 sparsified

	model	nsv	aer	time	gtrans	ptrans	phead	notes
	m9	503651	3.33	19897	3.41	3.79	 9.99   201060 uniqSV
	m9s	79425	--	72721	3.49	3.94	10.35	i00

	Conclusion: it is possible to reduce nsv from 200K to 73K with
	some performance hit.  However this does not seem to be worth
	doing right now, given that the number of unique SV stays around
	200K we can fit everything on gpu.  Also the time cost seems a
	lot, should look at that if we ever decide to use this again.


	* trainparser_gpu.m: Adding the weight updates.  Appending a
	single SV is 25ms regardless of which way we turn the matrix.
	Unexpectedly overwriting to existing space takes about the same.
	We'll leave it be for now.  However training would be much faster
	if we could eliminate the redundancy in SVs caused by multi-epoch.
	So I wrote compactify.m to eliminate redundant SV's and add their
	weights.  Testing on m9 before applying dynamic oracle.

	>> m9c = compactify(m9)
	>> trainparser_gpu(m9c, dev1, fv804)
	Elapsed time is 861.776661 seconds.
	Same answer as below.


	* trainparser_gpu.m: rewriting for both test/train.
	>> load m1
	>> dev1=loadCoNLL('embedded/conllWSJToken_wikipedia2MUNK-50/01/wsj_0101.dp');
	>> newFeatureVectors  % defines fv804
	>> r1 = trainparser_gpu(m1, dev1, fv804, 0)
	Elapsed time is 530.643937 seconds.
	npct: 0.1175
	xpct: 0.0447
	wtot: 34681
	werr: 3473
	wpct: 0.0866

	Nonpunct number is wrong, should be 35324.  OK, looked at the
	difference, it is catching 351x$ (which is wrong), and the 292x`
	character (which is right).  I guess eval.pl has a bug with the
	backtick!  Looking at the P deprels like parser.py gets it mostly
	right, except misses one comma, does not take % as punct (which
	sounds right like $)  Anyway I fixed the code so it gives the same
	nonpunct count as eval.pl for now.

	>> r9 = trainparser_gpu(m9, dev1, fv804, 0)
	Elapsed time is 2215.061447 seconds.
	ntot: 40117
	nerr: 4007
	npct: 0.0999
	xtot: 76834
	xerr: 2917
	xpct: 0.0380
	wtot: 35324
	werr: 3089
	wpct: 0.0770 (92.3)

	* DONE:
	- Try smaller word vectors, type based word vectors, other embeddings.
	oo Try /ai/home/vcirik/eParse/run/embedded/conllWSJToken_rcv1UNK50  -- should be easy 1-epoch
	most parsers do better with it.  can do this without waiting for featselect...
	oo or we can make featureindices part of ArcHybrid, that way code should work!
	-- need to finish feature optimization and fix features before
	experimenting with embeddings!
	-- run_embeddings.m: Try all embeddings under
	/mnt/ai/home/vcirik/eParse/run/embedded
	problem is different sized embeddings have different indices so
	idx804 does not work.  better wait until featselection is over and
	use ArcHybrid that just produces the necessary features.

	* features.m: new feature script.  verified the same output as old
	features.  found a bug in old features (variable d overwritten),
	which may have caused distances not being selected.  should try
	again.  (actually no need, this happened at the end of the
	features fn and had no negative effect).  also should try
	different encoding of numbers.  however new features.m twice as
	slow.  no obvious improvement.  presumably shorter feature sets
	will be faster.  hardcoding features will speed up future code.

	>> newFeatureVectors % defines the usual features in new format
	>> [dev1_x, dev1_y] = dumpfeatures2(dev1(1:100), fv804);
	Elapsed time is 5.990231 seconds.
	>> [dev_x, dev_y] = dumpfeatures(dev_w(1:100), dev_h(1:100));
	Elapsed time is 7.681979 seconds.
	>> dev_x2=dev_x(idx804,:);
	>> all(dev_x2(:)==dev1_x(:)) => 1

	Addressed the following, archybrid no longer has the sentence or
	the features: -- make parser and features, embedding dim etc part
	of model or vice versa.  think about software engineering.  conll
	fields.  embeddings.  features.  arceager. forget dogma.

	* loadCoNLL.m: modified to load the whole information.

==> run/run_batchsize.m <==
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3);
m0 = model_init(@compute_kernel, hp);
m0.step = 10;
for bs=500:500:3000
  batchsize = bs
  m0.batchsize = bs;
  gpuDevice(1);
  tic();m1 = k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m0); ...
             toc();
end


2014-07-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	- make sure testparser is efficient.
	-- convert into trainer (like primal case)
	- try dynamic oracle / sparsification on top of multi-epoch.
	-- we can do dynamic oracle by running the parser and collecting data
	-- is running the parser faster?  try on small sets.  we need to speed up dyn oracle.
	-- 38wps seems to be max

	* DONE:
	- Need performance numbers excluding punctuation on section 23.
	oo change loadCoNLL.m to load all fields and testparser to report punc and non-punc scores.
	-- or testparser can output conll format to be fed into eval.pl.
	-- wsj22 has 40117 tokens 35324 non-punc according to all versions of eval.pl.
	-- Honnibal says: I then converted the gold-standard trees from WSJ 22, for the evaluation. Accuracy scores refer to unlabelled attachment score (i.e. the head index) of all non-punctuation tokens.
	-- Nivre says: The numbers are excluding punctuation on the standard split (2-21, 23).
	-- Husnu says: Asagidaki wordler icin parent prediction basarimi hesaba katilmiyor.
	if (strcmp(w->postag, ",") != 0 && strcmp(w->postag, ":") != 0 && strcmp(w->postag, ".") != 0 && strcmp(w->postag, "``") != 0 && strcmp(w->postag, "''") != 0)
	-- eval.pl uses: scalar(Encode::decode_utf8($word)=~ /^\p{Punctuation}+$/).  wsj22 has 35324 non-punc.
	-- parser.py uses: deplabel P or punct.  wsj22 has 4728 P, leaving 35389 non-punc.

	* DONE:
	- bigger models suffer more from copying. pre-allocate in perceptron_train.
	-- daume trick for beta2 in perceptron_train.
	-- are we using singletons to train or not?
	-- transpose cost anything in perceptron_train?

	* DONE:
	x gpu multiply how can it be so slow?  how can it improve our
	other code?  (e.g. model_predict, sparsify etc)?
	x New version of primal (with turan) should get same accuracy. it doesn't!
	r0=trainparser_primal_12(dev_w(1:50),dev_h(1:50),idx708,w0,0);
	% s=50 w=1175 we=0.4647 m=2250 me=0.1720 wps=10.20 Elapsed time is 115.170739 seconds.
	r1=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w1,0);
	% s=50 w=1175 we=0.4247 m=2250 me=0.1631 wps=14.33 Elapsed time is 82.015995 seconds.
	x Also compare and confirm match of primal with dual model.
	x Maybe some more feat optimization with dynamic oracle.
	x Do a kernel x feature exhaustive experiment.
	x Try second degree primal?
	x Adding dep labels help?  Only .0007 according to ZN11.

	* DONE:
	x try other algorithms that do better in one epoch (pegasos, pa) after converting to minibatch. needs code.
	-- Effect of learning algorithm: pegasos, pa etc. on perf, feats.
	-- Could actually try on 5k before converting to minibatch.  running...
	-- Figure out the right way to run pegasos, make it multiclass...
	-- Figure out opt params for pa, make it gpu-minibatch...
	-- Just try pa for one epoch: not impressed...

	* multi-epoch:
	>> m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;
	>> tic();m1=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m0);toc();
	Elapsed time is 321.474055 seconds.
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m1, 1); toc();
	Elapsed time is 26.001716 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0401  % avg model
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m1, 0); toc();
	Elapsed time is 26.334112 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0482  % last model
	>> save m1.mat m1
	>> tic();m2=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m1);toc();
	Elapsed time is 6105.521107 seconds.
	>> save m2.mat m2
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m2, 1); toc();
	Elapsed time is 44.278723 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0364
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m2, 0); toc();
	Elapsed time is 42.569236 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0524
	>> tic(); r1=testparser(m1, dev_w, dev_h, idx); toc();
	Elapsed time is 2214.591667 seconds.
	>> r1
	ntot: 40117
	nerr: 4715  (0.1175)
	xtot: 76834
	xerr: 3433  (0.0447)
	>> tic(); r2=testparser(m2, dev_w, dev_h, idx); toc();
	Elapsed time is 3468.987827 seconds.
	>> r2
	ntot: 40117
	nerr: 4333  (0.1080)
	xtot: 76834
	xerr: 3142  (0.0409)
	>> tic(); r3=testparser(m3, dev_w, dev_h, idx); toc();r3
	Elapsed time is 4621.268330 seconds.
	>> r3
	ntot: 40117
	nerr: 4161  (0.1037)
	xtot: 76834
	xerr: 3046  (0.0399)


	* testparser_gpu.m: Here is the wps for various things for a model
	with 250K SV (model_trn01):

	dumpfeatures: 313wps -- does not use a model, just gold parser moves
	testparser: 12wps
	testparser_par: 24wps
	testparser_gpu: 8wps -- improved to 38wps by the end of this note
	model_predict_gpu: 700wps -- advantage of minibatch processing

	Dynamic oracle requires at least testparser cost.  Parallelization
	is limited because the next state is determined by the last move.
	If we keep the model fixed we could use kernel caching.  It would
	be fast to cache gold contexts and moves.  Add new elements to
	cache when we stray from gold path for future epochs.  But model
	updated and cache useless before the next epoch.  Need a hash
	table with several million context/score entries.

	Is gpu useless even when SV matrix is large?  Maybe try dot
	instead of mtimes?  Let's profile testparser_gpu.  Testing with
	model_trn with SV:804x101517.

	Is bsxfun better than mtimes for vector x matrix?  Not so, in fact
	5x slower.

	>> s=gpuArray(rand(804,100000));
	>> x=gpuArray(rand(804,1));
	>> str=s';
	>> tic;for i=1:1000 k2=sum(bsxfun(@times,s,x));wait(gpuDevice);end; toc;
	Elapsed time is 20.198047 seconds.
	>> tic;for i=1:1000 k1=str*x;wait(gpuDevice);end; toc;
	Elapsed time is 4.740397 seconds.
	>> max(abs(k1(:)-k2(:)))
	3.9790e-13

	But both at least 5x faster than the cpu:
	>> s=rand(804,100000);
	>> x=rand(804,1);
	>> str=s';
	>> tic;for i=1:1000 k1=str*x;end; toc;
	Elapsed time is 23.783135 seconds.
	>> tic;for i=1:10 k2=sum(bsxfun(@times,s,x));end; toc;
	Elapsed time is 3.486507 seconds.

	How about the skinny beta * K_x operation?  It seems like the best
	option is to use bsxfun+sum on the gpu.  Faster than mtimes or dot.

	>> b=rand(3,100000);
	>> c=b';
	>> k=rand(100000,1);
	>> bgpu=gpuArray(b);
	>> cgpu=gpuArray(b);
	>> kgpu=gpuArray(k);
	>> tic;for i=1:10000 f1=b*k;end;toc
	Elapsed time is 3.429669 seconds.
	>> tic;for i=1:10000 f2=sum(bsxfun(@times,c,k));end;toc
	Elapsed time is 24.121618 seconds.
	>> max(abs(f1(:)-f2(:)))
	1.9281e-10
	>> tic;for i=1:10000 f1gpu=bgpu*kgpu;end;wait(gpuDevice);toc
	Elapsed time is 84.003939 seconds.
	>> max(abs(f1(:)-f1gpu(:)))
	1.8190e-10
	>> tic;for i=1:10000 f2gpu=sum(bsxfun(@times,cgpu,kgpu));end;wait(gpuDevice);toc
	Elapsed time is 4.584367 seconds.
	>> max(abs(f1(:)-f2gpu(:)))
	1.8190e-10
	>> tic;for i=1:10000 f1x=gather(bgpu)*gather(kgpu);end;toc
	Elapsed time is 27.313679 seconds.
	>> tic;for i=1:10000 for j=1:3 f1gpu(j)=dot(cgpu(:,j),kgpu);end;end;wait(gpuDevice);toc
	Elapsed time is 10.073763 seconds.
	>> max(abs(f1gpu(:)-f1(:)))
	1.8554e-10

	Applying the bsxfun trick to testparser_gpu makes a difference.
	On dev(1:100), 2449 words, the speed goes from 8wps to 38wps.
	Regular testparser is 12wps, parfor version with 12 workers is
	24wps.  Can't use parfor and gpu together.  So 38wps is as fast as
	it gets for dynamic-oracle or any problem where we have to
	evaluate contexts one by one.  This is 25000 secs (7 hrs) for the
	ptb (for a 250K SV model).

	Before:
	>> tic(); r=testparser_gpu(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 295.032944 seconds.

	After:
	>> tic(); r=testparser_gpu(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 64.945317 seconds.
	>> tic(); r=testparser_par(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 129.508759 seconds.
	>> tic(); r=testparser(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 290.181428 seconds.


	* k_pa_multi_train: run for one epoch unoptimized.

	>> m0=model_init(@compute_kernel,hp);
	>> tic();m1pa=k_pa_multi_train(trn_x(idx,:),trn_y,m0);toc();
	#1721 SV:16.33(281074)	AER: 5.01
	Elapsed time is 87270.739688 seconds.

	This is way too slow but comparable to m3 in terms of aer and nsv.

	PA models have sparse betas for some reason?
	>> issparse(m1pa.beta2) => 1
	>> m1pa.beta = full(m1pa.beta);
	>> m1pa.beta2 = full(m1pa.beta2);
	>> save m1pa.mat m1pa

	It does not generalize as well as m2 (2 epochs of avg perceptron):

	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m1pa, 1); toc();
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0370  % for avg model, better than m1 but worse than m2
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m1pa, 1); toc();
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0470  % for last model

	Conclusion: maybe worth trying to optimize parameters etc on a
	smaller dataset but results are not promising.



2014-07-23  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_idx708.m: try optimized feats for one epoch and compare to
	idx804.  Does worse.  I guess we need to wait until full corpus
	optimization is done.  --i00.

	idx804 = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	idx708=featureindices({'n0','n0l1','n0l2','n1','s0','s0r','s0r1','s0s1','s1'});
	>> m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;
	>> tic();m1=k_perceptron_multi_train_gpu(trn_x(idx708,:),trn_y,m0);toc();
	Elapsed time is 314.146673 seconds. SV:708x118690
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx708,:), m1, 1); toc();
	Elapsed time is 26.159195 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0424  % avg model - compare to 0.0401 with idx804
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx708,:), m1, 0); toc();
	Elapsed time is 26.030733 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0509  % last model - compare to 0.0482 with idx804
	>> tic(); r1=testparser(m1, dev_w, dev_h, idx708); toc();
	Elapsed time is 2074.158120 seconds.
	>> r1
	ntot: 40117
	nerr: 4949  (0.1234) - compare to 0.1175 for idx804
	xtot: 76834
	xerr: 3585  (0.0467) - compare to 0.0447 for idx804


	* parser.py: compare parser.py at one epoch.  OK, first parser.py
	counts as punctuation any token with DEPREL=P and excludes them
	from scoring, reporting 32025/35389 correct for wsj22 after 15
	epochs of dynamic-oracle training.  It is difficult to compare
	this with my results for multiple reasons: excluding punctuation
	-- getting the parser_with_punct.py results with all 40117 tokens
	now.  The way punct is excluded is nonstandard, eval.pl uses
	characters in the word and gets 35324 non-punc tokens.  Dynamic
	oracle means to ramp up is pretty slow during the first few epochs
	(compare model00 with model5k).  The initial results are on the
	training set and do not include weight averaging, only the final
	result is comparable.  The initial results do not exclude
	punctuation.  When punctuation is included the final result of
	parser.py falls 1.5% to 88.9!  So our static parser is already way
	ahead.

	altay:parser[09:25:36(0)]$ make parser_py_model
	awk '{if(NF==0){print $0}else{print $2, $4, $7-1, $8}}' wsj_0001.dp > parser_py_train
	cut -f2,4 wsj_0101.dp | perl -pe 'if(/\S/){s/\t/\//;s/\n/ /;}' > parser_py_test
	awk '{if(NF==0){print $0}else{print $2, $4, $7-1, $8}}' wsj_0101.dp > parser_py_gold
	python parser.py parser_py_model parser_py_train parser_py_test parser_py_gold
	0 0.758
	1 0.816
	2 0.836
	3 0.849
	4 0.860
	5 0.874
	6 0.880
	7 0.886
	8 0.891
	9 0.894
	10 0.896
	11 0.900
	12 0.902
	13 0.904
	14 0.906
	Averaging weights
	Saving model to parser.pickle
	Parsing took 21271.548 ms
	32025 35389 0.904942213682
	altay:parser[12:34:46(11347)]$

	balina:parser[18:52:08(0)]$ python parser_with_punct.py parser_py_model parser_py_train parser_py_test parser_py_gold
	0 0.757
	1 0.815
	2 0.836
	3 0.850
	4 0.859
	5 0.874
	6 0.881
	7 0.886
	8 0.890
	9 0.894
	10 0.897
	11 0.900
	12 0.903
	13 0.905
	14 0.906
	Averaging weights
	Saving model to parser.pickle
	Parsing took 21752.239 ms
	35675 40117 0.889273873919
	balina:parser[21:41:53(10163)]$


==> run/run_idx708.m <==
m0=model_init(@compute_kernel,hp);m0.batchsize=1250; m0.step=8;
tic();m1=k_perceptron_multi_train_gpu(trn_x(idx708,:),trn_y,m0);toc();
show_m1=m1
tic(); [a,b]=model_predict_gpu(dev_x(idx708,:), m1, 1); toc();
numel(find(a ~= dev_y))/numel(dev_y)
tic(); [a,b]=model_predict_gpu(dev_x(idx708,:), m1, 0); toc();
numel(find(a ~= dev_y))/numel(dev_y)
tic(); r1=testparser(m1, dev_w, dev_h, idx708); toc();
show_r1=r1
r1.nerr/r1.ntot
r1.xerr/r1.xtot


2014-07-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* k_perceptron_multi_train_gpu.m: To optimize featselect we need
	to optimize train: look into mini-batch perceptron training so we
	can use gpu: http://acl.cs.qc.edu/~lhuang/papers/minibatch.pdf

	Implementing minibatch training.  Kernel calculation takes 8.5ms
	per 500 batch, 17us per instance.  This is 10x faster than
	yesterdays results, because we are starting with an empty model,
	yesterday we were experimenting with 100K sv.  Updates are done in
	a 500 for loop which is expensive (2921ms).  Vectorized updates
	reduce the whole operation (kernel plus update) to 27ms per 500
	minibatch.  This means 20K instances/sec, 86 seconds for the whole
	ptb?  Well it slows down as we collect more SV and one epoch takes
	432 secs at a slight accuracy cost:

	old:
	>> model_trn = k_perceptron_multi_train(trn_x(idx,:), trn_y, model_trn);
	#1720 SV: 5.90(101473)	AER: 5.90
	Elapsed time is 15841.799743 seconds.
        SV: [804x101517 double]
        pred: [3x1720842 double]

	new:
	m=model_init(@compute_kernel,hp);m.batchsize=500; m.step=10;
	tic();m=k_perceptron_multi_train_gpu(trn_x(idx,:),trn_y,m);toc();
	#1720000 SV: 6.13(105351)	AER: 6.13	t=431.945

	* done: Implemented beta2 ignoring age differences within one batch.

	* done: Fastest batch size seems to be 1250.  Training with 100K:
	batch	time
	250	4.65
	500	3.56
	750	3.22
	1000	3.18
	1250	3.14
	1500	3.18
	1750	3.26
	2000	3.36

	* run_learners_5k.m: compare learners on 5k data.  I think I did
	5k instances instead of 5k sentences.  Rerunning on 5k sentences.
	SVM has an advantage that gets smaller as data gets larger and it
	is too slow.  PA has an advantage on the first epoch but later
	results show that second epoch of avg perceptron beats PA.

	5k instance results
	last	avg	nsv	time	algo
	16.86	14.17	941	190.49	k_perceptron_multi_train
	13.26	12.58	3201	260.74	k_pa_multi_train
	16.86	14.17	941	186.68	k_projectron2_multi_train
	0.00	11.26	2930	21.44	svmtrain

	5k sentence results
	last	avg	nsv	time	algo
	7.01	5.35	18164	1127.07	k_perceptron_multi_train
	5.97	5.17	55428	2657.21	k_pa_multi_train
	7.01	5.35	18145	30339.92 k_projectron2_multi_train
	0.00	5.09	42035	108601.55	svmtrain

	>> save m4svm.mat m4  % ilac-2-2

	old 5k results for comparison
	model	nsv	gtrans	ptrans	phead	time	notes
	mo5k	17318	5.43	5.78	15.14	1939	one epoch static, from scratch
	mo01	40502	5.56	5.36	14.13	6489	static+dynamic, from model5k
	mo02	58979	5.74	5.31	13.79	10679	static+2*dynamic, from model01

	* run_batchsize.m: look at differences in final performance as a
	fn of batchsize.

	batch	nsv	aer	time
	1	101517	5.90	15481.80
	100	102869	5.98	945.91
	500	105390	6.12	401.66
	1000	109144	6.34	335.52
	1500	113888	6.61	325.42
	2000	118131	6.86	336.41
	2500	out-of-memory


==> run/run_learners_5k.m <==
x_tr = trn_x(idx, 1:217603);
y_tr = trn_y(1:217603);
x_te = dev_x(idx, :);
y_te = dev_y;
hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3)
m0 = model_init(@compute_kernel,hp);
fprintf('last\tavg\tnsv\ttime\talgo\n');

tic();
m1 = k_perceptron_multi_train(x_tr, y_tr, m0)
m1ttm = toc();
m1nsv = size(m1.beta, 2);
m1lst = model_predict(x_te, m1, 0);
m1avg = model_predict(x_te, m1, 1);
m1lst_err = numel(find(m1lst ~= y_te))/numel(y_te)*100;
m1avg_err = numel(find(m1avg ~= y_te))/numel(y_te)*100;
fprintf('%.2f\t%.2f\t%d\t%.2f\tk_perceptron_multi_train\n', ...
        m1lst_err, m1avg_err, m1nsv, m1ttm);

tic();
m2 = k_pa_multi_train(x_tr, y_tr, m0)
m2ttm = toc();
m2nsv = size(m2.beta, 2);
m2lst = model_predict(x_te, m2, 0);
m2avg = model_predict(x_te, m2, 1);
m2lst_err = numel(find(m2lst ~= y_te))/numel(y_te)*100;
m2avg_err = numel(find(m2avg ~= y_te))/numel(y_te)*100;
fprintf('%.2f\t%.2f\t%d\t%.2f\tk_pa_multi_train\n', ...
        m2lst_err, m2avg_err, m2nsv, m2ttm);

tic();
m3 = k_projectron2_multi_train(x_tr, y_tr, m0)
m3ttm = toc();
m3nsv = size(m3.beta, 2);
m3lst = model_predict(x_te, m3, 0);
m3avg = model_predict(x_te, m3, 1);
m3lst_err = numel(find(m3lst ~= y_te))/numel(y_te)*100;
m3avg_err = numel(find(m3avg ~= y_te))/numel(y_te)*100;
fprintf('%.2f\t%.2f\t%d\t%.2f\tk_projectron2_multi_train\n', ...
        m3lst_err, m3avg_err, m3nsv, m3ttm);

path('libsvm-3.18/matlab', path);
tic();
m4 = svmtrain(y_tr', x_tr', '-t 1 -d 3 -g 1 -r 1 -c 1')
m4ttm = toc();
m4nsv = m4.totalSV;
[m4lst, m4acc, m4val] = svmpredict(y_te', x_te', m4);
m4lst_err = numel(find(m4lst ~= y_te'))/numel(y_te)*100;
fprintf('%.2f\t%.2f\t%d\t%.2f\tsvmtrain\n', ...
        m4lst_err, 0, m4nsv, m4ttm);


2014-07-21  Deniz Yuret  <dyuret@ku.edu.tr>

	* trainparser_gpu.m: Profiling trainparser to find the ideal
	minibatch size.  When computing kernels for single x's cpu and gpu
	have the same speed (18ms for SV=804x101517).  The gpu advantage
	comes out as we process multiple x's:

	xcnt	cpus/x	gpus/x
	1	16479	16606
	10	4152	3549
	100	2037	427
	250		301
	500		272
	1000	1752	262
	2500	1683	271

	Seems like we hit max performance around 500 x's and memory
	overflows at 2500 x's.  This should speed up static oracle
	training 60x!  Make 500 decisions at once and compare with static
	oracle, apply all updates together.  However dynamic oracle
	requires making decisions sequentially so this won't help us...
	Still, one ptb epoch in 5 mins makes multi-epoch experiments
	easier.

	* model_sparsify_profile.m: Profiling version.  margins is taking
	most of the time, more than twice that of pred_tr update.
	specifically the max(pred_tr) in margins.  It turns out gpu max
	takes 134ms where cpu max on the same data takes 2.5ms.  MAX IS
	EXTREMELY SLOW ON THE GPU.  So changing max(f) with max(gather(f))
	makes the program roughly 2.5x as fast.  This should reduce full
	ptb time from 9 hours to 3.6.

	* model_sparsify_popt.m: Parameter optimization version.  The
	errors are reported at 1000 SV.  Seems same point is (epsilon=0.1,
	margin=1.0, eta=0.3) still efficient.

	epsilon	margin	eta	time	iter	err_tr	err_te
	0.1	1.0	0.3	1590	8392	138754	6827
	0.2	1.0	0.3	1386	7310	138983	6970
	0.05	1.0	0.3	1625	8580	144475	7328
	0.1	2.0	0.3	1590	8392	138752	6827  ?? why so close
	0.1	0.5	0.3	1590	8392	138752	6827  ?? why so close
	0.1	1.0	0.6	898	4704	152362	7786
	0.1	1.0	0.15	2816	14943	145843	7473

2014-07-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* model_sparsify.m: New version uses singles and subsamples the
	data to fit the gpu.  Running with trn_x(idx804,:) Subsampling
	x_tr from 1720842 to 1411972 instances to fit in gpu.  It
	eventually reaches a better point but with more time and more SV.
	Should optimize the parameters on the big set.  Here are the
	results in comparison with the earlier 433k training set results.
	(model_sparsify_0718.log vs model_sparsify_0721.log) The 433K is
	better up to 18000 SVs and about 3x faster (which is linear with
	the size increase in the training set).  Starting parameter
	optimization at 1000 SV experiment (model_sparsify_popt).

	433K results			1.4M results
	nsv	time	iter	err_tr	err_te	time	iter	err_tr	err_te
	101517	0	0	13044	2740	0	0	47820	2740
	1000	610	8368	36997	6464	1590	8392	138754	6827
	2000	1178	16151	27445	4909	3068	16180	104949	5206
	3000	1738	23831	23736	4250	4490	23660	92833	4616
	4000	2260	30972	21570	3948	5876	30951	83229	4098
	5000	2730	37405	20138	3733	7240	38118	78250	3913
	6000	3202	43857	18654	3533	8490	44685	73853	3763
	7000	3621	49586	17835	3452	9733	51209	71192	3656
	8000	4017	54994	17249	3405	10940	57538	68933	3490
	9000	4409	60333	16495	3303	12120	63723	66933	3454
	10000	4766	65200	15888	3275	13256	69666	64980	3355
	11000	5098	69724	15400	3228	14301	75133	63546	3324
	12000	5406	73920	14930	3179	15303	80368	62171	3284
	13000	5703	77945	14552	3138	16357	85883	61033	3264
	14000	5969	81558	14169	3104	17321	90918	59697	3183
	15000	6212	84843	13877	3100	18290	95978	58643	3136
	16000	6432	87807	13601	3058	19215	100810	57913	3105
	17000	6623	90380	13348	3042	20104	105449	57129	3066
	18000	6794	92682	13192	3037	20946	109840	56270	3036
	19000	6944	94680	13002	3044	21781	114189	55358	3008
	20000	7075	96424	12885	3028	22579	118348	54607	3004
	21000	7197	98028	12764	3018	23350	122359	54211	2957
	22000	7311	99540	12686	3007	24076	126138	53686	2936
	22340	7349	100038	12641	2998
	23000			(2.9%)	(4.1%)	24777	129780	53187	2944
	40614					32470	169251	48144	2808
								(3.4%)	(3.9%)

2014-07-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* model_sparsify.m: Look into sparsification with full data.
	>> m0s=model_trn;
	>> m0s.step=10;
	>> idx804 = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	>> p=struct('average',1,'x_te',dev_x(idx,:),'y_te',dev_y,'eta',0.3,'epsilon',0.1,'margin',1.0);
	% edit model_sparsify to turn off gpu completely...
	>> m0sc = model_sparsify(m0s,trn_x(idx,:),trn_y,p);
	% takes too long
	% increase max_num_el to 1e10 (80GB)
	% still takes too long
	% try turning gpu back on but not pushing x_tr
	% It becomes 2x slower than the cpu version!
	% We have to keep x in gpu :(
	% Or just use the cpu at one sv every 5 seconds!
	% Use singles and automatically subsample x_tr.

	* featselect.m: Now reading cache.
	[bestf, besterr, cache] = featselect(x5k(:,10001:end),y5k(10001:end),x5k(:,1:10000),y5k(1:10000),3,'n0,s0,s1',featselect_03_5k)

	* featselect-cache.m,featselect-cache.pl,featselect_03_5k.mat:
	Convert text output to cache file.

2014-07-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* timing: To get model_trn with static oracle from 950028 words we
	spent 15841 seconds (60wps=4.4h/ptb) going from 0 to 100K SVs.
	Dynamic training the next epoch, model_trn01, took 52817 secs
	(18wps=14.7h/ptb) going from 100K to 250K SVs.  trainparser_primal
	goes at a constant speed (13wps=20.3h/ptb).  In dual models, the
	time cost of static and dynamic are roughly the same, the number
	of SVs determine the runtime (see m5s-vs-m5d below).
	Sparsification seems very fast (129wps=2h/ptb) but note that we
	were not able to use the full trn (1720842 move instances) with
	the gpu, we had to subsample.  parser.py finishes 15 epochs at 100
	words/sec (i.e. goes at 1500 words/sec?)!

	full trn experiments:
	model		nsv	gtrans	ptrans	phead	time	notes
	model_trn	101517	3.78	4.25	11.02	15841	fulltrain+static, from scratch
	model_trn_sp	22340	4.13	4.59	11.92	7349	fulltrain+static+sparsify, from trn
	model_trn01	251480	3.89	3.94	10.15	52817	fulltrain+static+dynamic, from trn
	model_trn_sp01	186554	4.14	4.22	10.87	46844	fulltrain+static+sparsify+dynamic, from trn_sp

	5k experiments:
	model		nsv	gtrans	ptrans	phead	time	notes
	model00		29187	9.21	6.78	17.56	?	one epoch dynamic, from scratch
	model5k		17318	5.43	5.78	15.14	1939	one epoch static, from scratch
	model01		40502	5.56	5.36	14.13	6489	static+dynamic, from model5k
	model02		58979	5.74	5.31	13.79	10679	static+2*dynamic, from model01
	m6		12555	5.34	5.72	15.04	9219	static+sparsify, from model5k
	model03b	36073	5.47	5.46	14.18	5467	static+sparsify+dynamic, from m6
	model04		13043	5.35	5.59	14.47	2157	static+sparsify+dynamic+sparsify, from model03b w/gpu?

	* m5s-vs-m5d: m5s takes 593 secs on ilac-0-0 but gets dumpfeatures
	for free which is 358 secs, m5d takes 1251 secs doing its own
	feature extraction.  The small difference is due to the larger
	number of SVs m5d accumulates (17318 vs 27954).

	m0=model_init(@compute_kernel,hp);
	tic(); m5s=k_perceptron_multi_train(x_tr,y_tr,m0); toc();
	% Elapsed time is 592.851920 seconds.
	tic();size(dumpfeatures(trn_w(210:5000),trn_h(210:5000)));toc();
	% Elapsed time is 357.939658 seconds.
	tic(); m5d = trainparser(m0, trn_w(210:5000), trn_h(210:5000), idx); toc();
	% Elapsed time is 1251.707183 seconds.

	* naming: Better naming convention for models:
	1. 'm' is the first char
	2. [0-9]+ indicating the training set size in 1000 sentences
	   use 0 to indicate the full ptb training set.
	3. Followed by a sequence of letters indicating training steps:
	   's' for static oracle (perceptron)
	   'd' for dynamic oracle (trainparser)
	   'c' for compression (sparsify)

	* x5k: we got x5k from the first 5000 sentences of trn.  in the 5k
	experiments we have been using the first 10k features as test and
	the remaining 207603 as training set.  How many sentences does
	this correspond to, so we can run corresponding experiments with
	trainparser?  It turns out the answer is the 209 sentences give
	the first 10013 instances.  So we can use trn_w(1:209) as test and
	trn_w(210:5000) as train.

	* trainparser_primal.m: Now we can also do averaging at a cost of
	25% speed.  At this speed 1 epoch of ptb (950028 words) will take
	18 hours.  Testing on dev (40117 words) is 45 mins.

	% This time with w2 calculation.
	>> [w,w2]=trainparser_primal(trn_w(1:10),trn_h(1:10),idx708);
	% s=10 w=189 wps=14.6609 Elapsed time is 12.891465 seconds.

	% Run old/new versions on (1:50) to confirm:
	[w1,w1b]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	% s=50 w=1259 wps=13.2452 Elapsed time is 95.053105 seconds.
	[w0,w0b]=trainparser_primal_12(trn_w(1:50),trn_h(1:50),idx708);
	% 50 Elapsed time is 136.959744 seconds.  w0b empty.

	% Averaged version should get better accuracy.
	r1b=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w1b,0);
	% s=50 w=1175 we=0.4085 m=2250 me=0.1551 wps=14.32 Elapsed time is 82.079185 seconds.

	% Check the averaging calculation by comparing with slow version.
	[nv,v,v2,v2b]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	% s=50 w=1259 we=0.4845 m=2418 me=0.1729 wps=8.04 Elapsed time is 156.651738 seconds.
	all(w1(:)==v(:)) => 1
	all(w1b(:)==v2(:)) => 1
	all(v2(:)==v2b(:)) => 0
	max(abs(v2(:)-v2b(:))) => 0.0029  % we have rounding errors
	% Daume's trick could be numerically less stable :(

	% Try adding 1 as x0.  Definitely improves the averaged results.
	[n2,w2,w2b]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	% s=50 w=1259 we=0.4599 m=2418 me=0.1625 wps=12.12 Elapsed time is 103.896044 seconds.
	% n2.f=709 -- used to be 708
	r2=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w2,0);
	% s=50 w=1175 we=0.4409 m=2250 me=0.1733 wps=14.26 Elapsed time is 82.371071 seconds.
	% compare to r1 we=0.4247 me=0.1631
	% compare to r0 we=0.4647 me=0.1720
	r2b=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w2b,0);
	% s=50 w=1175 we=0.3651 m=2250 me=0.1378 wps=14.28 Elapsed time is 82.311780 seconds.
	% compare to r1b we=0.4085 me=0.1551

	% How slow is the non-gpu version?
	[n3,w3,w3b]=trainparser_primal_nogpu(trn_w(1:50),trn_h(1:50),idx708);
	% s=11 w=222 we=0.5811 m=422 me=0.1848 wps=0.56 Elapsed time is 396.590939 seconds.
	% compare to n2 with wps=12.12 about 20x faster

	% BAD IDEA: is this good for learning?  try moving after max and
	% learn from guessing impossible moves.
	% scores(cost == inf) = -inf;
	% [maxscore, move] = max(scores);
	[n2,w2,w2b]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	% s=50 w=1259 we=0.4599 m=2418 me=0.1625 wps=12.12 Elapsed time is 103.896044 seconds.
	r2b=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w2b,0);
	% s=50 w=1175 we=0.3651 m=2250 me=0.1378 wps=14.28 Elapsed time is 82.311780 seconds.
	% Now after the mod:
	[n4,w4,w4b]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	% s=50 w=1259 we=0.4821 m=2418 me=0.2047 wps=13.34 t=94.41
	r4b=trainparser_primal(dev_w(1:50),dev_h(1:50),idx708,w4b,0);
	% s=50 w=1175 we=0.3736 m=2250 me=0.1516 wps=16.79 t=69.98

	* turan.m: Sule suggested Turan's theorem to construct the filter
	matrix.  This reduces the redundancy in the matrix representation
	of triples, halves memory use and doubles the speed:

	>> [w,w2]=trainparser_primal(trn_w(1:10),trn_h(1:10),idx708);
	Training in 8.89744e+07 dims...
	% Used to be Training in 1.77698e+08 dims...
	Sentences=10 Words=189 Words/Sec=17.4957
	Elapsed time is 10.802716 seconds.
	% Used to be 20.96 seconds with wps=9.

	* model_trn_sparse01.mat: fulltrn-static+sparsify+dynamic.
	#3541 SV: 5.27(186537)	AER: 7.50
	Elapsed time is 46844.421232 seconds.

	* trainparser_primal.m: Test training with primal vectors for
	efficiency.  It does 7.5 wps, which would finish PTB in 35 hours.
	Compared to dual models it is not very good.  Also gpu memory is
	limited and can only handle 788 dims (at least until I can figure
	out how to reduce dims to n(n+1)(n+2)/6.)

	>> [w,w2]=trainparser_primal(trn_w(1:10),trn_h(1:10),idx708);
	Training in 1.77698e+08 dims...
	Sentences=10 Words=189 Words/Sec=9.01343
	Elapsed time is 20.968780 seconds.
	>> size(model_trn01.SV)
         804      251480
	>> tic();m=trainparser_gpu(model_trn01,trn_w(1:10),trn_h(1:10),idx);toc();
	Elapsed time is 28.091093 seconds.
	>> tic();m=trainparser(model_trn01,trn_w(1:10),trn_h(1:10),idx);toc();
	Elapsed time is 16.848159 seconds.

	- ok not working again.  i give up.  either singles causing cuda
	failure.  or multiply not working.  this is all messed up.  fixed
	when I sprinkled some wait(gpuDevice) in the code.

	>> [w,w2]=trainparser_primal(trn_w,trn_h,idx708);
	Sentences=39832 Words=950028 Words/Sec=9.4902
	Elapsed time is 100106.226213 seconds.
	M-x write-region trainparser_primal_0719.log
	save trainparser_primal_0719.mat w w2
	This is the first version of trainparser_primal,
	i.e. unnecessarily large w, no w2, no 1 added...

	* profile: Use matlab profiler (e.g. on dumpfeatures).
	>> profile on -history
	>> [n2p,w2p,w2bp]=trainparser_primal(trn_w(1:50),trn_h(1:50),idx708);
	>> p=profile('info');
	>> profsave(p,'primal-profile');
	Saves nice html file.
	But was useless with GPU code.

2014-07-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* q: gpu instability caused by singles?  no we just need a bunch of wait(gpuDevice)

	* gpu: weird shit is happening.  gpu takes 15 secs to multiply.
	what am i doing wrong?  I think applying matrix multiply for dot
	product is the cause.  MATRIX MULTIPLY IS NOT GOOD FOR SKINNY
	MATRICES ON GPU! Should use 'dot'.
	http://blog.theincredibleholk.org/blog/2012/12/10/optimizing-dot-product/

	* gpu: This is out of control:

	>> a=gpuArray(rand(1,1e8));
	>> b=gpuArray(rand(1,1e8));
	>> tic();c=sum(b.*a);disp(c);toc();
	   2.5000e+07
	Elapsed time is 0.025064 seconds.
	>> tic();c=b*a';disp(c);toc();
	   2.5000e+07
	Elapsed time is 7.178565 seconds.


2014-07-16  Deniz Yuret  <dyuret@ku.edu.tr>

	* poly3basis.c (mexFunction): 0.29 secs to generate one poly 3
	basis vector from a 804 dimensional original vector.  The
	resulting vector size is n(n+1)(n+2)/6 = 86943220 (695MB).  Dot
	product is negligible.  model_trn takes about the same amount of
	time to do kernels with 100K SV for 100 x vectors.

	GPU does it 7 times faster:
	>> x1 = gpuArray(rand(804,1));
	>> xi = gpuArray(triu(true(804,804)));
	>> tic();for i=1:1000 x2=x1*x1';x3=x2(xi)*x1';end; toc();
	Elapsed time is 40.811311 seconds.

	If I could find a better xi filter it could be even faster and
	more memory efficient.  The array indexing operation does not seem
	to slow it down:

	n=500;
	x1=gpuArray(rand(n,1));
	xi=gpuArray(triu(true(n,n)));
	x2=x1*x1';
	size(x2(:))  250000           1
	size(x2(xi)) 125250           1
	tic();for i=1:1000 x2=x1*x1';x3=x2(:)*x1';end; toc();
	% Elapsed time is 14.054163 seconds.
	tic();for i=1:1000 x2=x1*x1';x3=x2(xi)*x1';end; toc();
	% Elapsed time is 8.054333 seconds.

	Does the speed have to do with the shape of xi?  No...

	>> xi=false(n,n);
	>> xi(round(rand(175000,1)*n*n))=true;
	>> sum(xi(:))
	125848
	>> xi=gpuArray(xi);
	>> x1=gpuArray(rand(n,1));
	>> tic();for i=1:1000 x2=x1*x1';x3=x2(xi)*x1';end; toc();
	Elapsed time is 7.933409 seconds.

	Working with single's to save memory the gpuDevice crashes.
	Unless I use wait(gpuDevice).  Does that slow us down?  No.

	>> tic();for i=1:1000 wait(gpuDevice);x2=x1*x1';x3=x2(xi)*x1';end; toc();
	Elapsed time is 8.042499 seconds.
	>> x1=gpuArray(rand(n,1,'single'));
	>> tic();for i=1:1000 wait(gpuDevice);x2=x1*x1';x3=x2(xi)*x1';end; toc();
	Elapsed time is 6.866563 seconds.

	We can now do the full triple product with 804 dimensions, but no need:

	>> tic();for i=1:1000 wait(gpuDevice);x2=x1*x1';x3=x2(:)*x1';end; toc();
	Elapsed time is 60.887271 seconds.
	>> tic();for i=1:1000 wait(gpuDevice);x2=x1*x1';x3=x2(xi)*x1';end; toc();
	Elapsed time is 27.982342 seconds.

	The operations with singles are still a bit unstable.  Getting GPU
	errors half the time.  If I can find a smaller xi, it would be
	more memory efficient and possibly faster.

	* model_trn_sparse: Optimize model_sparsify and try on model03,
	model_trn, model_trn01.  GPU memory is not sufficient to handle
	the full problem.  We have to subsample the training data.

	load dumpfeatures
	load model_trn
	pick = false(1,size(trn_x,2));
	pick(model_trn.S)=true;
	pick(round(rand(1,500000)*size(trn_x,2)))=true;
	sum(pick)
	% 433500
	x_tr500k = trn_x(idx,pick);
	y_tr500k = trn_y(pick);
	model_trn_sparse = model_sparsify(model_trn, x_tr500k, y_tr500k, struct('x_te',dev_x(idx,:),'y_te',dev_y,'eta',0.3,'epsilon',0.1,'margin',1.0));

	time	iter	nsv	err_tr	err_te	maxdiff
	0	0	101517	13044	2740	0
	7349	100038	22340	12641	2998	2.329387e+08

	save model_trn_sparse.mat model_trn_sparse
	>> model_trn_sparse
	SV: [804x22340 double]    %% compare with 101517 of model_trn
        pred: [3x1720842 double]
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model_trn_sparse, 1); toc();
	Elapsed time is 8.466255 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0413   %% compare to 0.0378 of model_trn
	>> tic(); r=testparser(model_trn_sparse, dev_w, dev_h, idx); toc();
	Elapsed time is 560.445457 seconds.
	>> r
	ntot: 40117
	nerr: 4782  (0.1192)
	xtot: 76834
	xerr: 3526  (0.0459)

	* featselect.m: Implemented the SFFS algorithm with backtrack.
	From Guyon's feature selection book pp.149.  Exit when no more
	improvement?  Well it always has a chance to come back after a few
	iterations, so for now run continuously and save in a cache.

	* featselect: do 5k featselect for order6 kernel.  Below is the
	top two for each size, and comparison with order3.   It looks like
	order3 has the advantage after the third feature.

	nfeat	last	avg6	avg3	best3	nsv	time	feats
	2	15.06	11.68	12.27	12.27	30848	289.06	n0,s0
	2	19.19	14.82	14.57	12.27	39803	398.05	n0,s1
	3	10.59	8.36	8.75	8.75	23186	317.80	n0,s0,s1
	3	14.56	11.14	?	8.75	28960	273.85	n0,s0,s0r1r
	4	9.69	7.83	7.74	7.64	21493	300.65	n0,s0,s0r1r,s1
	4	10.86	7.88	8.49	7.64	22723	298.05	n0,n0l2-,s0,s1
	4	9.34	7.88	7.64	7.64	20588	363.11	n0,n0l1,s0,s1
	5	9.66	7.53	?	7.08	21326	392.55	n0,s0,s0r1r,s1,s1r2
	5	9.91	7.53	?	7.08	21063	286.53	n0,n0l2-,s0,s0r1r,s1
	5	9.60	7.58	?	7.08	21106	296.12	n0,s0,s0r1r,s1,s1+
	6	9.67	7.22	?	6.63	21248	287.42	n0,n0l2-,s0,s0r1r,s0r2r,s1
	6	8.95	7.27	?	6.63	19466	347.79	n0,n0l2-,n1,s0,s0r1r,s1
	6	9.76	7.27	?	6.63	21292	291.94	n0,n0+,n0l2-,s0,s0r1r,s1
	6	9.76	7.27	?	6.63	21292	297.71	n0,n0l2-,s0,s0+,s0r1r,s1
	6	9.76	7.27	?	6.63	21292	301.43	n0,n0l2-,n0r,s0,s0r1r,s1

	* dogma/model_sparsify.m: Added gpu support.  Unfortunately gives
	only 50% speedup.  GPU is detected and used automatically if
	present.  Here is re-optimization with new beta scaling.  The
	ideal settings in the new version are:

	eta=0.3
	epsilon=0.1
	margin=1.0

	>> model_sparsify(model5k, x_tr, y_tr, struct('x_te',x_te,'y_te',y_te,'eta',0.5,'epsilon',0.1,'margin',1));
	Scaled epsilon = 1.92221e+07, margin = 1.92221e+08
	Scaled eta = 64858.8 (used to be 37984.1)

	time	iter	nsv	err_tr	err_te	notes(eta/epsilon/margin)
	0	0	17318	275	618	(original model)
	1199	5761	1000	17968	919	(old best 0.5/0.1/1.0)
	197	5728	1000	17639	912	(new best 0.3/0.1/1.0)
	186	5390	1000	17729	912	(0.3/0.15/1.0)
	227	6609	1000	17705	925	(0.25/0.1/1.0)
	173	5021	1000	17578	940	(0.35/0.1/1.0)
	207	6025	1000	17640	966	(0.3/0.05/1.0)
	177	5138	1000	17997	978	(0.3/0.1/0.75)
	276	8043	1000	18161	989	(0.2/0.1/1.0)
	176	5122	1000	17739	994	(0.3/0.2/1.0)
	212	6168	1000	18874	994	(0.3/0.1/1.5)
	132	3833	1000	18622	1029	(0.5/0.1/1.0)

2014-07-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* dogma/model_sparsify.m: The output beta needs to be scaled.
	If we approximate beta2, then use it as beta, new training data
	cannot make a dent.  Norm^2 of beta is nsv if binary problem, 2*nsv
	if multiclass problem.  Other learning algorithms may have
	different scaling.  So we will just copy the norm from the old
	beta at the same number of sv.

==> run/run_altay.m <==
tic(); r3=testparser(model03, dev_w, dev_h, idx); toc();
r3
r3.nerr/r3.ntot
r3.xerr/r3.xtot
x_te = x5k(idx,1:10000);
y_te = y5k(1:10000);
x_tr = x5k(idx,10001:end);
y_tr = y5k(10001:end);
p4 = struct('x_te', x_te, 'y_te', y_te, epsilon, 0.1, margin, 1, eta, 0.5);
tic(); model04 = model_sparsify(model03, x_tr, y_tr, p4); toc();
model04
tic(); [a4,b4]=model_predict_gpu(dev_x(idx,:), model04, 0); toc();
numel(find(a4 ~= dev_y))/numel(dev_y)
tic(); r4=testparser_par(model04, dev_w, dev_h, idx); toc();
r4
r4.nerr/r4.ntot
r4.xerr/r4.xtot

2014-07-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* testparser_par.m: parfor version.
	- schd = findResource('scheduler', 'configuration', 'local')
	gives ClusterSize=12.  Should be higher.
	- new version parpool, old version matlabpool to start pool before
	using parfor.
	- says findResource is deprecated, use parcluster.
	- parfor copies everything.  To speed up need shared memory.
	Found at http://www.mathworks.com/matlabcentral/fileexchange/28572-sharedmatrix
	- labindex and numlabs show the id and number of threads inside an
	spmd block but not a parfor block.  for parfor getCurrentTask()
	works.
	- matlab prior to 2014 allows up to 12 workers which is what these
	results are based on:
	- does not work well on altay/balina as opposed to biyofiz.
	matlab version possible difference.

	>> tic(); r=testparser(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 202.581656 seconds.
	>> tic(); r=testparser_gpu(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 295.032944 seconds.
	>> tic(); r=testparser_par(model_trn01, dev_w(1:100), dev_h(1:100), idx); toc();
	Elapsed time is 102.460237 seconds.

	* testparser_gpu.m: GPU does not give much improvement in
	testparser, which needs to go sequential.  Maybe parfor is better?

	>> tic(); r=testparser_gpu(model_trn01, dev_w(1:30), dev_h(1:30), idx); toc();
	Loading model on GPU.
	Elapsed time is 3.647802 seconds.
	Processing sentences...
	Elapsed time is 52.543305 seconds.
	Elapsed time is 52.546265 seconds.
	>> tic(); r=testparser(model_trn01, dev_w(1:30), dev_h(1:30), idx); toc();
	Elapsed time is 61.559950 seconds.

	* dogma/model_predict_gpu.m: GPU optimized version of
	model_predict.m.  About 6x faster:

	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model_trn, 1); toc();
	tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model_trn, 1); toc();
	Gpu mem: 4.90497e+09, will use max 6.12508e+08 doubles.
	svtr:101517x804 beta:3x101517 margins:3x72551 numel*8:6.57135e+08 free:4.24764e+09
	Processing 28 chunks of ksize:101517x2601 (numel*8:2.11237e+09).
	Elapsed time is 22.509422 seconds.
	>> tic(); [a2,b2]=model_predict(dev_x(idx,:), model_trn, 1); toc();
	tic(); [a2,b2]=model_predict(dev_x(idx,:), model_trn, 1); toc();
	Elapsed time is 149.088427 seconds.

	* startup: To startup a gpu machine for the experiment above:
	dyuret@yunus:~[0]$ ssh biyofiz-4-2
	-bash-3.2$ /mnt/kufs/progs/matlab/R2013a/bin/matlab
	cd parser
	path('dogma',path);
	load conllWSJToken_wikipedia2MUNK-50.mat
	load model_trn.mat
	load model_trn01.mat
	idx = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	load dumpfeatures.mat

2014-07-13  Deniz Yuret  <dyuret@ku.edu.tr>

	* GPU: matlab and gpu: gpuArray, arrayfun.  nvidia-smi for info gives
	Tesla K20m with 4799MB memory.  Web search says 2496 cores, >1 TFlop,
	706MHz clock.  Try some operations with gpuArray.  Maybe model.beta2
	can be a gpuArray.  Do some profiling and speed tests.  In matlab
	gpuDevice and gpuDeviceCount give information about the GPU
	hardware.  Use class and classUnderlying to find out about
	variable types.

	The advantage is not too big for loop operations:
	>> a=rand(1000,100000);
	>> b=rand(1000,1);
	>> tic();for i=1:1000 c=a'*b;end;toc();
	Elapsed time is 19.321784 seconds.
	>> aa=gpuArray(a);
	>> bb=gpuArray(b);
	>> tic();for i=1:1000 cc=aa'*bb;end;toc();
	Elapsed time is 12.098012 seconds.

	But it is huge for single operations:
	>> a=rand(2e4,1e3);
	>> b=rand(1e3,2e4);
	>> tic(); c=a*b; toc();
	Elapsed time is 4.911055 seconds.

	>> aa=gpuArray(a);
	>> bb=gpuArray(b);
	>> tic();cc=aa*bb;toc();
	Elapsed time is 0.015769 seconds.

	GPU memory is 4.8e9 bytes, i.e. 6e8 doubles.  When exceeded we get:
	Out of memory on device. To view more detail about available memory on the GPU,
	use 'gpuDevice()'. If the problem persists, reset the GPU by calling
	'gpuDevice(1)'.

	More realistic example of our kernel calculation:
	>> a=rand(1e5,1e3);
	>> b=rand(1e3,2.5e3);  % the biggest size gpu can handle
	>> tic();c=(a*b+1).^3;toc();
	Elapsed time is 5.796152 seconds.
	>> aa=gpuArray(a);bb=gpuArray(b);
	>> tic();cc=(aa*bb+1).^3;toc();
	Elapsed time is 0.671553 seconds.

	arrayfun and bsxfun serve the function of map with one and two
	elements, respectively.  But memory needs to be sufficient to keep
	intermediate values.

	We should be able to improve model_predict and model_sparsify
	ten-fold.  Maybe some other day... (done) Training will be harder to
	improve unless we can organize it in mini-batches.

	* featselect-5k.log: do feature selection for the 5k dataset with
	poly3 kernel.  avg2 gives the second best score.  Results similar to
	1k.

	nfeat	last	avg		avg2		nsv	time	feats
	2	16.74	12.27(n0,s0)	14.57(n0,s1)	32924	173.68	n0,s0
	3	11.84	8.75(s1)	11.23(n0l1)	24319	183.60	n0,s0,s1
	4	10.64	7.64(n0l1)	7.68(s0r1)	21430	176.57	n0,n0l1,s0,s1
	5	10.49	7.15(s0r)	7.18(s1r1r)	21127	187.71	n0,n0l1,s0,s0r,s1
xx	5	9.08	7.08(s0r1+n1)			19979	251.18	n0,n1,s0,s0r1,s1
	6	9.07	6.70(n1)	7.04(n0l1r)	19017	204.83	n0,n0l1,n1,s0,s0r,s1
xx	6	8.72	6.63(-s0r+s0r1)			18609	270.89	n0,n0l1,n1,s0,s0r1,s1
	7	8.38	6.35(s0r1)	6.45(s0l2+,s0s1)18800	258.10	n0,n0l1,n1,s0,s0r,s0r1,s1
xx	7	8.94	6.34(-s0r+s1r2r)6.38(-s0r+s1r1+)18349	265.81	n0,n0l1,n1,s0,s0r1,s1,s1r2r
	8	8.60	6.20(s0l1)	6.22(s1r1l,s0s1)17879	300.95	n0,n0l1,n1,s0,s0l1,s0r,s0r1,s1
xx	8	8.53	6.22(s0s1)			17775	252.31	n0,n0l1,n1,s0,s0r,s0r1,s0s1,s1
	9	8.05	5.99(n0l2)	6.16(s1r2r)	17736	298.63	n0,n0l1,n0l2,n1,s0,s0r,s0r1,s0s1,s1

	* idx708.mat: idx708=featureindices({'n0','n0l1','n0l2','n1','s0','s0r','s0r1','s0s1','s1'});

	* model_trn.mat:
	>> path('dogma',path);
	>> load dumpfeatures.mat
	>> idx = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	>> hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3)
	>> model_trn = model_init(@compute_kernel, hp);
	>> model_trn = k_perceptron_multi_train(trn_x(idx,:), trn_y, model_trn);
	#1720 SV: 5.90(101473)	AER: 5.90
	Elapsed time is 15841.799743 seconds.
        SV: [804x101517 double]
        pred: [3x1720842 double]
	>> tic(); [a,b]=model_predict(dev_x(idx,:), model_trn, 1); toc();
	Elapsed time is 151.497801 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0378
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model_trn, 0); toc();
	Elapsed time is 22.676895 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0527
	>> tic(); r=testparser(model_trn, dev_w, dev_h, idx); toc();
	Elapsed time is 1408.589679 seconds.
	ntot: 40117
	nerr: 4422  (0.1102)
	xtot: 76834
	xerr: 3263  (0.0425)
	>> tic(); model_trn01 = trainparser(model_trn, trn_w, trn_h, idx); toc();
	Elapsed time is 52817.115166 seconds.
	>> model_trn01
        SV: [804x251480 double]
        pred: [3x3541234 double]
	>> tic(); [a,b]=model_predict(dev_x(idx,:), model_trn01, 1); toc();
	Elapsed time is 393.421166 seconds.
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model_trn01, 1); toc();
	Elapsed time is 56.210434 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0389
	>> tic(); r=testparser_par(model_trn01, dev_w, dev_h, idx); toc();
	Elapsed time is 1610.735364 seconds.
	>> r
	ntot: 40117
	nerr: 4072  (0.1015)
	xtot: 76834
	xerr: 3029  (0.0394)

	* trainparser.m: Next train using dynamic oracle to fine tune.  Use
	trn_w and trn_h (1:5000) to train, dev_w and dev_h to test.  gtrans
	is the error rate on gold transitions (static oracle), ptrans is the
	error rate during parsing (dynamic oracle), phead is the UAS.

	- model00: starting from scratch and training for one epoch with
	dynamic oracle we get parser transition error of 6.78, gold-path
	transition error of 9.21, head error of 17.56,

	>> path('dogma', path);
	>> load conllWSJToken_wikipedia2MUNK-50.mat
	>> hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3)
	>> idx = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	>> model00 = model_init(@compute_kernel,hp);
	>> model00.n_cla = 3;
	>> model00 = trainparser(model00, trn_w(1:5000), trn_h(1:5000), idx)
	#230 SV:12.69(29177)	AER:12.69
	SV: [804x29187 double]
	pred: [3x230120 double]
	>> [dev_x,dev_y]=dumpfeatures(dev_w,dev_h);
	Elapsed time is 114.633561 seconds.
	dev_x           1768x72551             1026161344  double
	dev_y              1x72551                 580408  double
	>> [a,b]=model_predict(dev_x(idx,:), model00, 1);
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0921
	>> tic(); r=testparser(model00, dev_w, dev_h, idx); toc();
	Elapsed time is 2098.151274 seconds.
	ntot: 40117 (0.1756)
	nerr: 7044
	xtot: 76834
	xerr: 5210  (0.0678)

	- model01: starting from static x5k model and train one epoch with
	dynamic oracle.  The static model has gold transition error of 5.43,
	parser transition error of 5.78, and head error of 15.14.  After one
	epoch of dynamic oracle training we have gold: 5.56, ptrans:5.36,
	head:14.13.

	>> load dump5000-model.mat
	model         1x1                 120667128  struct
	SV: [804x17318 double]
        pred: [3x207603 double]
	>> model.step = 1000;
	>> tic(); [a,b]=model_predict(dev_x(idx,:), model, 1); toc();
	Elapsed time is 58.129140 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0543
	>> tic(); r = testparser(model, dev_w, dev_h, idx); toc();
	Elapsed time is 336.569856 seconds.
	>> tic(); r2 = testparser_par(model, dev_w, dev_h, idx); toc();
	Elapsed time is 103.098387 seconds.
	ntot=40117 % number of words
	nerr=6072 (0.1514) % number of wrong heads
	xtot=76834 % number of parser transitions
	xerr=4443 (0.0578) % number of suboptimal transitions
	% Note the original model was not trained on 1:5000, it used the
	% first 10k transitions as test
	>> tic(); model01 = trainparser(model, trn_w(1:5000), trn_h(1:5000), idx); toc();
	Elapsed time is 6489.773220 seconds.
        SV: [804x40502 double]
        pred: [3x437723 double]
	>> tic(); [a,b]=model_predict(dev_x(idx,:), model01, 1); toc();
	Elapsed time is 140.067637 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0556
	>> tic(); r=testparser(model01, dev_w, dev_h, idx); toc();
	Elapsed time is 2865.993597 seconds.
	ntot: 40117
	nerr: 5667  (0.1413)
	xtot: 76834
	xerr: 4121  (0.0536)

	- model02: try multiple epochs.  doing the second round of dynamic
	oracle over model01.

	>> model02 = trainparser(model01, trn_w(1:5000), trn_h(1:5000), idx);
	#667 SV: 8.83(58916)	AER: 8.83
	Elapsed time is 10679.100909 seconds.
	>> model02
        SV: [804x58979 double]
	pred: [3x667843 double]
	>> [a,b]=model_predict(dev_x(idx,:), model02, 1);
	Elapsed time is 211.158363 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0574
	>> tic(); r=testparser(model02, dev_w, dev_h, idx); toc();
	Elapsed time is 4107.217578 seconds.
	>> r
	ntot: 40117
	nerr: 5533  (0.1379)
	xtot: 76834
	xerr: 4081  (0.0531)

	- model03: starting from sparsified x5k model m6.

	>> load m6
	SV: [804x12555 double]
        pred: [3x207603 double]
	>> tic(); [a,b]=model_predict(dev_x(idx,:), m6, 0); toc();
	Elapsed time is 80.493971 seconds.
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), m6, 1); toc();
	Elapsed time is 10.364309 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0534
	>> tic(); r = testparser_par(m6, dev_w, dev_h, idx); toc();
	Elapsed time is 75.726568 seconds.
	>> r
	ntot: 40117
	nerr: 6035  (0.1504)
	xtot: 76834
	xerr: 4393  (0.0572)
	>> tic(); model03 = trainparser(m6, trn_w(1:5000), trn_h(1:5000), idx); toc();
	Elapsed time is 5483.439487 seconds.
	>> model03
        SV: [804x36073 double]
        pred: [3x274392 double]
	>> tic(); [a,b]=model_predict(dev_x(idx,:), model03, 1); toc();
	Elapsed time is 123.986750 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0716
	>> tic(); r3=testparser_par(model03, dev_w, dev_h, idx); toc();
	Elapsed time is 2573.779485 seconds.
	>> r3
	ntot: 40117
	nerr: 6319  (0.1575)
	xtot: 76834
	xerr: 4638  (0.0604)

	% maybe getting rid of beta2 was not a good idea.  let us try with
	% appropriately weighted beta2 this time.

	>> n1=norm(model.beta2(:,1:nsv),'fro')
	>> n2=norm(m6.beta2, 'fro')
	>> m6.beta2 = m6.beta2 * (n1/n2);
	>> tic(); model03b = trainparser(m6, trn_w(1:5000), trn_h(1:5000), idx); toc();
	Elapsed time is 5467.717893 seconds.
	>> model03b
        SV: [804x36073 double]
        pred: [3x274392 double]
	% same errors, same sv, but different beta2
	>> a0=model_predict_gpu(dev_x(idx,:), model03, 0); numel(find(a0 ~= dev_y))/numel(dev_y)
	0.1192
	>> b0=model_predict_gpu(dev_x(idx,:), model03b, 0); numel(find(b0 ~= dev_y))/numel(dev_y)
	0.1192
	>> a1=model_predict_gpu(dev_x(idx,:), model03, 1); numel(find(a1 ~= dev_y))/numel(dev_y)
	0.0716
	>> b1=model_predict_gpu(dev_x(idx,:), model03b, 1); numel(find(b1 ~= dev_y))/numel(dev_y)
	0.0547
	>> tic(); r3b=testparser(model03b, dev_w, dev_h, idx); toc();
	Elapsed time is 2574.903771 seconds.
	>> r3b
	ntot: 40117
	nerr: 5689  (0.1418)
	xtot: 76834
	xerr: 4192  (0.0546)

	>> model04 = model_sparsify(model03b, x_tr, y_tr, struct('x_te',x_te,'y_te',y_te,'eta',0.3,'epsilon',0.1,'margin',1.0));
	Elapsed 2157 secs.
	>> model04
        SV: [804x13043 double]
        pred: [3x274392 double]
	pred_te: [3x10000 double]
	>> tic(); [a,b]=model_predict_gpu(dev_x(idx,:), model04, 1); toc();
	Elapsed time is 16.827549 seconds.
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0535
	>> tic(); r=testparser_par(model04, dev_w, dev_h, idx); toc();
	Elapsed time is 379.683786 seconds.
	>> r
	ntot: 40117
	nerr: 5803  (0.1447)
	xtot: 76834
	xerr: 4296  (0.0559)

	* model_dbg: test to see if trainparser gives the same result when oracle moves
	are picked.  However be careful because it is using the whole x5k as training,
	older model used the first 10k instances for testing.  Another difference is
	whether or not to call the learner when there is only one legal move.  I started
	not to in dumpfeatures, but now I think maybe that was a bad idea.  So we need to
	add options to dumpfeatures for this test.

	Result: trainparse is faster to train than dumpfeatures+k_perceptron
	and results in an identical model.

	>> path('dogma',path);
	>> hp = struct('type', 'poly', 'gamma', 1, 'coef0', 1, 'degree', 3)
	>> idx = featureindices({'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'});
	>> load conllWSJToken_wikipedia2MUNK-50.mat
	>> [x5k1,y5k1] = dumpfeatures(trn_w(1:5000), trn_h(1:5000), 1);
	Elapsed time is 348.282005 seconds.
	x5k1           1768x230120            3254817280  double
	y5k1              1x230120               1840960  double
	>> model_ref = model_init(@compute_kernel,hp);
	>> tic(); model_ref = k_perceptron_multi_train(x5k1(idx,:), y5k1, model_ref); toc();
	#230 SV: 7.98(18348)	AER: 7.98
	Elapsed time is 2262.626120 seconds.
	>> model_dbg = model_init(@compute_kernel,hp);
	>> model_dbg.n_cla = 3;
	>> tic(); model_dbg = trainparser_dbg(model_dbg, trn_w(1:5000), trn_h(1:5000), idx); toc();
	Elapsed time is 2509.524566 seconds.
	>> all(model_ref.S == model_dbg.S)
	1

	* dumpfeatures.m: added option to dump singleton moves (moves without
	alternative) as well.

	* testparser.m (testparser): modified to return single struct
	array.

	* dumpfeatures.mat: convert all data to features once and for all. (on yunus)
	[trn_x,trn_y]=dumpfeatures(trn_w,trn_h);
	Elapsed time is 2922.888628 seconds.
	[dev_x,dev_y]=dumpfeatures(dev_w,dev_h);
	Elapsed time is 123.879583 seconds.
	[tst_x,tst_y]=dumpfeatures(tst_w,tst_h);
	Elapsed time is 571.966855 seconds.
	save dumpfeatures.mat -v7.3

	Name          Size                       Bytes  Class     Attributes
	dev_h         1x1700                    511336  cell
	dev_w         1x1700                  32284000  cell
	dev_x      1768x72551               1026161344  double
	dev_y         1x72551                   580408  double
	trn_h         1x39832                 12061408  cell
	trn_w         1x39832                764483584  cell
	trn_x      1768x1720842            24339589248  double
	trn_y         1x1720842               13766736  double
	tst_h         1x7676                   2328680  cell
	tst_w         1x7676                 147756512  cell
	tst_x      1768x332821              4707420224  double
	tst_y         1x332821                 2662568  double

	* featselect.m: take an optional starting cell array of features
	argument.

	* featureindices.m: modified to output actual index vector in
	addition to an index hash and the total number of features.

	* polydegree.m: try 8 words (n0,s0,s1,n1,n0l1,s0r1,s0l1,s1r1) and
	all their aux features (e.g. n0l,n0r,n0+,n0-) on x5k with all
	kernels.  Compare to earlier results we got with x5k.  Order 6 gives
	best last-error, 8/9 give best avg-error but none are as good as
	order 3 with its optimized smaller 9 feature set.  6.18 is still the
	best we get with x5k.

	degree	last	avg	nsv	time
	1	18.34	13.34	36732	3890.22
	2	14.56	8.21	24299	2698.12
	3	12.73	7.10	21440	2454.09
	4	11.90	6.90	20083	2310.41
	5	9.00	6.75	19167	2223.89
	6	7.97	6.83	18649	2169.10
	7	8.67	6.89	18372	2152.53
	8	8.44	6.73	18225	2127.00
	9	8.13	6.73	18100	2115.84

	>> dump5000
	Using poly3 kernel on n0,s0,s1,n1,n0l1,s0r1,s0l1,s1r1,s0r
	train	last	avg	nsv	time
	207603	7.81	6.18	17318	1939.12


==> run/run_polydegree.m <==
% load conllWSJToken_wikipedia2MUNK-50.mat;
% Elapsed time is 9.625437 seconds.
% [x5k,y5k]=dumpfeatures(trn_w(1:5000),trn_h(1:5000));
% Elapsed time is 341.514392 seconds.

[fi,i] = featureindices();
assert(i == size(x5k,1));
feats = {'n0','n0l','n0r','n0+','n0-', ...
         's0','s0l','s0r','s0+','s0-', ...
         's1','s1l','s1r','s1+','s1-', ...
         'n1','n1l','n1r','n1+','n1-', ...
         'n0l1','n0l1l','n0l1r','n0l1+','n0l1-', ...
         's0r1','s0r1l','s0r1r','s0r1+','s0r1-', ...
         's0l1','s0l1l','s0l1r','s0l1+','s0l1-', ...
         's1r1','s1r1l','s1r1r','s1r1+','s1r1-', ...
         'n0s0','s0s1'};
idx = [];
for i=1:length(feats) idx = [idx, fi(feats{i})]; end

x_te = x5k(idx,1:10000);
y_te = y5k(1:10000);
x_tr = x5k(idx,10001:end);
y_tr = y5k(10001:end);

hp.type = 'poly';
hp.gamma = 1;
hp.coef0 = 1;

fprintf('degree\tlast\tavg\tnsv\ttime\n');
for d=1:9
  hp.degree = d;
  model = model_init(@compute_kernel, hp);
  model.step = 1000000;
  tic();
  model = k_perceptron_multi_train(x_tr, y_tr, model);
  telapsed = toc();
  nsv = size(model.beta, 2);
  last = numel(find(y_te ~= model_predict(x_te, model, 0)))/numel(y_te)*100;
  avg = numel(find(y_te ~= model_predict(x_te, model, 1)))/numel(y_te)*100;
  fprintf('%d\t%.2f\t%.2f\t%d\t%.2f\n', hp.degree, last, avg, nsv, telapsed);
end


2014-07-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* testparser.m: shows that 5.4% static error rate corresponds to 15%
	parse error rate.
	- This model is from 5K words, more should decrease the static rate.
	- This model did not train with dynamic oracle, that should decrease
	the difference.
	- This model was only trained for one epoch.

	>> path('dogma',path);
	>> load conllWSJToken_wikipedia2MUNK-50.mat
	dev_h         1x1700                 511336  cell
	dev_w         1x1700               32284000  cell
	>> load dump5000-model.mat
	model         1x1                 120667128  struct
	>> feats = {'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'};
	>> idx = [];
	>> for i=1:length(feats) idx = [idx, fi(feats{i})]; end
	idx           1x804                    6432  double
	>> [dev_x,dev_y]=dumpfeatures(dev_w, dev_h);
	Elapsed time is 143.275703 seconds.
	dev_x      1768x72551            1026161344  double
	dev_y         1x72551                580408  double
	>> [a,b]=model_predict(dev_x(idx,:), model, 1);
	>> numel(find(a ~= dev_y))/numel(dev_y)
	0.0543
	>> [nerr,ntot,xerr,xtot] = testparser(model, dev_w, dev_h, idx);
	Elapsed time is 336.569856 seconds.
	ntot=40117 % number of words
	nerr=6072 (0.1514) % number of wrong heads
	xtot=76834 % number of parser transitions
	xerr=4443 (0.0578) % number of suboptimal transitions

	* cluster: figure out how to use the cluster.
	man sge_intro for a summary.
	qhost to see status of hosts.
	qstat to see status of jobs.
	qsub to submit a job.
	/mnt/kufs/progs/matlab/R2013a/bin/matlab is the binary.
	/mnt/kufs/scratch/dyuret is the scratch space.
	ssh biyofiz-4-3 if available.

	* multithreading: figure out how to set number of processors.

	MATLAB supports three kinds of parallelism: multithreaded, distributed
	computing, and explicit parallelism.

	Note:   maxNumCompThreads will be removed in a future version. You
	can set the -singleCompThread option when starting MATLAB to limit
	MATLAB to a single computational thread. By default, MATLAB makes
	use of the multithreading capabilities of the computer on which it
	is running.

	https://newton.utk.edu/bin/view/Main/MatlabParallelization

	taskset -pc pid: retrieve a processess CPU affinity

	* why: are ural and balina so much slower than altay?  Because we
	haven't setup the step parameter, so they don't print out anything!

	* sparsify5000d.log: running the optimal params to conclusion in x5k.

	>> m6 = model_sparsify(model, x_tr, y_tr, p);
	Using averaged solution beta2.
	epsilon=0.1 margin=1 eta=0.5
	train: 207603, test: 10000, classes: 3, nsv: 17318
	Computing initial scores...
	Elapsed time is 150.042002 seconds.
	Scaled epsilon = 1.92221e+07, margin = 1.92221e+08
	Scaled eta = 37984.1
	time	iter	nsv	err_tr	err_te	maxdiff
	0	0	17318	7268	618	(original model)
	1200	5761	1000	17968	919	2.160987e+08
	2317	11131	2000	13775	787	1.797998e+08
	3399	16322	3000	11897	712	1.529027e+08
	4419	21225	4000	10571	675	1.201366e+08
	5323	25561	5000	9688	657	1.051552e+08
	5743	27577	5500	9361	649	8.962474e+07
	6378	30627	6300	8808	636	7.853851e+07
	6942	33341	7100	8433	628	6.410252e+07
	7748	37203	8500	7831	614	5.001900e+07
	8254	39639	9600	7513	607	3.902603e+07
	9119	43789	12200	7012	601	2.695608e+07
	9219	44272	12555	6963	610	1.911820e+07

	* model-sparsify-parameters: So what are the ideal parameters for
	sparsify.  Some samples so far (all results for x5k, y5k, at
	nsv=1000): epsilon/margin scale: 1.92221e+08 (mean positive margin)
	eta scale: 75968.2 (mean absolute beta)

	However note that the eta scaling changed in the updated version
	of 2014-07-16.

	eta	epsilon	margin	err_tr	err_te	time	iter	maxdiff
	(original nsv=17318)	7268	618	-	-	-
	0.5	0.1	1.0	17968	919	1199	5761	2.160987e+08
	0.5	0.05	1.0	17756	923	1287	6180	2.217087e+08
	0.25	0.1	1.0	17654	941	2241	10783	2.158356e+08
	0.75	0.1	1.0	18008	960	869	4176	2.350121e+08
	0.5	0.25	1.0	17634	968	1019	4887	2.084023e+08
	0.5	0.5	1.0	17815	977	729	3498	2.06433e+08
	0.5	0.1	1.25	18402	983	1269	6094	2.333505e+08
	0.5	0.1	0.75	18815	986	1090	5239	2.052092e+08
	0.5	0.5	0.75	18342	1000	532	2550	1.650722e+08
	0.5	0.5	1.5	19641	1030	908	4360	2.389662e+08
	0.5	0.5	2.0	20894	1081	959	4605	2.456402e+08
	0.5	0.25	2.0	21075	1102	1171	5626	2.489516e+08
	0.5	0.25	0.5	23241	-	-	3120	1.35809e+08
	0.5	0.1	0.2	38032	-	-	2605	7.10636e+07
	0.5	0.05	0.1	54547	2638	491	2413	5.24318e+07

	* dogma/model_sparsify.m: done: epsilon and eta should be input
	parameters not model fields (should not touch the original model).
	Is it a good idea to cap the margins at 2*epsilon?  Those two
	should be independent (done).  The new signature is:
	function m = model_sparsify(model,x_tr,y_tr,p)
	with optional parameters in p: margin, epsilon, eta, average,
	x_te, y_te.  done: compare results with older log files.

	* sparsify5000c.log:
	x_tr             804x207603            1335302496  double
	y_tr               1x207603               1660824  double
	p =
	x_te: [804x10000 double]
	y_te: [1x10000 double]
	epsilon: 0.2500
	margin: 2
	>> m5 = model_sparsify(model, x_tr, y_tr, p);
	Using averaged solution beta2.
	epsilon=0.25 margin=2 eta=0.5
	train: 207603, test: 10000, classes: 3, nsv: 17318
	Computing initial scores...
	Elapsed time is 150.562805 seconds.
	Scaled epsilon = 4.80553e+07, margin = 3.84442e+08
	Scaled eta = 37984.1
	time	iter	nsv	err_tr	err_te	maxdiff
	0	0	17318	7268	618	0
	128	614	100	49243	2334	4.266032e+08
	3740	17950	4000	10988	696	1.170816e+08
	3995	19173	4400	10607	679	1.097213e+08
	4176	20039	4700	10372	666	1.019421e+08
	4294	20605	4900	10122	656	9.877697e+07
	4348	20865	5000	10126	649	9.834636e+07
	5018	24079	6500	9310	639	6.960811e+07
	5616	26946	8589	8685	632	4.795281e+07

	* dogma/model_sparsify.m: should also take a test set so we can
	see how out of sample performance evolves.
	Done (sparsify5000b.log), but epsilon and the margin cap are tied
	which should be fixed.

	time	iter	nsv	err_tr	err_te	max(h-c)
	0	0	17318	7268	618	0
	3717	17878	7300	7325	631	3.15407e+07
	3760	18077	7400	7193	624	2.9221e+07
	4203	20135	8300	6696	621	2.62548e+07
	4239	20299	8400	6630	615	2.69433e+07
	6213	29002	13377	5899	599	9.47589e+06

==> run/run_perceptron.m <==
% path('dogma',path);
function [last,avg,nsv,time,model] = run_perceptron(x_tr, y_tr, x_te, y_te, kernel)
model = model_init(@compute_kernel, kernel);
model.step = 1000000;
tic();
model = k_perceptron_multi_train(x_tr,y_tr,model);
pred_perceptron_last = model_predict(x_te,model,0);
pred_perceptron_av = model_predict(x_te,model,1);
time = toc();
last = numel(find(pred_perceptron_last~= y_te))/numel(y_te)*100;
avg = numel(find(pred_perceptron_av~=y_te))/numel(y_te)*100;
nsv = size(model.beta, 2);
end


==> run/run_sparsify5000.m <==
% load conllWSJToken_wikipedia2MUNK-50.mat;
% Elapsed time is 9.625437 seconds.
% [x5k,y5k]=dumpfeatures(trn_w(1:5000),trn_h(1:5000));
% Elapsed time is 341.514392 seconds.
% load dump5000-model.mat; % loads model
% Elapsed time is 0.704188 seconds.

[fi,i] = featureindices();
assert(i == size(x5k,1));
feats = {'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'};
idx = [];
for i=1:length(feats)
  idx = [idx, fi(feats{i})];
end % for

x_te = x5k(idx,1:10000);
y_te = y5k(1:10000);
x_tr = x5k(idx,10001:end);
y_tr = y5k(10001:end);
return
model.step = 100;
m = model_sparsify(x_tr, y_tr, model, 1);

p0 = model_predict(x_te, model, 1);
numel(find(p0 ~= y_te))
p = model_predict(x_te, m, 0);
numel(find(p ~= y_te))


2014-07-11  Deniz Yuret  <dyuret@ku.edu.tr>

	* sparsify5000.m: Saved as sparsify5000.log and
	sparsify5000-models.mat.

	epsilon	iter	nsv	train	test	secs	notes
	0.5	12666	5821	10889	683	2708	m
	0.25	23329	10413	8286	604	4848	m2
	0.10	30826	13534	6387	590	6360	m3
	--	--	17318	7268	618	1939	model (original)

	* dogma/model_sparsify.m: First results.  We scale eta with
	mean(abs(beta)) and epsilon with mean(margin(margin>0)).  To see
	the effect of eta on performance and convergence, we take a
	snapshot at 300 sv on the dump1000 model.  eta=0.5 does seem like
	a good time performance trade-off.

	Initial nsv=3999 error=1453/34217
	eta	iter	nsv	err	max(h-c)
	2.0	393	300	6007	2.81e+07
	1.0	535	300	5058	2.58e+07
	0.5	858	300	4835	2.51e+07
	0.25	1461	300	4773	2.42e+07
	0.10	3158	300	4680	2.32e+07

	Running to conclusion with eta=0.5 we get
	iter	nsv	err	max(h-c)
	4754	2446	2089	1.07428e+07

	Evaluating this model on the test set (10K) we get 882 errors vs
	838 with the original model:

	[x5k,y5k]=dumpfeatures(trn_w(1:5000),trn_h(1:5000));
	x_te = x5k(idx,1:10000);
	y_te = y5k(1:10000);
	x_tr = x5k(idx,10001:end);
	y_tr = y5k(10001:end);
	>> m = model_sparsify(x_tr, y_tr, model1bak, 1);
	   m.SV: [804x2446 double]
	   model1bak.SV: [804x3999 double]
	>> p = model_predict(x_te, m, 0);
	>> numel(find(p ~= y_te))
	ans = 882
	>> p0 = model_predict(x_te, model1bak, 1);
	>> numel(find(p0 ~= y_te))
	ans = 838

	* sparsify5000.m: Time to try sparsifying big model.  The original
	model took 1740 secs to train on 207603 instances with err=7268,
	nsv=17318, test_error=618/10K.

	Instances: 207603, classes: 3, nsv: 17318
	Initial nsv=17318 error=7268/207603 test_error=618/10K


2014-07-10  Deniz Yuret  <dyuret@ku.edu.tr>

	* dogma/model_sparsify.m: Implementing cotter13.
	- Need to figure out scaling.  Both beta and x magnitudes effect
	scale.  eta determines the size of beta updates and epsilon
	determines when we quit by looking at margin differences.  Problem
	is averaged perceptron does not give us a well defined margin.
	Maybe we can get an idea of where the margin is by looking at the
	SV activations.  Or simply assume take max margin as 1.  epsilon
	would then be 1/2 max margin.

	f = w.(x) =  bi (xi).(x)  where bi = ai.yi

	The paper suggests stopping when f > 1/2, but assumes K(x,x)<=1
	and svm margin = 1.  We will scale eta using mean_beta and epsilon
	using mean_margin.

	* test_models.mat: Saved a few test models to test model_sparsify:
	model is multiclass, model2 is binary, both trained on x_tr and
	y_tr (multi), y_tr2 (binary).

	* perceptron.m: Things to try:
	+ Dump gold sequences and first get a good model for classification.
	+ Kernel: poly degree, gauss, etc.
	+ Features: which words, distances, children etc.
	+ Effect of kernel on features.
	+ Effect of data size performance.
	+ Effect of data size on features.
	+ Next reduce SV's using cotter13.

	* featselect-poly3-n2000.out: Effect of data size on feature
	selection.  Going from 1000 to 2000 training sentences.  Resulting
	features are identical to previous experiment but featselect stops
	at 5 features, previously it went to 10.  We need more experiments
	to understand what is going on.

	n	feat	last	avg	nsv	time
	2	s0	17.81	13.16	13202	60.08	n0,s0
	3	s1	12.40	9.42	10348	67.12	n0,s0,s1
	4	n0l1	11.56	8.52	9304	97.72	n0,n0l1,s0,s1
	5	s0r1r	10.73	7.82	8994	93.17	n0,n0l1,s0,s0r1r,s1

2014-07-09  Deniz Yuret  <dyuret@ku.edu.tr>

	* dump5000.m: Effect of data size on performance.

	>> load conllWSJToken_wikipedia2MUNK-50.mat
	>> [x,y]=dumpfeatures(trn_w(1:5000),trn_h(1:5000));
	Elapsed time is 343.961048 seconds.
	>> dump5000
	Using poly3 kernel on n0,s0,s1,n1,n0l1,s0r1,s0l1,s1r1,s0r
	train	last	avg	nsv	time
	10000	12.75	11.24	1469	11.33
	20000	11.10	9.54	2599	21.30
	30000	9.84	8.65	3564	45.20
	40000	9.80	8.18	4511	83.13
	50000	9.98	7.75	5376	145.05
	60000	9.31	7.58	6237	207.82
	70000	10.17	7.48	7080	275.49
	80000	9.40	7.22	8032	392.28
	90000	9.01	7.00	8859	389.71
	100000	9.43	6.92	9618	479.01
	110000	8.72	6.89	10304	579.18
	120000	8.72	6.77	10995	688.76
	130000	8.80	6.78	11696	795.67
	140000	8.66	6.75	12435	923.55
	150000	8.48	6.66	13120	1046.87
	160000	7.85	6.54	13914	1190.26
	170000	8.29	6.45	14611	1333.57
	180000	8.14	6.30	15322	1481.91
	190000	7.81	6.25	16100	1643.82
	200000	8.40	6.23	16832	1800.59
	207603	7.81	6.18	17318	1939.12

==> run/dump1000b.m <==
% path('dogma', path);
%

tic();
fprintf('Loading dump1000b dataset\n');
load dump1000b.mat;                     % load x,y
toc();

% reduce features to 8 basic words
tic();
fprintf('Splitting features\n');
[fi,i] = featureindices();
assert(i == size(x,1));

feats = {'n0','s0','s1','n1','n0l1','s0r1','s0l1','s1r1','s0r'};
fprintf('Using poly kernel on %s\n', strjoin(feats,','));
idx = [];
for i=1:length(feats)
  idx = [idx, fi(feats{i})];
end % for
x = x(idx, :);
x_te = x(:,1:10000);
y_te = y(1:10000);
x_tr = x(:,10001:end);
y_tr = y(10001:end);
%x_tr = x(:,10001:20000);
%y_tr = y(10001:20000);
toc();

fprintf('degree\tlast\tavg\tnsv\ttime\n');
for degree=1:9
hp.type = 'poly';
hp.gamma = 1;
hp.coef0 = 1;
hp.degree = degree;

% fprintf('Using %dth degree poly kernel\n', hp.degree);

% for i=[.07,.09]
% hp.type = 'rbf';
% hp.gamma = i;
% fprintf('Using rbf kernel with gamma=%f\n', hp.gamma);

model_bak = model_init(@compute_kernel,hp);
model_bak.step = 1000000;

tic();
%% PERC
% train Perceptron
% fprintf('Training Perceptron model...\n');
model_perceptron = k_perceptron_multi_train(x_tr,y_tr,model_bak);
% fprintf('Done!\n');
% fprintf('Number of support vectors last solution:%d\n',size(model_perceptron.beta, 2));
% fprintf('Number of support vectors averaged solution:%d\n',size(model_perceptron.beta2, 2));
% fprintf('Testing last solution...');
pred_perceptron_last = model_predict(x_te,model_perceptron,0);
% fprintf('Done!\n');
% fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_perceptron_last~=y_te))/numel(y_te)*100);
% fprintf('Testing averaged solution...');
pred_perceptron_av = model_predict(x_te,model_perceptron,1);
% fprintf('Done!\n');
% fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_perceptron_av~=y_te))/numel(y_te)*100);

telapsed = toc();
err_last = numel(find(pred_perceptron_last~= y_te))/numel(y_te)*100;
err_av = numel(find(pred_perceptron_av~=y_te))/numel(y_te)*100;
nsv = size(model_perceptron.beta, 2);
fprintf('%d\t%.2f\t%.2f\t%d\t%.2f\n', ...
        degree, err_last, err_av, nsv, telapsed);

end
return

%% PA_I
tic();
fprintf('Training PA-I model...\n');
model_pa1 = k_pa_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_pa1.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_pa1.beta2, 2));
fprintf('Testing last solution...');
pred_pa1_last = model_predict(x_te,model_pa1,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_pa1_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_pa1_av = model_predict(x_te,model_pa1,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_pa1_av~=y_te))/numel(y_te)*100);
toc();

%% PA_II
tic();
fprintf('Training PA-II model...\n');
model_pa2 = model_bak;
model_pa2.update = 2;
model_pa2 = k_pa_multi_train(x_tr,y_tr,model_pa2);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_pa2.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_pa2.beta2, 2));
fprintf('Testing last solution...');
pred_pa2_last = model_predict(x_te,model_pa2,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_pa2_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_pa2_av = model_predict(x_te,model_pa2,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_pa2_av~=y_te))/numel(y_te)*100);
toc();


%% PROJ++ eta=0.1
tic();
% train Projectron++
fprintf('Training Projectron++ model...\n');
model_projectron2 = k_projectron2_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_projectron2.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_projectron2.beta2, 2));
fprintf('Testing last solution...');
pred_projectron2_last = model_predict(x_te,model_projectron2,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_projectron2_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_projectron2_av = model_predict(x_te,model_projectron2,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_projectron2_av~=y_te))/numel(y_te)*100);
toc();

%% RBP
tic();
% set maximum number of support vectors for RBP
model_bak.maxSV = size(model_projectron2.beta,2);
% train RBP
fprintf('Training RBP...\n');
model_rbp = k_perceptron_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_rbp.beta, 2));
fprintf('Testing last solution...');
pred_rbp_last = model_predict(x_te,model_rbp,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_rbp_last~=y_te))/numel(y_te)*100);
toc();

% tic();
% fprintf('Plotting error and SV curves...\n');
% % plot error curves
% figure(1)
% plot(model_pa1.aer(100:end),'k')
% hold on
% plot(model_perceptron.aer(100:end),'c')
% plot(model_projectron.aer(100:end),'b')
% plot(model_projectron2.aer(100:end),'m')
% plot(model_rbp.aer(100:end),'r')
% plot(model_forgetron.aer(100:end),'y')
% plot(model_oisvm.aer(100:end),'g')
% grid
% legend('PA-I','Perceptron','Projectron','Projectron++','RBP','Forgetron','OISVM')
% xlabel('Number of Samples')
% ylabel('Average Online Error')

% % plot support vector curves
% figure(2)
% plot(model_pa1.numSV,'k')
% hold on
% plot(model_perceptron.numSV,'c')
% plot(model_projectron.numSV,'b')
% plot(model_projectron2.numSV,'m')
% plot(model_rbp.numSV,'r')
% plot(model_forgetron.numSV,'y')
% plot(model_oisvm.numSV,'g')
% legend('PA-I','Perceptron','Projectron','Projectron++','RBP','Forgetron','OISVM','Location','NorthWest')
% grid
% xlabel('Number of Samples')
% ylabel('Number of Support Vectors')
% toc();

	* dump1000b.m: Rerun the poly kernel experiment with the basic
	poly3 feature set:  Best result 7.65->8.38 probably due to removal
	of easy (single choice) instances.  Poly6 no longer has an
	advantage.  Seems each poly degree requires own feature
	optimization.  But the optimal feature set could also change based
	on data size.  It will change yet again with dynamic oracle.

	Loading dump1000b dataset
	Elapsed time is 3.208295 seconds.
	Splitting features
	Using poly kernel on n0,s0,s1,n1,n0l1,s0r1,s0l1,s1r1,s0r
	Elapsed time is 0.411737 seconds.
	degree	last	avg	nsv	time
	1	21.33	14.63	6905	83.23
	2	11.65	8.93	4470	26.64
	3	9.63	8.38	3999	29.65
	4	10.37	8.58	3948	24.89
	5	10.01	9.02	3949	26.10
	6	10.48	9.30	4084	28.22
	7	10.62	9.72	4182	30.28
	8	10.73	9.94	4286	33.43
	9	11.29	10.37	4436	35.86

	* note: rbf kernel does not do as good with poly3 optimized
	features, although we have not optimized gamma=0.1.
	last	avg	nsv	time	feats
	11.32	9.75	4836	457.16	n0,n0l1,n1,s0,s0l1,s0r,s0r1,s1,s1r1

	* featselect-poly3b.out: Is this set still optimal for
	poly3 (done)?  A small improvement is possible by adding
	s1r2+ (8.16) but locally that's it.  Starting featselect from
	scratch to make sure: we have four new features after #4, the
	final result improves from 8.38 to 8.14.

	n	feat	last	avg	nsv	time
	1	n0	48.90	48.37	15094	42.20
	2	s0	18.66	14.47	6231	20.88
	3	s1	12.97	10.75	5205	20.02
	4	n0l1	12.75	10.01	4755	20.32
	5	s0r1r	12.25	9.17	4595	18.69
	6	s1-	11.79	9.15	4391	17.30
	7	n1	11.21	9.02	4023	19.06	# +s0s1-s1- gives 8.60 here
	8	s0s1	10.52	8.67	4064	18.23	# +s0r-s1- gives 8.55 here
	9	s0r	10.91	8.43	4064	18.44
	10	s1r1r	10.07	8.14	3995	19.23

	* dumpfeatures.m (1.2): Checked in the version for experiments
	below as version 1.1.  Version 1.2 does not output transitions for
	which there is a single valid move.

	>> load conllWSJToken_wikipedia2MUNK-50.mat
	>> [x,y]=dumpfeatures(trn_w(1:1000),trn_h(1:1000));
	Elapsed time is 68.892985 seconds.
	x          1768x44217            625405248  double
	y             1x44217               353736  double
	>> save dump1000b.mat x y;

	* ArcHybrid.m (1.2): Checked in the version for the experiments
	below as 1.1.  Version 1.2 adds an exists bit in addition to a
	not-exists bit.

	* featselect-poly3.out: Experiment to see if kernel makes a
	difference in feature selection.  Here is the order of features
	with a poly3 kernel.  There is definitely effect in both
	directions.  Poly 3 improves much faster with fewer features
	compared to poly 6 tops at 9 features (7.65).  Poly3 uses almost
	exclusively word features.

	length	feat1	score1	feat2	score2
	1	n0	47.87
	2	s0	13.89
	3	s1	10.62	s1r	11.39
	4	n0l1	9.54	s0r	9.66
	5	s0r	9.01	s0r1	9.12
	6	s1r1	8.59	s0r2	8.60
	7	n1	8.35	s1r2r	8.62	+n1+s0r1-s1r1	8.49	+n1+s0r1-s0r	8.51	+n1+s0r1-n0l1	8.56
	8	s0r1	7.86	s1r2l	8.08
	9	s0l1	7.65	s0s1	7.67

	* featselect-poly6.out: Here is the order of features selected and the
	closest alternatives: (compare to 9.57 for all 66 features).
	Feature selection seems to be worth 2% points.
	$ perl -lane 'print if scalar(split(/,/,$F[4]))==8' featselect4.out | sort -g -k2,2 | head

	length	feat1	score1	feat2	score2
	1	n0	47.90	n2x	38.54
	2	s0	13.68	?
	3	s1	11.22	s1r	11.67
	4	s0r1	9.95	s0r1r	10.02
	5	n0l1x	9.30	s1r2x	9.39
	6	n0r	9.24	s1r	9.33
	7	s0r1x	9.08	s1r1x	9.08
	8	s1x	8.88	n0l1	8.89	+s1x+n0l1-n0l1x	8.82
	9	n0l1	8.73	s1r1x	8.75	+n0l1+n1-n0l1x	8.28	+n0l1+n1-s1x	8.56
	10	n1	8.26	s1r2x	8.50	+n1+s1r1r-n0l1x	8.19	+n1+s1r1r-n0l1	8.36	+n1+s1r1r-n0r	8.43	+n1+s1r1r-s1x	8.44
	11	s1r1r	7.86	s1r	8.02
	12	s0l1x	7.73	n0l2r	7.77	+s0l1x+s0r-s0r1x	7.77
	13	s0r	7.68	s1r1	7.72	+s0r+s0x-s0l1x	7.64
	14	s0x	7.58	s0s1	7.59	s1r	7.62	s0r2r	7.63

2014-07-08  Deniz Yuret  <dyuret@ku.edu.tr>

	* featselect3.out: Confirmed that s1 is the third feature picked
	when started with n0,s0.

	* featselect2.out: Started from n0,s0,s1 (11.22), search stopped at
	n0,s0,s1,s0r1,n0l1x,n0r (9.24) due to a bug, restarting.  BTW n0r
	should always have the zero bit set (except when there is no n0, in
	which case n0x is set).  So n0r and n0x have the same information.
	But their results look different.  Maybe we should have an exists
	bit instead of (or in addition to) a not exists bit (done).  Why
	should it matter?  6-degree poly?  Other curiosities: n0,s0 pair
	seems to do well, s1 not so necessary.  Start a new search from
	n0,s0 (done, picks s1).  Also curious about the interaction of
	kernel vs feature, rerun feature selection with other kernels to
	see (done).  Do not output instances where parser has a single
	option (done).

	So far no counts, no distances, no children other than s0r1 and
	n0l1 picked.

	* featselect1.out: Just did a single feature select for test.  n2,
	n1 related features did best probably because they can see the end
	of sentence.  Now need a search algorithm.

	* featselect.m: experimenting with feature selection using
	dump1000 dataset and poly 6 kernel.

	last	avg	nsv	time	feats
	10.77	9.57	4848	189.93	all
	12.57	11.22	5530	19.89	n0,s0,s1

	* feature-vector: current feature vector consists of the following
	dimensions.  ni refer to buffer words, si stack words, 0 is
	closest to stack|buffer boundary.  l1 refers to leftmost child, l2
	second leftmost child, similarly for r1, r2.  Each word has 100
	dims of vector + 9 dims of binary features indicating how many
	left/right children word has and whether it is missing:

	[l0 l1 l2 l3+ r0 r1 r2 r3+ missing]

	Last two sets of features are binary indicators for the n0-s0
	distance and s0-s1 distance, the two arc candidates.

	DONE: should scan Zhang08, Nivre11, parser.py, redshift for more
	possible features.  They ZN11 is a superset.  Only uses labels in
	addition to stuff we have.

	0001:0109: n0
	0110:0218: n1
	0219:0327: n2
	0328:0436: s0
	0437:0545: s1
	0546:0654: s2
	0655:0763: s0l1
	0764:0872: s0l2
	0873:0981: s0r1
	0982:1090: s0r2
	1091:1199: s1l1
	1200:1308: s1l2
	1309:1417: s1r1
	1418:1526: s1r2
	1527:1635: n0l1
	1635:1744: n0l2
	1745:1748: n0-s0 dist bits: 1,2,3,4+
	1749:1752: s0-s1 dist bits: 1,2,3,4+

	* kernel-summary: poly kernel degree [4..7] and rbf kernel
	gamma=[0.07..0.11] give best results.  Poly kernel about 5x faster
	than rbf with no significant performance difference (well 9.22 vs
	9.57 may be a bit significant,

	* rbf-kernel: 5x slower with 37K trset, compared to poly-kernel,
	confirming same minimum as 10K:
	gamma	last	avg	nsv	time
	0.06	12.86	9.84	5095	1078.2
	0.07	13.26	9.72	4963	1050.6
	0.08	11.53	9.22	4970	1062.3
	0.09	11.52	9.52	4908	1041.2
	0.10	10.34	9.62	4883	1035.9
	0.12	11.05	10.12	4875	1039.9

	* rbf-kernel: with reduced 10K training set
	gamma	last	avg	nsv	time
	0.01	29.63	20.96	2400	130.91
	0.02	17.79	17.28	2138	120.58
	0.05	15.50	14.40	1786	98.79
	0.06	14.54	14.08	1776	98.05
	0.07	14.48	13.55	1713	93.73
	0.08	15.70	13.52	1727	95.51
	0.09	14.25	13.61	1723	95.25
	0.10	14.74	13.62	1705	94.89
	0.11	15.51	13.63	1744	94.63
	0.12	14.41	13.92	1723	97.52
	0.15	14.85	14.43	1780	99.66
	0.20	15.95	14.94	1881	103.71
	0.50	19.02	19.26	2244	122.13
	1.00	21.79	21.67	2421	132.29
	10.0	23.93	23.75	2565	139.51

	* poly-kernel:
	degree	last	avg	nsv	time
	1	19.92	14.89	7117	263.07
	2	13.33	11.09	5691	208.75
	3	11.56	9.93	5212	207.43
	4	10.93	9.76	4933	194.89
	5	11.67	9.82	4895	190.71
	6	10.77	9.57	4848	189.93
	7	10.83	9.78	4933	196.31
	8	11.68	10.18	5035	201.17
	9	11.44	10.28	5038	203.50

==> run/dump1000.m <==
% path('dogma', path);
%

tic();
fprintf('Loading dump1000 data set\n');
load dump1000.mat;

x_te = x(:,1:10000);
y_te = y(1:10000);
x_tr = x(:,10001:end);
y_tr = y(10001:end);
%x_tr = x(:,10001:20000);
%y_tr = y(10001:20000);
toc();

% for i=1:9
% hp.type = 'poly';
% hp.gamma = 1;
% hp.coef0 = 1;
% hp.degree = i;
% fprintf('Using %dth degree poly kernel\n', hp.degree);

for i=[.07,.09]
hp.type = 'rbf';
hp.gamma = i;
fprintf('Using rbf kernel with gamma=%f\n', hp.gamma);

model_bak = model_init(@compute_kernel,hp);

tic();
%% PERC
% train Perceptron
fprintf('Training Perceptron model...\n');
model_perceptron = k_perceptron_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_perceptron.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_perceptron.beta2, 2));
fprintf('Testing last solution...');
pred_perceptron_last = model_predict(x_te,model_perceptron,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_perceptron_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_perceptron_av = model_predict(x_te,model_perceptron,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_perceptron_av~=y_te))/numel(y_te)*100);
toc();
end
return

%% PA_I
tic();
fprintf('Training PA-I model...\n');
model_pa1 = k_pa_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_pa1.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_pa1.beta2, 2));
fprintf('Testing last solution...');
pred_pa1_last = model_predict(x_te,model_pa1,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_pa1_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_pa1_av = model_predict(x_te,model_pa1,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_pa1_av~=y_te))/numel(y_te)*100);
toc();

%% PA_II
tic();
fprintf('Training PA-II model...\n');
model_pa2 = model_bak;
model_pa2.update = 2;
model_pa2 = k_pa_multi_train(x_tr,y_tr,model_pa2);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_pa2.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_pa2.beta2, 2));
fprintf('Testing last solution...');
pred_pa2_last = model_predict(x_te,model_pa2,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_pa2_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_pa2_av = model_predict(x_te,model_pa2,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_pa2_av~=y_te))/numel(y_te)*100);
toc();


%% PROJ++ eta=0.1
tic();
% train Projectron++
fprintf('Training Projectron++ model...\n');
model_projectron2 = k_projectron2_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_projectron2.beta, 2));
fprintf('Number of support vectors averaged solution:%d\n',size(model_projectron2.beta2, 2));
fprintf('Testing last solution...');
pred_projectron2_last = model_predict(x_te,model_projectron2,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n',numel(find(pred_projectron2_last~=y_te))/numel(y_te)*100);
fprintf('Testing averaged solution...');
pred_projectron2_av = model_predict(x_te,model_projectron2,1);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_projectron2_av~=y_te))/numel(y_te)*100);
toc();

%% RBP
tic();
% set maximum number of support vectors for RBP
model_bak.maxSV = size(model_projectron2.beta,2);
% train RBP
fprintf('Training RBP...\n');
model_rbp = k_perceptron_multi_train(x_tr,y_tr,model_bak);
fprintf('Done!\n');
fprintf('Number of support vectors last solution:%d\n',size(model_rbp.beta, 2));
fprintf('Testing last solution...');
pred_rbp_last = model_predict(x_te,model_rbp,0);
fprintf('Done!\n');
fprintf('%5.2f%% of errors on the test set.\n\n',numel(find(pred_rbp_last~=y_te))/numel(y_te)*100);
toc();

% tic();
% fprintf('Plotting error and SV curves...\n');
% % plot error curves
% figure(1)
% plot(model_pa1.aer(100:end),'k')
% hold on
% plot(model_perceptron.aer(100:end),'c')
% plot(model_projectron.aer(100:end),'b')
% plot(model_projectron2.aer(100:end),'m')
% plot(model_rbp.aer(100:end),'r')
% plot(model_forgetron.aer(100:end),'y')
% plot(model_oisvm.aer(100:end),'g')
% grid
% legend('PA-I','Perceptron','Projectron','Projectron++','RBP','Forgetron','OISVM')
% xlabel('Number of Samples')
% ylabel('Average Online Error')

% % plot support vector curves
% figure(2)
% plot(model_pa1.numSV,'k')
% hold on
% plot(model_perceptron.numSV,'c')
% plot(model_projectron.numSV,'b')
% plot(model_projectron2.numSV,'m')
% plot(model_rbp.numSV,'r')
% plot(model_forgetron.numSV,'y')
% plot(model_oisvm.numSV,'g')
% legend('PA-I','Perceptron','Projectron','Projectron++','RBP','Forgetron','OISVM','Location','NorthWest')
% grid
% xlabel('Number of Samples')
% ylabel('Number of Support Vectors')
% toc();


	* dump1000.m, k_perceptron_multi_train: Using first 10000 as test
	and 37716 as train (1 epoch) using all 1752 features.  At 3
	minutes and 90+ accuracy we can use this to do feature and kernel
	optimization.

	#36 SV:13.51(4862)	AER:13.51
	Number of support vectors last solution:4933
	Number of support vectors averaged solution:4933
	Testing last solution...Done!
	10.93% of errors on the test set.
	Testing averaged solution...Done!
	9.76% of errors on the test set.
	Elapsed time is 194.888029 seconds.

	* dump1000.mat: Saved features for first 1000 sentences:
	>> [x,y]=dumpfeatures(trn_w(1:1000),trn_h(1:1000));
	x          1752x46716            654771456  double
	y             1x46716               373728  double
	>> save dump1000 x y
	238705982 Jul  8  2014 dump1000.mat

	* dumpfeatures.m: The transition sequence is NOT unique.  SLR and
	RSL can always be interchanged!  So there are cases where
	oracle_cost has multiple zeros.  Prefer shift first?

	ns=22 nw=9 nx=1011 sptr=2 wptr=4
	stack=1  3
	cost=0  0  1
	heads=
	1  2  3  4  5  6  7  8  9
	6  1  1  5  1  0  8  6  6
	0  1  0  0  0  0  0  0  0

	We do not always have a zero cost move.  When the gold tree is
	projective all moves have positive cost:

	ns=106 nw=15 nx=5490 sptr=3 wptr=8
	stack=2  4  7
	cost=1  1  2
	heads=
	1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
	2   0   2   2   7   7   4   4   8   7  10  14  14  11   2
	2  0  2  0  7  7  0  0  0  0  0  0  0  0  0

	The number of transitions for an n word sentence is 2n-2.

2014-07-07  Deniz Yuret  <dyuret@ku.edu.tr>

	* ArcHybrid.m (valid_moves): Some code optimization.
	Constructor: 3886 sentences/sec
	Shift: 6141 w/s
	Shift+Right: 2680 w/s
	Shift+Left: 3014 w/s
	2xmove,2xhonnibal/w: 592 w/s
	2xmove,2xoracle/w: 1480 w/s
	2xmove,2xoracle,2xhonnibal/w: 468 w/s

	* ArcHybrid.m (update_features): Moved feature calculation in
	class, so feature vector does not need allocating.  This slowed
	down the code from 25 secs to 35 secs.  Creating and filling a new
	feature vector seems faster.  Keep feature calc out!?  Moving
	oracle_cost out did not make much difference.

	* honnibalFeatures.m:
	% Notes:
	% - We replace word-form and part-of-speech with word vector.
	% - We use polynomial kernel to represent feature combinations.
	% - In both left and right moves, s0 will be the child and popped
	% - There is no distance info for the two link candidates n0-s0, s0-s1
	% -- We should add them.
	% - We need indicator variables of when words do not exist.
	% -- Yes.
	% - There is no indication of sentence boundary (first/last word)
	% -- Missing flags should take care of this.
	% - There is no indication of root?
	% -- In general words that get heads get popped.
	% - Does n0 not have right children?
	% -- No.
	% - How about children of s1?  It is also a candidate (for RIGHT).
	% -- We should add them.
	% - Are children in the ldeps, rdeps lists ordered?
	% -- Yes, furthest children at the end.
	% - We should do some feature selection, are these all necessary?
	% - There is no in-between word features.
	% - Compare with graph based features of Zhang&Clark08
	% - Compare with feature engineering in Zhang&Nivre11

2014-07-06  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	+ We should debug honnibalFeatures before writing more code...
	+ Write oracle -- done but check in case.
	+ Write averaged perceptron.
	+ Test with raw features.
	+ Write outer product or kernel. (using dogma).

	* honnibalFeatures.m: Implemented features based on honnibal's
	blog entry which probably comes from:
	Goldberg, Yoav; Nivre, Joakim
	Training Deterministic Parsers with Non-Deterministic Oracles
	TACL 2013

	I added children of s1 as well and the distances n0-s0, s0-s1
	because those are the two current link candidates.  Each word also
	gets a nonexistent flag and left/right child count flags.  Total
	number of features is 109 per word, i.e. 16*109+8=1752.

	Timing for all shift/left transition followed by honnibalFeatures:
	35.5 secs for 1000 sentences 23358 words = 658 w/s.

	* ArcHybrid.m: Need to implement parse state using oop, otherwise
	parse transitions copy object data.  Takes 15 secs to create
	initial parse states 39832 training sentences.  146 secs if we
	initialize 39832 sentences and then shift through each of the
	950028 words.  283 seconds if we shift/left all words.  318
	seconds if we shift/right all words.  So parsing is more than 3000
	w/s.  Next we should look at extracting features, training etc.

2014-07-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* parser.py (Parser.train_one): gold_moves comes back empty when
	sentence non projective.  Modified script to ignore those cases by
	setting best = guess.  Training is roughly 100 words / sec for the
	15 iterations specified in code.  Training on WSJ-02-21 and
	evaluation on WSJ-22 gives 0.9034.  Commands in Makefile.

	* saveData.m: Convert conll data to matlab format for faster
	load.  Reduces load speed from several minutes to 11 seconds.
	Converted conllWSJToken_wikipedia2MUNK-50.mat.

	* loadCONLL.m: Load conll data to cell arrays.

==> run/savedata.m <==
clear;
dir = '/ai/home/vcirik/eParse/run/embedded/conllWSJToken_wikipedia2MUNK-50/';
tic();
[trn_w, trn_h] = loadCoNLL([dir '00/wsj_0001.dp']); % 989860 lines, wsj 02-21
toc(); tic();
[dev_w, dev_h] = loadCoNLL([dir '01/wsj_0101.dp']); % 41817 lines, wsj 22
toc(); tic();
[tst_w, tst_h] = loadCoNLL([dir '02/wsj_0201.dp']); % 191297 lines, wsj 00, 01, 23, 24
toc(); tic();
save('conllWSJToken_wikipedia2MUNK-50');
toc();
